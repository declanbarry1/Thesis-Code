{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a5faee-6231-46b1-9ed6-2f56acb13c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fa_support\n",
    "import evall\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82875705-b553-4a61-bebd-3bf6e1fe5e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [ 56, 5550, 2, 2 ]\n",
    "names = ['country', 'postcode', 'gender', 'loyalty']\n",
    "dic = dict(zip(names,numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a41dd21a-7d14-4ce4-9b7c-95f1b9faf4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui_matrix = np.load(\"fa_ui_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94ab18a9-2f8c-4119-a130-c1a4fd42f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_emb_dim = sum(dic.values())\n",
    "\n",
    "D_emb_dim = 50\n",
    "\n",
    "G_emb_dim = 50\n",
    "\n",
    "hidden_dim = 128\n",
    "alpha = 0\n",
    "\n",
    "# Initializer\n",
    "init = tf.initializers.glorot_normal()\n",
    "\n",
    "'''Generator and Discriminator Attribute Embeddings'''\n",
    "D_country_embs = tf.keras.layers.Embedding(input_dim = dic['country'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['country'],D_emb_dim))])\n",
    "D_postcode_embs = tf.keras.layers.Embedding(input_dim = dic['postcode'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['postcode'],D_emb_dim))])\n",
    "D_gender_embs = tf.keras.layers.Embedding(input_dim = dic['gender'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['gender'],D_emb_dim))])\n",
    "D_loyalty_embs = tf.keras.layers.Embedding(input_dim = dic['loyalty'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['loyalty'],D_emb_dim))])\n",
    "\n",
    "\n",
    "G_country_embs = tf.keras.layers.Embedding(input_dim = dic['country'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['country'],G_emb_dim))])\n",
    "G_postcode_embs = tf.keras.layers.Embedding(input_dim = dic['postcode'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['postcode'],G_emb_dim))])\n",
    "G_gender_embs = tf.keras.layers.Embedding(input_dim = dic['gender'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['gender'],G_emb_dim))])\n",
    "G_loyalty_embs = tf.keras.layers.Embedding(input_dim = dic['loyalty'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['loyalty'],G_emb_dim))])\n",
    "\n",
    "\n",
    "# Model input sizes\n",
    "G_input_size =  G_emb_dim*4\n",
    "D_input_size = item_emb_dim + D_emb_dim*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a487e0d9-4444-43fa-8fd4-f8bca787a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_input(country, postcode, gender, loyalty):\n",
    "    dic = {}\n",
    "    dic[\"country\"] = G_country_embs(tf.constant(country))\n",
    "    dic[\"postcode\"] = G_postcode_embs(tf.constant(postcode))\n",
    "    dic[\"gender\"] = G_gender_embs(tf.constant(gender))\n",
    "    dic[\"loyalty\"] = G_loyalty_embs(tf.constant(loyalty))\n",
    "    emb = tf.keras.layers.concatenate(list(dic.values()), 1)\n",
    "    return emb\n",
    "\n",
    "# Generates user based on concatenation of all attributes\n",
    "def generator():\n",
    "    bc_input = tf.keras.layers.Input(shape=(G_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', activity_regularizer = 'l2')(bc_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', activity_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(item_emb_dim, activation ='sigmoid', activity_regularizer = 'l2')(x)\n",
    "    g_model = tf.keras.models.Model(bc_input, x, name = 'generator')\n",
    "    return g_model\n",
    "g_model = generator()\n",
    "\n",
    "def discriminator_input(country, postcode, gender, loyalty, item_emb):\n",
    "    dic = {}\n",
    "    dic[\"country\"] = D_country_embs(tf.constant(country))\n",
    "    dic[\"postcode\"] = D_postcode_embs(tf.constant(postcode))\n",
    "    dic[\"gender\"] = D_gender_embs(tf.constant(gender))\n",
    "    dic[\"loyalty\"] = D_loyalty_embs(tf.constant(loyalty))\n",
    "    item_emb = tf.cast(item_emb, dtype=float)\n",
    "    emb = tf.keras.layers.concatenate(list(dic.values()), 1)\n",
    "    final_emb = tf.keras.layers.concatenate([emb, item_emb], 1)\n",
    "    return final_emb\n",
    "\n",
    "def discriminator():\n",
    "    d_input = tf.keras.layers.Input(shape=(D_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', kernel_regularizer = 'l2')(d_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.models.Model(d_input, x, name = 'discriminator')\n",
    "    return model\n",
    "d_model = discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd3147db-f719-46a3-9164-679dd2558685",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load(\"new_fa_train_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ee7a517-2559-4ef9-9cad-5fac20d99285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake):\n",
    "    return -tf.reduce_mean(fake)\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    logit = tf.reduce_mean(fake- real)\n",
    "    return logit\n",
    "\n",
    "def counter_loss(counter):\n",
    "    return tf.reduce_mean(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eef9458-f99a-4c4e-8780-cadb4349d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGAN Class\n",
    "class WGAN(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        discriminator_extra_steps=5,\n",
    "        batch_size = 577\n",
    "    ):\n",
    "        super(WGAN, self).__init__()\n",
    "        self.discriminator = d_model\n",
    "        self.generator = g_model\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.k = 10        \n",
    "        self.index = 0 \n",
    "        self.c_index = 0 \n",
    "        self.gp_weight = 10\n",
    "        self.eval_steps = 0\n",
    "        self.max_p10 = .01 \n",
    "        self.max_g10 = .01\n",
    "        self.max_m10 = .01\n",
    "        self.max_p20 = .01\n",
    "        self.max_g20 = .01\n",
    "        self.max_m20 = 0.01\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn,c_loss_fn, run_eagerly):\n",
    "        super(WGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.c_loss_fn = c_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "        self.run_eagerly = run_eagerly\n",
    "\n",
    "\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_items, fake_items, country, postcode, gender, loyalty):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated image\n",
    "        alpha = tf.random.normal([batch_size,1], 0.0, 1.0)\n",
    "        diff = fake_items - real_items\n",
    "        interpolated = real_items + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            interpolated_input = discriminator_input(  country, postcode, gender, loyalty,  interpolated)\n",
    "            pred = self.discriminator(interpolated_input)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = tape.gradient(pred, [interpolated])[0] #+1e-10\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, real_users):\n",
    "        self.eval_steps +=1 \n",
    "        c_batch_size = 2*self.batch_size\n",
    "        for i in range(self.d_steps):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Get batch data\n",
    "                country, postcode, loyalty, gender,  brand_id, category, colour, divisioncode, \\\n",
    "                itemcategorycode, itemfamilycode, itemseason, productgroup, user, real_items = fa_support.get_batchdata(self.index, self.index + self.batch_size)\n",
    "                # Get batch of counter examples\n",
    "                #counter_brand_id, counter_category, counter_colour, counter_divisioncode, \\\n",
    "                #counter_itemcategorycode, counter_itemfamilycode, counter_itemseason, \\\n",
    "                #counter_productgroup,  counter_users = support.get_counter_batch(self.c_index, self.c_index + c_batch_size)\n",
    "                # Generate fake users from attributes\n",
    "                g_input0 = generator_input(country, postcode, gender, loyalty)\n",
    "                fake_items = self.generator(g_input0)\n",
    "                # Get the logits for the fake users\n",
    "                d_input0 = discriminator_input(country, postcode, gender, loyalty, fake_items)\n",
    "                fake_logits = self.discriminator(d_input0)\n",
    "                # Get the logits for the real user\n",
    "                d_input1 = discriminator_input( country, postcode, gender, loyalty, real_items)\n",
    "                real_logits = self.discriminator(d_input1)\n",
    "                # Get logits for counter examples\n",
    "                \n",
    "                #d_input2 = discriminator_input(counter_brand_id, counter_category, counter_colour, counter_divisioncode, \\\n",
    "                #counter_itemcategorycode, counter_itemfamilycode, counter_itemseason, \\\n",
    "                #counter_productgroup,  counter_users)\n",
    "                #counter_logits = self.discriminator(d_input2)\n",
    "                # Calculate the discriminator loss using the fake and real image logits\n",
    "                d_cost = self.d_loss_fn(real_logits, fake_logits)\n",
    "                #c_loss = self.c_loss_fn(counter_logits)\n",
    "                # Get gradient penalty\n",
    "                gp = self.gradient_penalty(self.batch_size, real_items, fake_items, country, postcode, gender, loyalty)\n",
    "                # Later add counter loss\n",
    "                d_loss = d_cost + gp*self.gp_weight +1e-10 # +c_loss\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(zip(d_gradient, self.discriminator.trainable_variables))\n",
    "\n",
    "        # Train the generator\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Generate fake images using the generator\n",
    "            g_input1 = generator_input( country, postcode, gender, loyalty)\n",
    "            gen_items = self.generator(g_input1)\n",
    "            # Get the discriminator logits for fake images\n",
    "            d_input2 = discriminator_input( country, postcode, gender, loyalty, gen_items)\n",
    "            gen_logits = self.discriminator(d_input2)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_logits) + 1e-10\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        '''   if self.eval_steps %760==0:\n",
    "            p_at_10,G_at_10,M_at_10 = wgan.test_step(10)\n",
    "            p_at_20,G_at_20,M_at_20 = wgan.test_step(20)\n",
    "            if p_at_10 > self.max_p10:\n",
    "                self.max_p10 = p_at_10\n",
    "            if G_at_10 > self.max_g10:\n",
    "                self.max_g10 = G_at_10\n",
    "            if M_at_10 > self.max_m10:\n",
    "                self.max_m10 = M_at_10\n",
    "            if p_at_20 > self.max_p20:\n",
    "                self.max_p10 = p_at_20\n",
    "            if G_at_20 > self.max_g20:\n",
    "                self.max_g10 = G_at_20\n",
    "            if M_at_20 > self.max_m20:\n",
    "                self.max_m10 = M_at_20\n",
    "            \n",
    "            return {\"d_loss\": d_loss, \"g_loss\": g_loss, \"p10\":p_at_10,\n",
    "                           \"G10\":G_at_10,\"M10\":M_at_10, \"p20\": p_at_20,\"G20\":G_at_20,\"M20\":M_at_20}\n",
    "        else:'''\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "\n",
    "    def test_step(self, k):\n",
    "        country, postcode, loyal, gender,  brand_id, category, colour, divisioncode, \\\n",
    "        itemcategorycode, itemfamilycode, itemseason, productgroup, item, user = fa_support.get_testdata(0,5000)\n",
    "\n",
    "        test_BATCH_SIZE = user.size\n",
    "        g_input1 = generator_input( country, postcode, gender, loyal)\n",
    "        gen_items = self.generator(g_input1)\n",
    "        sim_items = fa_support.get_intersection_similar_item(gen_items, k)\n",
    "        count = 0\n",
    "\n",
    "        count = 0\n",
    "        for u, item_list in zip(user, sim_items):       \n",
    "            for i in item_list:\n",
    "                if ui_matrix[u,i] == 1:\n",
    "                    count = count + 1            \n",
    "        p_at_k = round(count/(test_BATCH_SIZE * k), 4)\n",
    "\n",
    "        RS = []\n",
    "        ans = 0.0\n",
    "        for u, item_list in zip(user, sim_items):           \n",
    "            r=[]\n",
    "            for i in item_list:\n",
    "                 r.append(ui_matrix[u][i])\n",
    "            ans = ans + evall.ndcg_at_k(r, k, method=1)\n",
    "            RS.append(r)\n",
    "        G_at_k = ans/test_BATCH_SIZE\n",
    "        M_at_k = evall.mean_average_precision(RS)\n",
    "\n",
    "\n",
    "        return p_at_k,G_at_k,M_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af23a68e-d241-4632-9698-cd7e8c4c682b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "762/762 [==============================] - 115s 145ms/step - d_loss: -8.5359 - g_loss: 14.6170\n",
      "Epoch 2/50\n",
      "762/762 [==============================] - 109s 143ms/step - d_loss: -8.1898 - g_loss: 55.8338\n",
      "Epoch 3/50\n",
      "762/762 [==============================] - 111s 146ms/step - d_loss: -9.9607 - g_loss: 93.5689\n",
      "Epoch 4/50\n",
      "762/762 [==============================] - 112s 147ms/step - d_loss: -10.6498 - g_loss: 119.4493\n",
      "Epoch 5/50\n",
      "762/762 [==============================] - 112s 147ms/step - d_loss: -10.9154 - g_loss: 132.4554\n",
      "Epoch 6/50\n",
      "762/762 [==============================] - 112s 147ms/step - d_loss: -11.1955 - g_loss: 138.7192\n",
      "Epoch 7/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -11.2118 - g_loss: 143.0084\n",
      "Epoch 8/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -11.2269 - g_loss: 146.0350\n",
      "Epoch 9/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -10.5811 - g_loss: 146.9510\n",
      "Epoch 10/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -7.5767 - g_loss: 153.0173\n",
      "Epoch 11/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -6.8262 - g_loss: 164.5763\n",
      "Epoch 12/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -7.4855 - g_loss: 172.3972\n",
      "Epoch 13/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -7.9001 - g_loss: 175.4420\n",
      "Epoch 14/50\n",
      "762/762 [==============================] - 113s 149ms/step - d_loss: -8.3030 - g_loss: 177.1325\n",
      "Epoch 15/50\n",
      "762/762 [==============================] - 112s 148ms/step - d_loss: -8.8473 - g_loss: 180.6000\n",
      "Epoch 16/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -9.4855 - g_loss: 192.3189\n",
      "Epoch 17/50\n",
      "762/762 [==============================] - 112s 148ms/step - d_loss: -9.6477 - g_loss: 204.3306\n",
      "Epoch 18/50\n",
      "762/762 [==============================] - 112s 148ms/step - d_loss: -9.8709 - g_loss: 212.3955\n",
      "Epoch 19/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -9.8819 - g_loss: 217.4149\n",
      "Epoch 20/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -9.5275 - g_loss: 219.4248\n",
      "Epoch 21/50\n",
      "762/762 [==============================] - 112s 148ms/step - d_loss: -7.5705 - g_loss: 220.9429\n",
      "Epoch 22/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -7.6464 - g_loss: 223.7645\n",
      "Epoch 23/50\n",
      "762/762 [==============================] - 112s 148ms/step - d_loss: -8.6953 - g_loss: 229.0319\n",
      "Epoch 24/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -7.4486 - g_loss: 236.1868\n",
      "Epoch 25/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -9.0905 - g_loss: 243.8016\n",
      "Epoch 26/50\n",
      "762/762 [==============================] - 112s 147ms/step - d_loss: -9.6936 - g_loss: 252.7407\n",
      "Epoch 27/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -9.9047 - g_loss: 257.8356\n",
      "Epoch 28/50\n",
      "762/762 [==============================] - 112s 147ms/step - d_loss: -9.3934 - g_loss: 262.4298\n",
      "Epoch 29/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -6.9788 - g_loss: 270.3520\n",
      "Epoch 30/50\n",
      "762/762 [==============================] - 112s 148ms/step - d_loss: -7.8396 - g_loss: 275.5281\n",
      "Epoch 31/50\n",
      "762/762 [==============================] - 112s 147ms/step - d_loss: -8.5141 - g_loss: 282.2629\n",
      "Epoch 32/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -8.2850 - g_loss: 288.2815\n",
      "Epoch 33/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -6.8814 - g_loss: 289.3845\n",
      "Epoch 34/50\n",
      "762/762 [==============================] - 112s 148ms/step - d_loss: -7.1242 - g_loss: 293.8159\n",
      "Epoch 35/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -7.3057 - g_loss: 296.6590\n",
      "Epoch 36/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -7.5275 - g_loss: 296.6816\n",
      "Epoch 37/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -6.7515 - g_loss: 296.4481\n",
      "Epoch 38/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -6.8869 - g_loss: 303.1755\n",
      "Epoch 39/50\n",
      "762/762 [==============================] - 112s 147ms/step - d_loss: -7.0011 - g_loss: 304.1635\n",
      "Epoch 40/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -7.4916 - g_loss: 310.5820\n",
      "Epoch 41/50\n",
      "762/762 [==============================] - 112s 148ms/step - d_loss: -7.4938 - g_loss: 322.3108\n",
      "Epoch 42/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -7.5498 - g_loss: 329.7155\n",
      "Epoch 43/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -6.9158 - g_loss: 329.9838\n",
      "Epoch 44/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -6.3819 - g_loss: 331.6551\n",
      "Epoch 45/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -6.3407 - g_loss: 335.9532\n",
      "Epoch 46/50\n",
      "762/762 [==============================] - 112s 148ms/step - d_loss: -6.5722 - g_loss: 337.5766\n",
      "Epoch 47/50\n",
      "762/762 [==============================] - 112s 147ms/step - d_loss: -6.8019 - g_loss: 339.9735\n",
      "Epoch 48/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -6.5956 - g_loss: 331.2423\n",
      "Epoch 49/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -6.7457 - g_loss: 330.1529\n",
      "Epoch 50/50\n",
      "762/762 [==============================] - 113s 148ms/step - d_loss: -6.9811 - g_loss: 334.6866\n",
      "(0.0003, 0.0012618509871032918, 0.0006999206349206349) \n",
      " (0.0004, 0.0025463297689551005, 0.0010633393034554025)\n"
     ]
    }
   ],
   "source": [
    "# Fit \n",
    "epochs = 50\n",
    "# Instantiate the WGAN model.\n",
    "wgan = WGAN(\n",
    "    discriminator=discriminator,\n",
    "    generator=generator,\n",
    "    discriminator_extra_steps=3\n",
    ")\n",
    "\n",
    "# Compile the WGAN model.\n",
    "wgan.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    g_optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    g_loss_fn=generator_loss,\n",
    "    d_loss_fn=discriminator_loss,\n",
    "    c_loss_fn = counter_loss,\n",
    "    run_eagerly=False)\n",
    "\n",
    "# Start training the model.\n",
    "fit = wgan.fit(train, batch_size=100, epochs=epochs, verbose=True)\n",
    "print(wgan.test_step(10), \"\\n\", wgan.test_step(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c8ffb53-e82c-4488-ac7a-a1a5f6b669ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_dic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c41858dc4991>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"User_Model_WGAN_initial\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_g10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_g20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_p10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_p20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_dic' is not defined"
     ]
    }
   ],
   "source": [
    "eval_dic[\"User_Model_WGAN_initial\"] = [wgan.max_g10, wgan.max_g20, wgan.max_m10, wgan.max_m20, wgan.max_p10, wgan.max_p20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "696792c3-70e1-4bf2-83d0-7b39d149bb14",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_dic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-40ce1b1fac62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_dic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_dic' is not defined"
     ]
    }
   ],
   "source": [
    "eval_dic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
