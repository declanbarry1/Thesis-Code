{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba60af98-31b5-4d43-a3af-175a8faef870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fa_support\n",
    "import evall\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c93068fd-a2f2-4ebf-a11d-78f9ef673319",
   "metadata": {},
   "outputs": [],
   "source": [
    "train =  np.load(\"new_fa_train_data.npy\")\n",
    "#test = np.load(\"new_fa_test_data.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2106a4f-6af1-4e9a-9909-2214001e7778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86297, 10456)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ui_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5006697c-bc0e-4352-bbe3-c7a7e02b39ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02a872e4-35a5-4ff2-8582-fe9a6a89c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = sparse.csr_matrix(ui_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e6186a3-e31b-428c-be11-58ce7788fb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76163, 14)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19704f01-aebc-4153-b77e-f64df997ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [  310, 8, 53, 3, 13, 4, 3, 78]\n",
    "names = ['brand_id',\n",
    "         'category', 'colour', 'divisioncode', 'itemcategorycode',\n",
    "         'itemfamilycode', 'itemseason', 'productgroup']\n",
    "dic = dict(zip(names,numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "844084a9-018b-448d-99dc-251ae3068724",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dic =pd.read_csv(\"Eval_dic\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c5ba273-7172-48d6-a5b1-80ec0a7d5715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WGAN</th>\n",
       "      <th>WGAN-GP_100_epochs_lr_0.0001</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.431059</td>\n",
       "      <td>0.478360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.475948</td>\n",
       "      <td>0.524330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.354883</td>\n",
       "      <td>0.394489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.378315</td>\n",
       "      <td>0.421206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.311600</td>\n",
       "      <td>0.350700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.338500</td>\n",
       "      <td>0.375900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       WGAN  WGAN-GP_100_epochs_lr_0.0001\n",
       "0  0.431059                      0.478360\n",
       "1  0.475948                      0.524330\n",
       "2  0.354883                      0.394489\n",
       "3  0.378315                      0.421206\n",
       "4  0.311600                      0.350700\n",
       "5  0.338500                      0.375900"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2532e270-afdb-4ca9-b73c-b4e908056280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_emb_dim = sum(dic.values())\n",
    "\n",
    "D_emb_dim = 50\n",
    "\n",
    "G_emb_dim = 50\n",
    "\n",
    "hidden_dim = 128\n",
    "\n",
    "# Initializer\n",
    "init = tf.initializers.glorot_normal()\n",
    "\n",
    "'''Generator and Discriminator Attribute Embeddings'''\n",
    "\n",
    "D_brand_embs = tf.keras.layers.Embedding(input_dim = dic['brand_id'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['brand_id'],D_emb_dim))])\n",
    "D_category_embs = tf.keras.layers.Embedding(input_dim = dic['category'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['category'],D_emb_dim))])\n",
    "D_colour_embs = tf.keras.layers.Embedding(input_dim = dic['colour'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['colour'],D_emb_dim))])\n",
    "D_div_embs = tf.keras.layers.Embedding(input_dim = dic['divisioncode'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['divisioncode'],D_emb_dim))])\n",
    "D_itemcat_embs = tf.keras.layers.Embedding(input_dim = dic['itemcategorycode'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemcategorycode'],D_emb_dim))])\n",
    "D_itemfam_embs = tf.keras.layers.Embedding(input_dim = dic['itemfamilycode'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemfamilycode'],D_emb_dim))])\n",
    "D_season_embs = tf.keras.layers.Embedding(input_dim = dic['itemseason'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemseason'],D_emb_dim))])\n",
    "D_prod_embs = tf.keras.layers.Embedding(input_dim = dic['productgroup'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['productgroup'],D_emb_dim))])\n",
    "\n",
    "\n",
    "#G_price_embs = tf.keras.layers.Embedding(input_dim = dic['pricetype'], output_dim = G_emb_dim,\n",
    "#                                               trainable=True, weights = [init(shape=( dic['pricetype'],G_emb_dim))])\n",
    "G_brand_embs = tf.keras.layers.Embedding(input_dim = dic['brand_id'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['brand_id'],G_emb_dim))])\n",
    "G_category_embs = tf.keras.layers.Embedding(input_dim = dic['category'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['category'],G_emb_dim))])\n",
    "G_colour_embs = tf.keras.layers.Embedding(input_dim = dic['colour'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['colour'],G_emb_dim))])\n",
    "G_div_embs = tf.keras.layers.Embedding(input_dim = dic['divisioncode'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['divisioncode'],G_emb_dim))])\n",
    "G_itemcat_embs = tf.keras.layers.Embedding(input_dim = dic['itemcategorycode'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemcategorycode'],G_emb_dim))])\n",
    "G_itemfam_embs = tf.keras.layers.Embedding(input_dim = dic['itemfamilycode'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemfamilycode'],G_emb_dim))])\n",
    "G_season_embs = tf.keras.layers.Embedding(input_dim = dic['itemseason'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemseason'],G_emb_dim))])\n",
    "G_prod_embs = tf.keras.layers.Embedding(input_dim = dic['productgroup'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['productgroup'],G_emb_dim))])\n",
    "\n",
    "\n",
    "# Model input sizes\n",
    "G_input_size =  G_emb_dim*8\n",
    "D_input_size = user_emb_dim + D_emb_dim*8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac64da8b-0de0-4ed9-a101-0c087f7e3750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generator_input( brand_id, category, colour, divisioncode, itemcategorycode, itemfamilycode, itemseason, productgroup):\n",
    "    emb_dic = {}\n",
    "#    dic[\"pricetype\"] = G_brand_embs(tf.constant(pricetype))\n",
    "    dic[\"brand_id\"] = G_brand_embs(tf.constant(brand_id))\n",
    "    dic[\"category\"] = G_brand_embs(tf.constant(category))\n",
    "    dic[\"colour\"] = G_brand_embs(tf.constant(colour))\n",
    "    dic[\"divisioncode\"] = G_brand_embs(tf.constant(divisioncode))\n",
    "    dic[\"itemcategorycode\"] = G_brand_embs(tf.constant(itemcategorycode))\n",
    "    dic[\"itemfamilycode\"] = G_brand_embs(tf.constant(itemfamilycode))\n",
    "    dic[\"itemseason\"] = G_brand_embs(tf.constant(itemseason))\n",
    "    dic[\"productgroup\"] = G_brand_embs(tf.constant(productgroup))\n",
    "    emb = tf.keras.layers.concatenate(list(dic.values()), 1)\n",
    "    return emb\n",
    "\n",
    "# Generates user based on concatenation of all attributes\n",
    "def generator():\n",
    "    bc_input = tf.keras.layers.Input(shape=(G_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', activity_regularizer = 'l2')(bc_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', activity_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(user_emb_dim, activation ='sigmoid', activity_regularizer = 'l2')(x)\n",
    "    g_model = tf.keras.models.Model(bc_input, x, name = 'generator')\n",
    "    return g_model\n",
    "g_model = generator()\n",
    "\n",
    "def discriminator_input( brand_id, category, colour, divisioncode, itemcategorycode, itemfamilycode, \n",
    "                        itemseason, productgroup, user_emb):\n",
    "    emb_dic = {}\n",
    "#    dic[\"pricetype\"]  = D_brand_embs(tf.constant(pricetype))\n",
    "    dic[\"brand_id\"]  = D_brand_embs(tf.constant(brand_id))\n",
    "    dic[\"category\"]  = D_brand_embs(tf.constant(category))\n",
    "    dic[\"colour\"]  = D_brand_embs(tf.constant(colour))\n",
    "    dic[\"divisioncode\"]  = D_brand_embs(tf.constant(divisioncode))\n",
    "    dic[\"itemcategorycode\"]  = D_brand_embs(tf.constant(itemcategorycode))\n",
    "    dic[\"itemfamilycode\"]  = D_brand_embs(tf.constant(itemfamilycode))\n",
    "    dic[\"itemseason\"]  = D_brand_embs(tf.constant(itemseason))\n",
    "    dic[\"productgroup\"]  = D_brand_embs(tf.constant(productgroup))\n",
    "    user_emb = tf.cast(user_emb, dtype=float)\n",
    "    emb = tf.keras.layers.concatenate(list(dic.values()), 1)\n",
    "    final_emb = tf.keras.layers.concatenate([emb, user_emb], 1)\n",
    "    return final_emb\n",
    "\n",
    "def discriminator():\n",
    "    d_input = tf.keras.layers.Input(shape=(D_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', kernel_regularizer = 'l2', kernel_constraint=tf.keras.constraints.MinMaxNorm(min_value=-.1, max_value=.1, rate=1.0, axis=0))(d_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', kernel_regularizer = 'l2',kernel_constraint = tf.keras.constraints.MinMaxNorm(min_value=-.1, max_value=.1, rate=1.0, axis=0))(x)\n",
    "    x = tf.keras.layers.Dense(1, kernel_constraint=tf.keras.constraints.MinMaxNorm(min_value=-.1, max_value=.1, rate=1.0, axis=0))(x)\n",
    "    model = tf.keras.models.Model(d_input, x, name = 'discriminator')\n",
    "    return model\n",
    "d_model = discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a6b7a29-3d36-4499-b981-9c60d9126466",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua_matrix = np.load(\"new_fa_ua_matrix.npy\")\n",
    "ia_matrix = np.load(\"new_fa_ia_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97527ce6-02dc-4463-b5cf-7e847e2d96a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Cross entropy loss means \\ndef generator_loss(d_logits):\\n    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits, labels=tf.ones_like(d_logits)))\\n\\ndef discriminator_loss(real, fake):\\n    logit = tf.reduce_mean(fake-real)\\n    r = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real, labels=tf.ones_like(real)))\\n    f = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake, labels=tf.zeros_like(fake)))\\n    return r+f\\n\\ndef counter_loss(counter):\\n    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=counter, labels=tf.zeros_like(counter))) \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Loss functions'''\n",
    "def generator_loss(fake_user):\n",
    "    return -tf.reduce_mean(fake_user)\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    logit = tf.reduce_mean(fake- real)\n",
    "    return logit\n",
    "\n",
    "def counter_loss(counter):\n",
    "    return tf.reduce_mean(counter)\n",
    "\n",
    "def discriminator_counter_loss(real, fake, counter):\n",
    "    logit = tf.reduce_mean(real - counter - fake)\n",
    "    return logit\n",
    "''' Cross entropy loss means \n",
    "def generator_loss(d_logits):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits, labels=tf.ones_like(d_logits)))\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    logit = tf.reduce_mean(fake-real)\n",
    "    r = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real, labels=tf.ones_like(real)))\n",
    "    f = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake, labels=tf.zeros_like(fake)))\n",
    "    return r+f\n",
    "\n",
    "def counter_loss(counter):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=counter, labels=tf.zeros_like(counter))) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7279ddc9-21b5-4132-8c72-d101dd96f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui_matrix = np.load(\"fa_ui_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e28cca6-75a3-49b2-9910-a6f3cc12b371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WGAN Class\n",
    "class WGAN(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        discriminator_extra_steps=5,\n",
    "        batch_size = 1000\n",
    "    ):\n",
    "        super(WGAN, self).__init__()\n",
    "        self.discriminator = d_model\n",
    "        self.generator = g_model\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.k = 10        \n",
    "        self.index = 0 \n",
    "        self.c_index = 0 \n",
    "        self.gp_weight = 10\n",
    "        self.eval_steps = 0\n",
    "        self.max_p10 = .01 \n",
    "        self.max_g10 = .01\n",
    "        self.max_m10 = .01\n",
    "        self.max_p20 = .01\n",
    "        self.max_g20 = .01\n",
    "        self.max_m20 = .01\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn,c_loss_fn, run_eagerly):\n",
    "        super(WGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.c_loss_fn = c_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "        self.run_eagerly = run_eagerly\n",
    "\n",
    "\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_users, fake_users,  brand_id,\\\n",
    "                                    category, colour, divisioncode, itemcategorycode, itemfamilycode, \\\n",
    "                                    itemseason, productgroup):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated image\n",
    "        alpha = tf.random.normal([batch_size,1], 0.0, 1.0)\n",
    "        diff = fake_users - real_users\n",
    "        interpolated = real_users + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            interpolated_input = discriminator_input(   brand_id,\\\n",
    "                                    category, colour, divisioncode, itemcategorycode, itemfamilycode, \\\n",
    "                                    itemseason, productgroup, interpolated)\n",
    "            pred = self.discriminator(interpolated_input)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = tape.gradient(pred, [interpolated])[0] #+1e-10\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, real_users):\n",
    "        self.eval_steps +=1 \n",
    "        c_batch_size = 2*self.batch_size\n",
    "        for i in range(self.d_steps):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Get batch data\n",
    "                country, postcode, loyal, gender,  brand_id, category, colour, divisioncode, \\\n",
    "                itemcategorycode, itemfamilycode, itemseason, productgroup, real_users, items = fa_support.get_batchdata(self.index, self.index + self.batch_size)\n",
    "                # Get batch of counter examples\n",
    "                counter_brand_id, counter_category, counter_colour, counter_divisioncode, \\\n",
    "                counter_itemcategorycode, counter_itemfamilycode, counter_itemseason, \\\n",
    "                counter_productgroup,  counter_users = fa_support.get_counter_batch(self.c_index, self.c_index + c_batch_size)\n",
    "                # Generate fake users from attributes\n",
    "                g_input0 = generator_input(  brand_id, category, colour, divisioncode, \\\n",
    "                                           itemcategorycode, itemfamilycode, itemseason, productgroup)\n",
    "                fake_users = self.generator(g_input0)\n",
    "                # Get the logits for the fake users\n",
    "                d_input0 = discriminator_input( brand_id, category, colour, divisioncode, \\\n",
    "                                           itemcategorycode, itemfamilycode, itemseason, productgroup, fake_users)\n",
    "                fake_logits = self.discriminator(d_input0)\n",
    "                # Get the logits for the real user\n",
    "                d_input1 = discriminator_input(  brand_id, category, colour, divisioncode, \\\n",
    "                                           itemcategorycode, itemfamilycode, itemseason, productgroup, real_users)\n",
    "                real_logits = self.discriminator(d_input1)\n",
    "                # Get logits for counter examples\n",
    "                \n",
    "                d_input2 = discriminator_input( counter_brand_id, counter_category, counter_colour, counter_divisioncode, \\\n",
    "                counter_itemcategorycode, counter_itemfamilycode, counter_itemseason, \\\n",
    "                counter_productgroup,  counter_users)\n",
    "                counter_logits = self.discriminator(d_input2)\n",
    "                # Calculate the discriminator loss using the fake and real image logits\n",
    "                d_cost = self.d_loss_fn(real_logits, fake_logits)\n",
    "                c_loss = self.c_loss_fn(counter_logits)\n",
    "                # Get gradient penalty\n",
    "                '''gp = self.gradient_penalty(self.batch_size, real_users, fake_users, brand_id,\\\n",
    "                                    category, colour, divisioncode, itemcategorycode, itemfamilycode, \\\n",
    "                                    itemseason, productgroup)'''\n",
    "                # Later add counter loss\n",
    "                d_loss = d_cost + c_loss +1e-16 # + gp*self.gp_weight \n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(zip(d_gradient, self.discriminator.trainable_variables))\n",
    "\n",
    "        # Train the generator\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Generate fake images using the generator\n",
    "            g_input1 = generator_input(  brand_id, category, colour, divisioncode, \\\n",
    "                                           itemcategorycode, itemfamilycode, itemseason, productgroup)\n",
    "            gen_users = self.generator(g_input1)\n",
    "            # Get the discriminator logits for fake images\n",
    "            d_input2 = discriminator_input(  brand_id, category, colour, divisioncode, \\\n",
    "                                           itemcategorycode, itemfamilycode, itemseason, productgroup, gen_users)\n",
    "            gen_logits = self.discriminator(d_input2)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_logits) +1e-16\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        if self.eval_steps %760==0:\n",
    "            p_at_10,G_at_10,M_at_10 = wgan.test_step(10)\n",
    "            p_at_20,G_at_20,M_at_20 = wgan.test_step(20)\n",
    "            if p_at_10 > self.max_p10:\n",
    "                self.max_p10 = p_at_10\n",
    "            if G_at_10 > self.max_g10:\n",
    "                self.max_g10 = G_at_10\n",
    "            if M_at_10 > self.max_m10:\n",
    "                self.max_m10 = M_at_10\n",
    "            if p_at_20 > self.max_p20:\n",
    "                self.max_p20 = p_at_20\n",
    "            if G_at_20 > self.max_g20:\n",
    "                self.max_g20 = G_at_20\n",
    "            if M_at_20 > self.max_m20:\n",
    "                self.max_m20 = M_at_20\n",
    "            \n",
    "            return {\"d_loss\": d_loss, \"g_loss\": g_loss, \"p10\":p_at_10,\n",
    "                           \"G10\":G_at_10,\"M10\":M_at_10, \"p20\": p_at_20,\"G20\":G_at_20,\"M20\":M_at_20}\n",
    "        else:\n",
    "            return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "\n",
    "    def test_step(self, k):\n",
    "        country, postcode, loyal, gender,  brand_id, category, colour, divisioncode, \\\n",
    "        itemcategorycode, itemfamilycode, itemseason, productgroup, item, user = fa_support.get_testdata(0,5000)\n",
    "        \n",
    "        test_BATCH_SIZE = country.size\n",
    "        g_input1 = generator_input( brand_id, category, colour, divisioncode, \\\n",
    "                                           itemcategorycode, itemfamilycode, itemseason, productgroup)\n",
    "        gen_users = self.generator(g_input1)\n",
    "        sim_users = fa_support.get_intersection_similar_user(gen_users, k)\n",
    "        count = 0\n",
    "        for i, user_list in zip(item, sim_users):       \n",
    "            for u in user_list:\n",
    "                if ui_matrix[u,i] == 1:\n",
    "                    count = count + 1            \n",
    "        p_at_k = round(count/(test_BATCH_SIZE * k), 4)\n",
    "\n",
    "        RS = []\n",
    "        ans = 0.0\n",
    "        for i, user_list in zip(item, sim_users):           \n",
    "            r=[]\n",
    "            for user in user_list:\n",
    "                 r.append(ui_matrix[user][i])\n",
    "            ans = ans + evall.ndcg_at_k(r, k, method=1)\n",
    "            RS.append(r)\n",
    "        G_at_k = ans/test_BATCH_SIZE\n",
    "        M_at_k = evall.mean_average_precision(RS)\n",
    "       \n",
    "\n",
    "        return p_at_k,G_at_k,M_at_k\n",
    "    \n",
    "    def test_coldstart(self, k):\n",
    "        country, postcode, loyal, gender,  brand_id, category, colour, divisioncode, \\\n",
    "        itemcategorycode, itemfamilycode, itemseason, productgroup, item, user = fa_support.get_coldstart_items()\n",
    "        \n",
    "        test_BATCH_SIZE = country.size\n",
    "        g_input1 = generator_input( brand_id, category, colour, divisioncode, \\\n",
    "                                           itemcategorycode, itemfamilycode, itemseason, productgroup)\n",
    "        gen_users = self.generator(g_input1)\n",
    "        sim_users = fa_support.get_intersection_similar_user(gen_users, k)\n",
    "        count = 0\n",
    "        for i, user_list in zip(item, sim_users):       \n",
    "            for u in user_list:\n",
    "                if ui_matrix[u,i] == 1:\n",
    "                    count = count + 1            \n",
    "        p_at_k = round(count/(test_BATCH_SIZE * k), 4)\n",
    "\n",
    "        RS = []\n",
    "        ans = 0.0\n",
    "        for i, user_list in zip(item, sim_users):           \n",
    "            r=[]\n",
    "            for user in user_list:\n",
    "                 r.append(ui_matrix[user][i])\n",
    "            ans = ans + evall.ndcg_at_k(r, k, method=1)\n",
    "            RS.append(r)\n",
    "        G_at_k = ans/test_BATCH_SIZE\n",
    "        M_at_k = evall.mean_average_precision(RS)\n",
    "       \n",
    "\n",
    "        return p_at_k,G_at_k,M_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a795441-9745-4864-8ab9-a2428a5203f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "762/762 [==============================] - 533s 699ms/step - d_loss: -0.9645 - g_loss: 0.9640 - p10: 0.0152 - G10: 0.0343 - M10: 0.0242 - p20: 0.0126 - G20: 0.0426 - M20: 0.0248\n",
      "Epoch 2/100\n",
      "762/762 [==============================] - 533s 700ms/step - d_loss: -1.4538 - g_loss: 1.4529 - p10: 0.0152 - G10: 0.0341 - M10: 0.0239 - p20: 0.0144 - G20: 0.0437 - M20: 0.0245\n",
      "Epoch 3/100\n",
      "762/762 [==============================] - 543s 712ms/step - d_loss: -1.9093 - g_loss: 1.9087 - p10: 0.0152 - G10: 0.0342 - M10: 0.0240 - p20: 0.0135 - G20: 0.0388 - M20: 0.0235\n",
      "Epoch 4/100\n",
      "762/762 [==============================] - 530s 696ms/step - d_loss: -2.3437 - g_loss: 2.3434 - p10: 0.0121 - G10: 0.0332 - M10: 0.0230 - p20: 0.0128 - G20: 0.0422 - M20: 0.0234\n",
      "Epoch 5/100\n",
      "762/762 [==============================] - 535s 702ms/step - d_loss: -2.7622 - g_loss: 2.7621 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0424 - M20: 0.0239\n",
      "Epoch 6/100\n",
      "762/762 [==============================] - 535s 703ms/step - d_loss: -3.1690 - g_loss: 3.1689 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0144 - G20: 0.0433 - M20: 0.0243\n",
      "Epoch 7/100\n",
      "762/762 [==============================] - 538s 707ms/step - d_loss: -3.5674 - g_loss: 3.5674 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 8/100\n",
      "762/762 [==============================] - 540s 708ms/step - d_loss: -3.9603 - g_loss: 3.9604 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0241\n",
      "Epoch 9/100\n",
      "762/762 [==============================] - 547s 718ms/step - d_loss: -4.3494 - g_loss: 4.3495 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0241\n",
      "Epoch 10/100\n",
      "762/762 [==============================] - 535s 703ms/step - d_loss: -4.7359 - g_loss: 4.7360 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0241\n",
      "Epoch 11/100\n",
      "762/762 [==============================] - 534s 701ms/step - d_loss: -5.1208 - g_loss: 5.1209 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0241\n",
      "Epoch 12/100\n",
      "762/762 [==============================] - 534s 701ms/step - d_loss: -5.5051 - g_loss: 5.5052 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 13/100\n",
      "762/762 [==============================] - 535s 703ms/step - d_loss: -5.8886 - g_loss: 5.8887 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 14/100\n",
      "762/762 [==============================] - 534s 701ms/step - d_loss: -6.2715 - g_loss: 6.2716 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 15/100\n",
      "762/762 [==============================] - 543s 713ms/step - d_loss: -6.6541 - g_loss: 6.6542 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 16/100\n",
      "762/762 [==============================] - 534s 701ms/step - d_loss: -7.0363 - g_loss: 7.0364 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 17/100\n",
      "762/762 [==============================] - 535s 702ms/step - d_loss: -7.4183 - g_loss: 7.4184 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 18/100\n",
      "762/762 [==============================] - 532s 699ms/step - d_loss: -7.8002 - g_loss: 7.8003 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 19/100\n",
      "762/762 [==============================] - 534s 701ms/step - d_loss: -8.1820 - g_loss: 8.1821 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 20/100\n",
      "762/762 [==============================] - 539s 707ms/step - d_loss: -8.5636 - g_loss: 8.5637 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 21/100\n",
      "762/762 [==============================] - 545s 716ms/step - d_loss: -8.9453 - g_loss: 8.9454 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 22/100\n",
      "762/762 [==============================] - 536s 704ms/step - d_loss: -9.3269 - g_loss: 9.3270 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 23/100\n",
      "762/762 [==============================] - 537s 705ms/step - d_loss: -9.7085 - g_loss: 9.7086 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 24/100\n",
      "762/762 [==============================] - 538s 706ms/step - d_loss: -10.0900 - g_loss: 10.0901 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 25/100\n",
      "762/762 [==============================] - 538s 706ms/step - d_loss: -10.4716 - g_loss: 10.4717 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 26/100\n",
      "762/762 [==============================] - 540s 709ms/step - d_loss: -10.8531 - g_loss: 10.8532 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 27/100\n",
      "762/762 [==============================] - 545s 715ms/step - d_loss: -11.2347 - g_loss: 11.2348 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 28/100\n",
      "762/762 [==============================] - 539s 707ms/step - d_loss: -11.6162 - g_loss: 11.6163 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 29/100\n",
      "762/762 [==============================] - 537s 704ms/step - d_loss: -11.9977 - g_loss: 11.9978 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 30/100\n",
      "762/762 [==============================] - 537s 705ms/step - d_loss: -12.3792 - g_loss: 12.3793 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 31/100\n",
      "762/762 [==============================] - 539s 707ms/step - d_loss: -12.7608 - g_loss: 12.7609 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 32/100\n",
      "762/762 [==============================] - 543s 712ms/step - d_loss: -13.1423 - g_loss: 13.1424 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 33/100\n",
      "762/762 [==============================] - 542s 711ms/step - d_loss: -13.5238 - g_loss: 13.5239 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 34/100\n",
      "762/762 [==============================] - 538s 706ms/step - d_loss: -13.9053 - g_loss: 13.9054 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 35/100\n",
      "762/762 [==============================] - 537s 705ms/step - d_loss: -14.2868 - g_loss: 14.2869 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 36/100\n",
      "762/762 [==============================] - 535s 702ms/step - d_loss: -14.6684 - g_loss: 14.6685 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 37/100\n",
      "762/762 [==============================] - 538s 707ms/step - d_loss: -15.0499 - g_loss: 15.0500 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 38/100\n",
      "762/762 [==============================] - 544s 714ms/step - d_loss: -15.4314 - g_loss: 15.4315 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 39/100\n",
      "762/762 [==============================] - 537s 704ms/step - d_loss: -15.8129 - g_loss: 15.8130 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 40/100\n",
      "762/762 [==============================] - 532s 698ms/step - d_loss: -16.1944 - g_loss: 16.1945 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 41/100\n",
      "762/762 [==============================] - 532s 699ms/step - d_loss: -16.5760 - g_loss: 16.5761 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 42/100\n",
      "762/762 [==============================] - 533s 700ms/step - d_loss: -16.9575 - g_loss: 16.9576 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 43/100\n",
      "762/762 [==============================] - 532s 698ms/step - d_loss: -17.3370 - g_loss: 17.3371 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 44/100\n",
      "762/762 [==============================] - 541s 711ms/step - d_loss: -17.7149 - g_loss: 17.7150 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 45/100\n",
      "762/762 [==============================] - 532s 699ms/step - d_loss: -18.0928 - g_loss: 18.0929 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 46/100\n",
      "762/762 [==============================] - 533s 700ms/step - d_loss: -18.4707 - g_loss: 18.4708 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 47/100\n",
      "762/762 [==============================] - 532s 699ms/step - d_loss: -18.8485 - g_loss: 18.8486 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 48/100\n",
      "762/762 [==============================] - 536s 704ms/step - d_loss: -19.2264 - g_loss: 19.2265 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 49/100\n",
      "762/762 [==============================] - 540s 709ms/step - d_loss: -19.6043 - g_loss: 19.6044 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 50/100\n",
      "762/762 [==============================] - 535s 702ms/step - d_loss: -19.9822 - g_loss: 19.9823 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 51/100\n",
      "762/762 [==============================] - 530s 696ms/step - d_loss: -20.3601 - g_loss: 20.3602 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 52/100\n",
      "762/762 [==============================] - 531s 697ms/step - d_loss: -20.7380 - g_loss: 20.7381 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 53/100\n",
      "762/762 [==============================] - 531s 697ms/step - d_loss: -21.1158 - g_loss: 21.1159 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 54/100\n",
      "762/762 [==============================] - 531s 698ms/step - d_loss: -21.4937 - g_loss: 21.4938 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 55/100\n",
      "762/762 [==============================] - 540s 709ms/step - d_loss: -21.8716 - g_loss: 21.8717 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 56/100\n",
      "762/762 [==============================] - 530s 696ms/step - d_loss: -22.2495 - g_loss: 22.2496 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 57/100\n",
      "762/762 [==============================] - 529s 695ms/step - d_loss: -22.6274 - g_loss: 22.6275 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 58/100\n",
      "762/762 [==============================] - 530s 695ms/step - d_loss: -23.0053 - g_loss: 23.0054 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 59/100\n",
      "762/762 [==============================] - 531s 697ms/step - d_loss: -23.3832 - g_loss: 23.3833 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 60/100\n",
      "762/762 [==============================] - 539s 708ms/step - d_loss: -23.7610 - g_loss: 23.7611 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 61/100\n",
      "762/762 [==============================] - 531s 697ms/step - d_loss: -24.1389 - g_loss: 24.1390 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 62/100\n",
      "762/762 [==============================] - 529s 695ms/step - d_loss: -24.5168 - g_loss: 24.5169 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 63/100\n",
      "762/762 [==============================] - 530s 695ms/step - d_loss: -24.8947 - g_loss: 24.8948 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 64/100\n",
      "762/762 [==============================] - 520s 682ms/step - d_loss: -25.2726 - g_loss: 25.2727 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 65/100\n",
      "762/762 [==============================] - 486s 638ms/step - d_loss: -25.6505 - g_loss: 25.6506 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 66/100\n",
      "762/762 [==============================] - 485s 637ms/step - d_loss: -26.0283 - g_loss: 26.0284 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 67/100\n",
      "762/762 [==============================] - 482s 633ms/step - d_loss: -26.4062 - g_loss: 26.4063 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 68/100\n",
      "762/762 [==============================] - 482s 633ms/step - d_loss: -26.7841 - g_loss: 26.7842 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 69/100\n",
      "762/762 [==============================] - 484s 636ms/step - d_loss: -27.1620 - g_loss: 27.1621 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 70/100\n",
      "762/762 [==============================] - 483s 634ms/step - d_loss: -27.5399 - g_loss: 27.5400 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 71/100\n",
      "762/762 [==============================] - 489s 642ms/step - d_loss: -27.9178 - g_loss: 27.9179 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 72/100\n",
      "762/762 [==============================] - 482s 633ms/step - d_loss: -28.2956 - g_loss: 28.2957 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 73/100\n",
      "762/762 [==============================] - 483s 634ms/step - d_loss: -28.6735 - g_loss: 28.6736 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 74/100\n",
      "762/762 [==============================] - 482s 633ms/step - d_loss: -29.0514 - g_loss: 29.0515 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 75/100\n",
      "762/762 [==============================] - 485s 636ms/step - d_loss: -29.4293 - g_loss: 29.4294 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 76/100\n",
      "762/762 [==============================] - 490s 643ms/step - d_loss: -29.8072 - g_loss: 29.8073 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 77/100\n",
      "762/762 [==============================] - 486s 638ms/step - d_loss: -30.1851 - g_loss: 30.1852 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 78/100\n",
      "762/762 [==============================] - 484s 635ms/step - d_loss: -30.5629 - g_loss: 30.5630 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 79/100\n",
      "762/762 [==============================] - 485s 636ms/step - d_loss: -30.9408 - g_loss: 30.9409 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 80/100\n",
      "762/762 [==============================] - 489s 641ms/step - d_loss: -31.3187 - g_loss: 31.3188 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 81/100\n",
      "762/762 [==============================] - 485s 636ms/step - d_loss: -31.6966 - g_loss: 31.6967 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 82/100\n",
      "762/762 [==============================] - 492s 646ms/step - d_loss: -32.0745 - g_loss: 32.0746 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 83/100\n",
      "762/762 [==============================] - 486s 638ms/step - d_loss: -32.4524 - g_loss: 32.4525 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 84/100\n",
      "762/762 [==============================] - 486s 638ms/step - d_loss: -32.8303 - g_loss: 32.8304 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 85/100\n",
      "762/762 [==============================] - 486s 638ms/step - d_loss: -33.2081 - g_loss: 33.2082 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 86/100\n",
      "762/762 [==============================] - 496s 651ms/step - d_loss: -33.5860 - g_loss: 33.5861 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 87/100\n",
      "762/762 [==============================] - 541s 710ms/step - d_loss: -33.9639 - g_loss: 33.9640 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 88/100\n",
      "762/762 [==============================] - 535s 703ms/step - d_loss: -34.3418 - g_loss: 34.3419 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 89/100\n",
      "762/762 [==============================] - 532s 699ms/step - d_loss: -34.7197 - g_loss: 34.7198 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 90/100\n",
      "762/762 [==============================] - 533s 699ms/step - d_loss: -35.0976 - g_loss: 35.0977 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 91/100\n",
      "762/762 [==============================] - 536s 704ms/step - d_loss: -35.4754 - g_loss: 35.4755 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 92/100\n",
      "762/762 [==============================] - 534s 700ms/step - d_loss: -35.8533 - g_loss: 35.8534 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 93/100\n",
      "762/762 [==============================] - 544s 714ms/step - d_loss: -36.2312 - g_loss: 36.2313 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 94/100\n",
      "762/762 [==============================] - 532s 698ms/step - d_loss: -36.6091 - g_loss: 36.6092 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 95/100\n",
      "762/762 [==============================] - 532s 699ms/step - d_loss: -36.9870 - g_loss: 36.9871 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 96/100\n",
      "762/762 [==============================] - 534s 701ms/step - d_loss: -37.3649 - g_loss: 37.3650 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 97/100\n",
      "762/762 [==============================] - 533s 700ms/step - d_loss: -37.7427 - g_loss: 37.7428 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 98/100\n",
      "762/762 [==============================] - 540s 709ms/step - d_loss: -38.1206 - g_loss: 38.1207 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 99/100\n",
      "762/762 [==============================] - 536s 703ms/step - d_loss: -38.4985 - g_loss: 38.4986 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "Epoch 100/100\n",
      "762/762 [==============================] - 532s 699ms/step - d_loss: -38.8764 - g_loss: 38.8765 - p10: 0.0152 - G10: 0.0333 - M10: 0.0227 - p20: 0.0128 - G20: 0.0425 - M20: 0.0240\n",
      "(0.0152, 0.033327000199545216, 0.022681666666666666) \n",
      " (0.0128, 0.042475571384414844, 0.02403704833951738)\n"
     ]
    }
   ],
   "source": [
    "# Fit \n",
    "epochs = 100\n",
    "# Instantiate the WGAN model.\n",
    "wgan = WGAN(\n",
    "    discriminator=discriminator,\n",
    "    generator=generator,\n",
    "    discriminator_extra_steps=5\n",
    ")\n",
    "\n",
    "# Compile the WGAN model.\n",
    "wgan.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    g_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    g_loss_fn=generator_loss,\n",
    "    d_loss_fn=discriminator_loss,\n",
    "    c_loss_fn = counter_loss,\n",
    "    run_eagerly=True)\n",
    "\n",
    "# Start training the model.\n",
    "fit = wgan.fit(train, batch_size=100, epochs=epochs, verbose=True)\n",
    "print(wgan.test_step(10), \"\\n\", wgan.test_step(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48963e20-dabe-42ad-a78f-8d8c52fd5c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.Series(fit.history['d_loss'])\n",
    "g = pd.Series(fit.history['g_loss'])\n",
    "losses = pd.DataFrame(pd.concat([d,g], axis=1))\n",
    "losses.rename(columns={0:'d_loss', 1:'g_loss'})\n",
    "\n",
    "losses.to_csv(\"losses/wgan_100epoch_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be65322a-6950-4811-9a6a-a29d21a88273",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dic[\"wGAN_lr.0001\"] = [wgan.max_g10, wgan.max_g20, wgan.max_m10, wgan.max_m20, wgan.max_p10, wgan.max_p20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54b35303-4a26-4849-8a71-5e62cd0f6663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WGAN</th>\n",
       "      <th>WGAN-GP_100_epochs_lr_0.0001</th>\n",
       "      <th>GAN_lr.0001_extrastep_1</th>\n",
       "      <th>wGAN_lr.0001</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.431059</td>\n",
       "      <td>0.478360</td>\n",
       "      <td>0.250895</td>\n",
       "      <td>0.034278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.475948</td>\n",
       "      <td>0.524330</td>\n",
       "      <td>0.288934</td>\n",
       "      <td>0.043712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.354883</td>\n",
       "      <td>0.394489</td>\n",
       "      <td>0.200178</td>\n",
       "      <td>0.024224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.378315</td>\n",
       "      <td>0.421206</td>\n",
       "      <td>0.216806</td>\n",
       "      <td>0.024797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.311600</td>\n",
       "      <td>0.350700</td>\n",
       "      <td>0.179600</td>\n",
       "      <td>0.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.338500</td>\n",
       "      <td>0.375900</td>\n",
       "      <td>0.203800</td>\n",
       "      <td>0.014400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       WGAN  WGAN-GP_100_epochs_lr_0.0001  GAN_lr.0001_extrastep_1  \\\n",
       "0  0.431059                      0.478360                 0.250895   \n",
       "1  0.475948                      0.524330                 0.288934   \n",
       "2  0.354883                      0.394489                 0.200178   \n",
       "3  0.378315                      0.421206                 0.216806   \n",
       "4  0.311600                      0.350700                 0.179600   \n",
       "5  0.338500                      0.375900                 0.203800   \n",
       "\n",
       "   wGAN_lr.0001  \n",
       "0      0.034278  \n",
       "1      0.043712  \n",
       "2      0.024224  \n",
       "3      0.024797  \n",
       "4      0.015200  \n",
       "5      0.014400  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "464894ee-3929-4e59-b27e-c581a472c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dic.to_csv(\"Eval_dic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
