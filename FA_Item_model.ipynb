{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e0a10c-4174-4783-9ace-de3bb716c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fa_support\n",
    "import evall\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e97fbd8-5b2e-416f-8f7f-7e84878e5cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train =  np.load(\"new_fa_train_data.npy\")\n",
    "#test = np.load(\"new_fa_test_data.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc568ea-3d90-4926-acc3-cdce612554d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [  308, 8, 53, 3, 13, 5, 3, 78]\n",
    "names = ['brand_id',\n",
    "         'category', 'colour', 'divisioncode', 'itemcategorycode',\n",
    "         'itemfamilycode', 'itemseason', 'productgroup']\n",
    "dic = dict(zip(names,numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdcb1a2a-24f2-4e1c-aebe-860e17e36168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_emb_dim = sum(dic.values())\n",
    "\n",
    "D_emb_dim = 50\n",
    "\n",
    "G_emb_dim = 50\n",
    "\n",
    "hidden_dim = 128\n",
    "alpha = 0\n",
    "\n",
    "# Initializer\n",
    "init = tf.initializers.glorot_normal()\n",
    "\n",
    "'''Generator and Discriminator Attribute Embeddings'''\n",
    "#D_price_embs = tf.keras.layers.Embedding(input_dim = dic['pricetype'], output_dim = D_emb_dim,\n",
    " #                                         trainable=True, weights = [init(shape=( dic['pricetype'],D_emb_dim))])\n",
    "D_brand_embs = tf.keras.layers.Embedding(input_dim = dic['brand_id'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['brand_id'],D_emb_dim))])\n",
    "D_category_embs = tf.keras.layers.Embedding(input_dim = dic['category'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['category'],D_emb_dim))])\n",
    "D_colour_embs = tf.keras.layers.Embedding(input_dim = dic['colour'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['colour'],D_emb_dim))])\n",
    "D_div_embs = tf.keras.layers.Embedding(input_dim = dic['divisioncode'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['divisioncode'],D_emb_dim))])\n",
    "D_itemcat_embs = tf.keras.layers.Embedding(input_dim = dic['itemcategorycode'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemcategorycode'],D_emb_dim))])\n",
    "D_itemfam_embs = tf.keras.layers.Embedding(input_dim = dic['itemfamilycode'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemfamilycode'],D_emb_dim))])\n",
    "D_season_embs = tf.keras.layers.Embedding(input_dim = dic['itemseason'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemseason'],D_emb_dim))])\n",
    "D_prod_embs = tf.keras.layers.Embedding(input_dim = dic['productgroup'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['productgroup'],D_emb_dim))])\n",
    "\n",
    "\n",
    "#G_price_embs = tf.keras.layers.Embedding(input_dim = dic['pricetype'], output_dim = G_emb_dim,\n",
    "#                                               trainable=True, weights = [init(shape=( dic['pricetype'],G_emb_dim))])\n",
    "G_brand_embs = tf.keras.layers.Embedding(input_dim = dic['brand_id'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['brand_id'],G_emb_dim))])\n",
    "G_category_embs = tf.keras.layers.Embedding(input_dim = dic['category'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['category'],G_emb_dim))])\n",
    "G_colour_embs = tf.keras.layers.Embedding(input_dim = dic['colour'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['colour'],G_emb_dim))])\n",
    "G_div_embs = tf.keras.layers.Embedding(input_dim = dic['divisioncode'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['divisioncode'],G_emb_dim))])\n",
    "G_itemcat_embs = tf.keras.layers.Embedding(input_dim = dic['itemcategorycode'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemcategorycode'],G_emb_dim))])\n",
    "G_itemfam_embs = tf.keras.layers.Embedding(input_dim = dic['itemfamilycode'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemfamilycode'],G_emb_dim))])\n",
    "G_season_embs = tf.keras.layers.Embedding(input_dim = dic['itemseason'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemseason'],G_emb_dim))])\n",
    "G_prod_embs = tf.keras.layers.Embedding(input_dim = dic['productgroup'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['productgroup'],G_emb_dim))])\n",
    "\n",
    "\n",
    "# Model input sizes\n",
    "G_input_size =  G_emb_dim*8\n",
    "D_input_size = user_emb_dim + D_emb_dim*8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cb359a8-a0ce-4f2f-ac47-21cadfc1cbde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generator_input( brand_id, category, colour, divisioncode, itemcategorycode, itemfamilycode, itemseason, productgroup):\n",
    "    emb_dic = {}\n",
    "#    dic[\"pricetype\"] = G_brand_embs(tf.constant(pricetype))\n",
    "    dic[\"brand_id\"] = G_brand_embs(tf.constant(brand_id))\n",
    "    dic[\"category\"] = G_brand_embs(tf.constant(category))\n",
    "    dic[\"colour\"] = G_brand_embs(tf.constant(colour))\n",
    "    dic[\"divisioncode\"] = G_brand_embs(tf.constant(divisioncode))\n",
    "    dic[\"itemcategorycode\"] = G_brand_embs(tf.constant(itemcategorycode))\n",
    "    dic[\"itemfamilycode\"] = G_brand_embs(tf.constant(itemfamilycode))\n",
    "    dic[\"itemseason\"] = G_brand_embs(tf.constant(itemseason))\n",
    "    dic[\"productgroup\"] = G_brand_embs(tf.constant(productgroup))\n",
    "    emb = tf.keras.layers.concatenate(list(dic.values()), 1)\n",
    "    return emb\n",
    "\n",
    "# Generates user based on concatenation of all attributes\n",
    "def generator():\n",
    "    bc_input = tf.keras.layers.Input(shape=(G_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', activity_regularizer = 'l2')(bc_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', activity_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(user_emb_dim, activation ='sigmoid', activity_regularizer = 'l2')(x)\n",
    "    g_model = tf.keras.models.Model(bc_input, x, name = 'generator')\n",
    "    return g_model\n",
    "g_model = generator()\n",
    "\n",
    "def discriminator_input( brand_id, category, colour, divisioncode, itemcategorycode, itemfamilycode, \n",
    "                        itemseason, productgroup, user_emb):\n",
    "    emb_dic = {}\n",
    "#    dic[\"pricetype\"]  = D_brand_embs(tf.constant(pricetype))\n",
    "    dic[\"brand_id\"]  = D_brand_embs(tf.constant(brand_id))\n",
    "    dic[\"category\"]  = D_brand_embs(tf.constant(category))\n",
    "    dic[\"colour\"]  = D_brand_embs(tf.constant(colour))\n",
    "    dic[\"divisioncode\"]  = D_brand_embs(tf.constant(divisioncode))\n",
    "    dic[\"itemcategorycode\"]  = D_brand_embs(tf.constant(itemcategorycode))\n",
    "    dic[\"itemfamilycode\"]  = D_brand_embs(tf.constant(itemfamilycode))\n",
    "    dic[\"itemseason\"]  = D_brand_embs(tf.constant(itemseason))\n",
    "    dic[\"productgroup\"]  = D_brand_embs(tf.constant(productgroup))\n",
    "    user_emb = tf.cast(user_emb, dtype=float)\n",
    "    emb = tf.keras.layers.concatenate(list(dic.values()), 1)\n",
    "    final_emb = tf.keras.layers.concatenate([emb, user_emb], 1)\n",
    "    return final_emb\n",
    "\n",
    "def discriminator():\n",
    "    d_input = tf.keras.layers.Input(shape=(D_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', kernel_regularizer = 'l2')(d_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.models.Model(d_input, x, name = 'discriminator')\n",
    "    return model\n",
    "d_model = discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3be7b975-da1c-42da-9145-a0607a2d5939",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua_matrix = np.load(\"new_fa_ua_matrix.npy\")\n",
    "ia_matrix = np.load(\"new_fa_ia_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a0f07a3-6974-4d34-96d6-f1b880872aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Cross entropy loss means\\ndef generator_loss(d_logits):\\n    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits, labels=tf.ones_like(d_logits)))\\n\\ndef discriminator_loss(real, fake):\\n    logit = tf.reduce_mean(fake-real)\\n    r = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real, labels=tf.ones_like(real)))\\n    f = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake, labels=tf.zeros_like(fake)))\\n    return r+f\\n\\ndef counter_loss(counter):\\n    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=counter, labels=tf.zeros_like(counter))) \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Loss functions'''\n",
    "def generator_loss(fake_user):\n",
    "    return -tf.reduce_mean(fake_user)\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    logit = tf.reduce_mean(fake- real)\n",
    "    return logit\n",
    "\n",
    "def counter_loss(counter):\n",
    "    return tf.reduce_mean(counter)\n",
    "\n",
    "def discriminator_counter_loss(real, fake, counter):\n",
    "    logit = tf.reduce_mean(real - counter - fake)\n",
    "    return logit\n",
    "''' Cross entropy loss means\n",
    "def generator_loss(d_logits):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits, labels=tf.ones_like(d_logits)))\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    logit = tf.reduce_mean(fake-real)\n",
    "    r = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real, labels=tf.ones_like(real)))\n",
    "    f = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake, labels=tf.zeros_like(fake)))\n",
    "    return r+f\n",
    "\n",
    "def counter_loss(counter):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=counter, labels=tf.zeros_like(counter))) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2057738-8692-443c-bea3-22c549ecd24d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WGAN Class\n",
    "class WGAN(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        discriminator_extra_steps=5,\n",
    "        batch_size = 1000\n",
    "    ):\n",
    "        super(WGAN, self).__init__()\n",
    "        self.discriminator = d_model\n",
    "        self.generator = g_model\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.k = 10        \n",
    "        self.index = 0 \n",
    "        self.c_index = 0 \n",
    "        self.gp_weight = 10\n",
    "        self.eval_steps = 0\n",
    "        self.max_p10 = .3 \n",
    "        self.max_g10 = .3\n",
    "        self.max_m10 = .3\n",
    "        self.max_p20 = .3\n",
    "        self.max_g20 = .3\n",
    "        self.max_m20 = .3\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn,c_loss_fn, run_eagerly):\n",
    "        super(WGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.c_loss_fn = c_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "        self.run_eagerly = run_eagerly\n",
    "\n",
    "\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_users, fake_users,  brand_id,\\\n",
    "                                    category, colour, divisioncode, itemcategorycode, itemfamilycode, \\\n",
    "                                    itemseason, productgroup):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated image\n",
    "        alpha = tf.random.normal([batch_size,1], 0.0, 1.0)\n",
    "        diff = fake_users - real_users\n",
    "        interpolated = real_users + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            interpolated_input = discriminator_input(   brand_id,\\\n",
    "                                    category, colour, divisioncode, itemcategorycode, itemfamilycode, \\\n",
    "                                    itemseason, productgroup, interpolated)\n",
    "            pred = self.discriminator(interpolated_input)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = tape.gradient(pred, [interpolated])[0] #+1e-10\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, real_users):\n",
    "        self.eval_steps +=1 \n",
    "        c_batch_size = 2*self.batch_size\n",
    "        for i in range(self.d_steps):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Get batch data\n",
    "                country, postcode, loyal, gender,  brand_id, category, colour, divisioncode, \\\n",
    "                itemcategorycode, itemfamilycode, itemseason, productgroup, real_users, items = fa_support.get_batchdata(self.index, self.index + self.batch_size)\n",
    "                # Get batch of counter examples\n",
    "                '''counter_brand_id, counter_category, counter_colour, counter_divisioncode, \\\n",
    "                counter_itemcategorycode, counter_itemfamilycode, counter_itemseason, \\\n",
    "                counter_productgroup,  counter_users = fa_support.get_counter_batch(self.c_index, self.c_index + c_batch_size)'''\n",
    "                # Generate fake users from attributes\n",
    "                g_input0 = generator_input(  brand_id, category, colour, divisioncode, \\\n",
    "                                           itemcategorycode, itemfamilycode, itemseason, productgroup)\n",
    "                fake_users = self.generator(g_input0)\n",
    "                # Get the logits for the fake users\n",
    "                d_input0 = discriminator_input( brand_id, category, colour, divisioncode, \\\n",
    "                                           itemcategorycode, itemfamilycode, itemseason, productgroup, fake_users)\n",
    "                fake_logits = self.discriminator(d_input0)\n",
    "                # Get the logits for the real user\n",
    "                d_input1 = discriminator_input(  brand_id, category, colour, divisioncode, \\\n",
    "                                           itemcategorycode, itemfamilycode, itemseason, productgroup, real_users)\n",
    "                real_logits = self.discriminator(d_input1)\n",
    "                # Get logits for counter examples\n",
    "                \n",
    "                '''d_input2 = discriminator_input( counter_brand_id, counter_category, counter_colour, counter_divisioncode, \\\n",
    "                counter_itemcategorycode, counter_itemfamilycode, counter_itemseason, \\\n",
    "                counter_productgroup,  counter_users)\n",
    "                counter_logits = self.discriminator(d_input2)'''\n",
    "                # Calculate the discriminator loss using the fake and real image logits\n",
    "                d_cost = self.d_loss_fn(real_logits, fake_logits)\n",
    "                #c_loss = self.c_loss_fn(counter_logits)\n",
    "                # Get gradient penalty\n",
    "                gp = self.gradient_penalty(self.batch_size, real_users, fake_users, brand_id,\\\n",
    "                                    category, colour, divisioncode, itemcategorycode, itemfamilycode, \\\n",
    "                                    itemseason, productgroup)\n",
    "                # Later add counter loss\n",
    "                d_loss = d_cost + gp*self.gp_weight # + c_loss\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(zip(d_gradient, self.discriminator.trainable_variables))\n",
    "\n",
    "        # Train the generator\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Generate fake images using the generator\n",
    "            g_input1 = generator_input(  brand_id, category, colour, divisioncode, \\\n",
    "                                           itemcategorycode, itemfamilycode, itemseason, productgroup)\n",
    "            gen_users = self.generator(g_input1)\n",
    "            # Get the discriminator logits for fake images\n",
    "            d_input2 = discriminator_input(  brand_id, category, colour, divisioncode, \\\n",
    "                                           itemcategorycode, itemfamilycode, itemseason, productgroup, gen_users)\n",
    "            gen_logits = self.discriminator(d_input2)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        if self.eval_steps %760==0:\n",
    "            p_at_10,G_at_10,M_at_10 = wgan.test_step(10)\n",
    "            p_at_20,G_at_20,M_at_20 = wgan.test_step(20)\n",
    "            if p_at_10 > self.max_p10:\n",
    "                self.max_p10 = p_at_10\n",
    "            if G_at_10 > self.max_g10:\n",
    "                self.max_g10 = G_at_10\n",
    "            if M_at_10 > self.max_m10:\n",
    "                self.max_m10 = M_at_10\n",
    "            if p_at_20 > self.max_p20:\n",
    "                self.max_p20 = p_at_20\n",
    "            if G_at_20 > self.max_g20:\n",
    "                self.max_g20 = G_at_20\n",
    "            if M_at_20 > self.max_m20:\n",
    "                self.max_m20 = M_at_20\n",
    "            \n",
    "            return {\"d_loss\": d_loss, \"g_loss\": g_loss, \"p10\":p_at_10,\n",
    "                           \"G10\":G_at_10,\"M10\":M_at_10, \"p20\": p_at_20,\"G20\":G_at_20,\"M20\":M_at_20}\n",
    "        else:\n",
    "            return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "\n",
    "    def test_step(self, k):\n",
    "        country, postcode, loyal, gender,  brand_id, category, colour, divisioncode, \\\n",
    "        itemcategorycode, itemfamilycode, itemseason, productgroup, item = fa_support.get_testdata(0,5000)\n",
    "        \n",
    "        test_BATCH_SIZE = country.size\n",
    "        g_input1 = generator_input( brand_id, category, colour, divisioncode, \\\n",
    "                                           itemcategorycode, itemfamilycode, itemseason, productgroup)\n",
    "        gen_users = self.generator(g_input1)\n",
    "        sim_users = fa_support.get_intersection_similar_user(gen_users, k)\n",
    "        count = 0\n",
    "        for i, userlist in zip(item, sim_users):       \n",
    "            for u in userlist:\n",
    "                if np.sum(ia_matrix[i] + ua_matrix[u] == 2) ==8:\n",
    "                    count = count + 1            \n",
    "        p_at_10 = round(count/(test_BATCH_SIZE * k), 4)\n",
    "\n",
    "        ans = 0.0\n",
    "        RS = []\n",
    "        for i, userlist in zip(item, sim_users):  \n",
    "            r=[]\n",
    "            for u in userlist:\n",
    "                if np.sum(ia_matrix[i] + ua_matrix[u] == 2) ==8:\n",
    "                    r.append(1)\n",
    "                else:\n",
    "                    r.append(0)\n",
    "            RS.append(r)\n",
    "            ans = ans + evall.ndcg_at_k(r, k, method=1)\n",
    "        G_at_10 = ans/test_BATCH_SIZE\n",
    "        M_at_10 = evall.mean_average_precision(RS)\n",
    "\n",
    "        return p_at_10,G_at_10,M_at_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f43892b4-5bfe-407d-9758-088738919ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "762/762 [==============================] - 282s 371ms/step - d_loss: -2.0680 - g_loss: -3.4180 - p10: 0.0635 - G10: 0.1632 - M10: 0.1312 - p20: 0.0490 - G20: 0.1800 - M20: 0.1286\n",
      "Epoch 2/50\n",
      "762/762 [==============================] - 285s 374ms/step - d_loss: -1.5877 - g_loss: -9.3851 - p10: 0.1317 - G10: 0.2494 - M10: 0.2040 - p20: 0.1217 - G20: 0.2682 - M20: 0.1983\n",
      "Epoch 3/50\n",
      "762/762 [==============================] - 285s 375ms/step - d_loss: -1.2013 - g_loss: -10.0814 - p10: 0.2098 - G10: 0.3348 - M10: 0.2874 - p20: 0.2021 - G20: 0.3577 - M20: 0.2803\n",
      "Epoch 4/50\n",
      "762/762 [==============================] - 286s 375ms/step - d_loss: -0.9304 - g_loss: -10.8972 - p10: 0.3151 - G10: 0.4431 - M10: 0.3898 - p20: 0.3064 - G20: 0.4646 - M20: 0.3820\n",
      "Epoch 5/50\n",
      "762/762 [==============================] - 286s 376ms/step - d_loss: -0.7007 - g_loss: -12.3077 - p10: 0.4187 - G10: 0.5313 - M10: 0.4872 - p20: 0.4016 - G20: 0.5515 - M20: 0.4825\n",
      "Epoch 6/50\n",
      "762/762 [==============================] - 286s 376ms/step - d_loss: -0.5725 - g_loss: -14.1474 - p10: 0.4643 - G10: 0.5755 - M10: 0.5348 - p20: 0.4425 - G20: 0.5955 - M20: 0.5320\n",
      "Epoch 7/50\n",
      "762/762 [==============================] - 286s 376ms/step - d_loss: -0.4749 - g_loss: -15.8425 - p10: 0.5218 - G10: 0.6276 - M10: 0.5904 - p20: 0.4942 - G20: 0.6464 - M20: 0.5858\n",
      "Epoch 8/50\n",
      "762/762 [==============================] - 286s 376ms/step - d_loss: -0.3727 - g_loss: -18.2919 - p10: 0.5433 - G10: 0.6449 - M10: 0.6091 - p20: 0.5111 - G20: 0.6624 - M20: 0.6059\n",
      "Epoch 9/50\n",
      "762/762 [==============================] - 287s 377ms/step - d_loss: -0.3851 - g_loss: -17.6782 - p10: 0.5567 - G10: 0.6560 - M10: 0.6217 - p20: 0.5229 - G20: 0.6738 - M20: 0.6200\n",
      "Epoch 10/50\n",
      "762/762 [==============================] - 320s 420ms/step - d_loss: -0.3646 - g_loss: -18.5809 - p10: 0.5612 - G10: 0.6589 - M10: 0.6251 - p20: 0.5272 - G20: 0.6774 - M20: 0.6240\n",
      "Epoch 11/50\n",
      "762/762 [==============================] - 321s 421ms/step - d_loss: -0.3533 - g_loss: -19.8402 - p10: 0.5656 - G10: 0.6669 - M10: 0.6330 - p20: 0.5278 - G20: 0.6859 - M20: 0.6330\n",
      "Epoch 12/50\n",
      "762/762 [==============================] - 289s 380ms/step - d_loss: -0.3338 - g_loss: -21.3101 - p10: 0.5670 - G10: 0.6674 - M10: 0.6351 - p20: 0.5296 - G20: 0.6863 - M20: 0.6346\n",
      "Epoch 13/50\n",
      "762/762 [==============================] - 289s 380ms/step - d_loss: -0.3227 - g_loss: -22.5997 - p10: 0.5707 - G10: 0.6694 - M10: 0.6365 - p20: 0.5314 - G20: 0.6890 - M20: 0.6372\n",
      "Epoch 14/50\n",
      "762/762 [==============================] - 291s 381ms/step - d_loss: -0.3180 - g_loss: -23.6508 - p10: 0.5728 - G10: 0.6701 - M10: 0.6374 - p20: 0.5349 - G20: 0.6911 - M20: 0.6388\n",
      "Epoch 15/50\n",
      "762/762 [==============================] - 298s 391ms/step - d_loss: -0.3125 - g_loss: -24.3019 - p10: 0.5761 - G10: 0.6744 - M10: 0.6425 - p20: 0.5360 - G20: 0.6933 - M20: 0.6432\n",
      "Epoch 16/50\n",
      "729/762 [===========================>..] - ETA: 11s - d_loss: -0.3068 - g_loss: -24.8687"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.22 GiB for an array with shape (5000, 86391) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c4c6c493ff51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Start training the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;34m\"\"\"Runs a training execution with one step.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstep_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m       outputs = reduce_per_replica(\n\u001b[1;32m    847\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n",
      "\u001b[0;32m~/thesis/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1283\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1284\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1285\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2831\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2832\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2833\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2835\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3606\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3607\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3608\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3610\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m         \u001b[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-93b6cd350a7a>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, real_users)\u001b[0m\n\u001b[1;32m    130\u001b[0m         )\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_steps\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;36m760\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mp_at_10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG_at_10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mM_at_10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0mp_at_20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG_at_20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mM_at_20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mp_at_10\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_p10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-93b6cd350a7a>\u001b[0m in \u001b[0;36mtest_step\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    158\u001b[0m                                            itemcategorycode, itemfamilycode, itemseason, productgroup)\n\u001b[1;32m    159\u001b[0m         \u001b[0mgen_users\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_input1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0msim_users\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfa_support\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_intersection_similar_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserlist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_users\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Thesis-Code/fa_support.py\u001b[0m in \u001b[0;36mget_intersection_similar_user\u001b[0;34m(G_user, k)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_intersection_similar_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0muser_emb_matrixT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_emb_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mintersection_rank_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_emb_matrixT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mintersection_rank_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 3.22 GiB for an array with shape (5000, 86391) and data type float64"
     ]
    }
   ],
   "source": [
    "# Fit \n",
    "epochs = 50\n",
    "# Instantiate the WGAN model.\n",
    "wgan = WGAN(\n",
    "    discriminator=discriminator,\n",
    "    generator=generator,\n",
    "    discriminator_extra_steps=3\n",
    ")\n",
    "\n",
    "# Compile the WGAN model.\n",
    "wgan.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    g_optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    g_loss_fn=generator_loss,\n",
    "    d_loss_fn=discriminator_loss,\n",
    "    c_loss_fn = counter_loss,\n",
    "    run_eagerly=True)\n",
    "\n",
    "# Start training the model.\n",
    "fit = wgan.fit(train, batch_size=100, epochs=epochs, verbose=True)\n",
    "print(wgan.test_step(10), \"\\n\", wgan.test_step(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08e6b048-5a11-4ff4-a7e2-6bf2618ac0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgan.max_p20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
