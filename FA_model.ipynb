{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24e0a10c-4174-4783-9ace-de3bb716c67d",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.72 GiB for an array with shape (767283908,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-691e101ce673>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfa_support\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mevall\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Thesis-Code/fa_support.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#ia_matrix = np.load(\"fa_ia_matrix.npy\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0muser_emb_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fa_ua_matrix.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_popular_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_memmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0m\u001b[1;32m    440\u001b[0m                                          pickle_kwargs=pickle_kwargs)\n\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0;31m# We can use the fast fromfile() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0;31m# This is not a real file. We have to read it the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 5.72 GiB for an array with shape (767283908,) and data type int64"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fa_support import *\n",
    "from evall import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc568ea-3d90-4926-acc3-cdce612554d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [ 2, 335, 8, 59, 3, 13, 7, 3, 78]\n",
    "names = ['pricetype', 'brand_id',\n",
    "         'category', 'colour', 'divisioncode', 'itemcategorycode',\n",
    "         'itemfamilycode', 'itemseason', 'productgroup']\n",
    "dic = dict(zip(names,numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdcb1a2a-24f2-4e1c-aebe-860e17e36168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "user_emb_dim = sum(dic.values())\n",
    "\n",
    "D_emb_dim = 50\n",
    "\n",
    "G_emb_dim = 50\n",
    "\n",
    "hidden_dim = 128\n",
    "alpha = 0\n",
    "\n",
    "# Initializer\n",
    "init = tf.initializers.glorot_normal()\n",
    "\n",
    "'''Generator and Discriminator Attribute Embeddings'''\n",
    "D_price_embs = tf.keras.layers.Embedding(input_dim = dic['pricetype'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['pricetype'],D_emb_dim))])\n",
    "D_brand_embs = tf.keras.layers.Embedding(input_dim = dic['brand_id'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['brand_id'],D_emb_dim))])\n",
    "D_category_embs = tf.keras.layers.Embedding(input_dim = dic['category'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['category'],D_emb_dim))])\n",
    "D_colour_embs = tf.keras.layers.Embedding(input_dim = dic['colour'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['colour'],D_emb_dim))])\n",
    "D_div_embs = tf.keras.layers.Embedding(input_dim = dic['divisioncode'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['divisioncode'],D_emb_dim))])\n",
    "D_itemcat_embs = tf.keras.layers.Embedding(input_dim = dic['itemcategorycode'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemcategorycode'],D_emb_dim))])\n",
    "D_itemfam_embs = tf.keras.layers.Embedding(input_dim = dic['itemfamilycode'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemfamilycode'],D_emb_dim))])\n",
    "D_season_embs = tf.keras.layers.Embedding(input_dim = dic['itemseason'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemseason'],D_emb_dim))])\n",
    "D_prod_embs = tf.keras.layers.Embedding(input_dim = dic['productgroup'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['productgroup'],D_emb_dim))])\n",
    "\n",
    "\n",
    "G_price_embs = tf.keras.layers.Embedding(input_dim = dic['pricetype'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['pricetype'],G_emb_dim))])\n",
    "G_brand_embs = tf.keras.layers.Embedding(input_dim = dic['brand_id'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['brand_id'],G_emb_dim))])\n",
    "G_category_embs = tf.keras.layers.Embedding(input_dim = dic['category'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['category'],G_emb_dim))])\n",
    "G_colour_embs = tf.keras.layers.Embedding(input_dim = dic['colour'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['colour'],G_emb_dim))])\n",
    "G_div_embs = tf.keras.layers.Embedding(input_dim = dic['divisioncode'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['divisioncode'],G_emb_dim))])\n",
    "G_itemcat_embs = tf.keras.layers.Embedding(input_dim = dic['itemcategorycode'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemcategorycode'],G_emb_dim))])\n",
    "G_itemfam_embs = tf.keras.layers.Embedding(input_dim = dic['itemfamilycode'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemfamilycode'],G_emb_dim))])\n",
    "G_season_embs = tf.keras.layers.Embedding(input_dim = dic['itemseason'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemseason'],G_emb_dim))])\n",
    "G_prod_embs = tf.keras.layers.Embedding(input_dim = dic['productgroup'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['productgroup'],G_emb_dim))])\n",
    "\n",
    "\n",
    "# Model input sizes\n",
    "G_input_size =  G_emb_dim*9\n",
    "D_input_size = user_emb_dim + D_emb_dim*9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cb359a8-a0ce-4f2f-ac47-21cadfc1cbde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generator_input(pricetype, brand_id, category, colour, divisioncode, itemcategorycode, itemfamilycode, itemseason, productgroup):\n",
    "    emb_dic = {}\n",
    "    dic[emb1] = G_brand_embs(tf.constant(pricetype))\n",
    "    dic[emb2] = G_brand_embs(tf.constant(brand_id))\n",
    "    dic[emb3] = G_brand_embs(tf.constant(category))\n",
    "    dic[emb4] = G_brand_embs(tf.constant(colour))\n",
    "    dic[emb5] = G_brand_embs(tf.constant(divisioncode))\n",
    "    dic[emb6] = G_brand_embs(tf.constant(itemcategorycode))\n",
    "    dic[emb7] = G_brand_embs(tf.constant(itemfamilycode))\n",
    "    dic[emb8] = G_brand_embs(tf.constant(itemseason))\n",
    "    dic[emb9] = G_brand_embs(tf.constant(productgroup))\n",
    "    emb = tf.keras.layers.concatenate([dic.values()], 1)\n",
    "    return emb\n",
    "\n",
    "# Generates user based on concatenation of all attributes\n",
    "def generator():\n",
    "    bc_input = tf.keras.layers.Input(shape=(G_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', activity_regularizer = 'l2')(bc_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', activity_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(user_emb_dim, activation ='sigmoid', activity_regularizer = 'l2')(x)\n",
    "    g_model = tf.keras.models.Model(bc_input, x, name = 'generator')\n",
    "    return g_model\n",
    "g_model = generator()\n",
    "\n",
    "def discriminator_input(pricetype, brand_id, category, colour, divisioncode, itemcategorycode, itemfamilycode, \n",
    "                        itemseason, productgroup, user_emb):\n",
    "    emb_dic = {}\n",
    "    dic[emb1] = D_brand_embs(tf.constant(pricetype))\n",
    "    dic[emb2] = D_brand_embs(tf.constant(brand_id))\n",
    "    dic[emb3] = D_brand_embs(tf.constant(category))\n",
    "    dic[emb4] = D_brand_embs(tf.constant(colour))\n",
    "    dic[emb5] = D_brand_embs(tf.constant(divisioncode))\n",
    "    dic[emb6] = D_brand_embs(tf.constant(itemcategorycode))\n",
    "    dic[emb7] = D_brand_embs(tf.constant(itemfamilycode))\n",
    "    dic[emb8] = D_brand_embs(tf.constant(itemseason))\n",
    "    dic[emb9] = D_brand_embs(tf.constant(productgroup))\n",
    "    dic[user_emb] = tf.cast(user_emb, dtype=float)\n",
    "    emb = tf.keras.layers.concatenate([dic.values()], 1)\n",
    "    return emb\n",
    "\n",
    "def discriminator():\n",
    "    d_input = tf.keras.layers.Input(shape=(D_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', kernel_regularizer = 'l2')(d_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.models.Model(d_input, x, name = 'discriminator')\n",
    "    return model\n",
    "d_model = discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c3db7-a97d-45ac-ad05-0481b939dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "country, postcode,  pricetype, loyal, gender,  brand_id,\\ \n",
    "category, colour, divisioncode, itemcategorycode, itemfamilycode, \\\n",
    "        itemseason, productgroup, user_emb_batch = fa_support.get_batchdata(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a0f07a3-6974-4d34-96d6-f1b880872aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Cross entropy loss means\\ndef generator_loss(d_logits):\\n    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits, labels=tf.ones_like(d_logits)))\\n\\ndef discriminator_loss(real, fake):\\n    logit = tf.reduce_mean(fake-real)\\n    r = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real, labels=tf.ones_like(real)))\\n    f = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake, labels=tf.zeros_like(fake)))\\n    return r+f\\n\\ndef counter_loss(counter):\\n    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=counter, labels=tf.zeros_like(counter))) \\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Loss functions'''\n",
    "def generator_loss(fake_user):\n",
    "    return -tf.reduce_mean(fake_user)\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    logit = tf.reduce_mean(fake- real)\n",
    "    return logit\n",
    "\n",
    "def counter_loss(counter):\n",
    "    return tf.reduce_mean(counter)\n",
    "\n",
    "def discriminator_counter_loss(real, fake, counter):\n",
    "    logit = tf.reduce_mean(real - counter - fake)\n",
    "    return logit\n",
    "''' Cross entropy loss means\n",
    "def generator_loss(d_logits):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits, labels=tf.ones_like(d_logits)))\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    logit = tf.reduce_mean(fake-real)\n",
    "    r = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real, labels=tf.ones_like(real)))\n",
    "    f = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake, labels=tf.zeros_like(fake)))\n",
    "    return r+f\n",
    "\n",
    "def counter_loss(counter):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=counter, labels=tf.zeros_like(counter))) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2057738-8692-443c-bea3-22c549ecd24d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WGAN Class\n",
    "class WGAN(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        discriminator_extra_steps=5,\n",
    "        batch_size = 577\n",
    "    ):\n",
    "        super(WGAN, self).__init__()\n",
    "        self.discriminator = d_model\n",
    "        self.generator = g_model\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.k = 10\n",
    "        self.ucb_matrix = ucb_matrix\n",
    "        self.ui_matrix = ui_matrix\n",
    "        self.sim = support.get_intersection_similar_user\n",
    "        self.index = 0 \n",
    "        self.c_index = 0 \n",
    "        self.gp_weight = 10\n",
    "        self.eval_steps = 0\n",
    "        self.max_p10 = .3 \n",
    "        self.max_g10 = .3\n",
    "        self.max_m10 = .3\n",
    "        self.max_p20 = .3\n",
    "        self.max_g20 = .3\n",
    "        self.max_m20 = 0.3\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn,c_loss_fn, run_eagerly):\n",
    "        super(WGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.c_loss_fn = c_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "        self.run_eagerly = run_eagerly\n",
    "\n",
    "\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_users, fake_users, brand_id, class_id):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated image\n",
    "        alpha = tf.random.normal([batch_size,1], 0.0, 1.0)\n",
    "        diff = fake_users - real_users\n",
    "        interpolated = real_users + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            interpolated_input = discriminator_input(brand_id, class_id, interpolated)\n",
    "            pred = self.discriminator(interpolated_input)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = tape.gradient(pred, [interpolated])[0] #+1e-10\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, real_users):\n",
    "        self.eval_steps +=1 \n",
    "        c_batch_size = 2*self.batch_size\n",
    "        for i in range(self.d_steps):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Get batch data\n",
    "                item_id, brand_id, class_id, real_users = support.get_batchdata(self.index, self.index + self.batch_size)\n",
    "                # Get batch of counter examples\n",
    "                counter_brand_id, counter_class_id, counter_users = support.get_counter_batch(self.c_index,\n",
    "                                                                                            self.c_index + c_batch_size)\n",
    "                # Generate fake users from attributes\n",
    "                g_input0 = generator_input(brand_id, class_id)\n",
    "                fake_users = self.generator(g_input0)\n",
    "                # Get the logits for the fake users\n",
    "                d_input0 = discriminator_input(brand_id, class_id, fake_users)\n",
    "                fake_logits = self.discriminator(d_input0)\n",
    "                # Get the logits for the real user\n",
    "                d_input1 = discriminator_input(brand_id, class_id, real_users)\n",
    "                real_logits = self.discriminator(d_input1)\n",
    "                # Get logits for counter examples\n",
    "                d_input2 = discriminator_input(counter_brand_id, counter_class_id, counter_users)\n",
    "                counter_logits = self.discriminator(d_input2)\n",
    "                # Calculate the discriminator loss using the fake and real image logits\n",
    "                d_cost = self.d_loss_fn(real_logits, fake_logits)\n",
    "                c_loss = self.c_loss_fn(counter_logits)\n",
    "                # Get gradient penalty\n",
    "                gp = self.gradient_penalty(self.batch_size, real_users, fake_users, brand_id, class_id)\n",
    "                # Later add counter loss\n",
    "                d_loss = d_cost +c_loss + gp*self.gp_weight\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(zip(d_gradient, self.discriminator.trainable_variables))\n",
    "\n",
    "        # Train the generator\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Generate fake images using the generator\n",
    "            g_input1 = generator_input(brand_id, class_id)\n",
    "            gen_users = self.generator(g_input1)\n",
    "            # Get the discriminator logits for fake images\n",
    "            d_input2 = discriminator_input(brand_id, class_id, gen_users)\n",
    "            gen_logits = self.discriminator(d_input2)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        if self.eval_steps %1000==0:\n",
    "            p_at_10,G_at_10,M_at_10 = wgan.test_step(10)\n",
    "            p_at_20,G_at_20,M_at_20 = wgan.test_step(20)\n",
    "            if p_at_10 > self.max_p10:\n",
    "                self.max_p10 = p_at_10\n",
    "            if G_at_10 > self.max_g10:\n",
    "                self.max_g10 = G_at_10\n",
    "            if M_at_10 > self.max_m10:\n",
    "                self.max_m10 = M_at_10\n",
    "            if p_at_20 > self.max_p10:\n",
    "                self.max_p10 = p_at_20\n",
    "            if G_at_20 > self.max_g10:\n",
    "                self.max_g10 = G_at_20\n",
    "            if M_at_20 > self.max_m10:\n",
    "                self.max_m10 = M_at_20\n",
    "            \n",
    "            return {\"d_loss\": d_loss, \"g_loss\": g_loss, \"p10\":p_at_10,\n",
    "                           \"G10\":G_at_10,\"M10\":M_at_10, \"p20\": p_at_20,\"G20\":G_at_20,\"M20\":M_at_20}\n",
    "        else:\n",
    "            return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "\n",
    "    def test_step(self, k):\n",
    "        item_id, brand_id, class_id = support.get_testdata()\n",
    "        test_BATCH_SIZE = item_id.size\n",
    "        g_input1 = generator_input(brand_id, class_id)\n",
    "        gen_users = self.generator(g_input1)\n",
    "        sim_users = support.get_intersection_similar_user(gen_users, k )\n",
    "        count = 0\n",
    "        for i, userlist in zip(item_id, sim_users):       \n",
    "            for u in userlist:\n",
    "                if np.sum(ia_matrix[i] + ua_matrix[u] == 2) >=9:\n",
    "                    count = count + 1            \n",
    "        p_at_10 = round(count/(test_BATCH_SIZE * k), 4)\n",
    "\n",
    "        ans = 0.0\n",
    "        RS = []\n",
    "        for i, userlist in zip(item_id, sim_users):  \n",
    "            r=[]\n",
    "            for u in test_userlist:\n",
    "                if np.sum(ia_matrix[i] + ua_matrix[u] == 2) >=9:\n",
    "                    r.append(1)\n",
    "                else:\n",
    "                    r.append(0)\n",
    "            RS.append(r)\n",
    "            ans = ans + evall.ndcg_at_k(r, k, method=1)\n",
    "        G_at_10 = ans/test_BATCH_SIZE\n",
    "        M_at_10 = evall.mean_average_precision(RS)\n",
    "\n",
    "        return p_at_10,G_at_10,M_at_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43892b4-5bfe-407d-9758-088738919ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "577/577 [==============================] - 318s 551ms/step - d_loss: -37.7928 - g_loss: 36.9667 - p10: 0.0298 - G10: 0.1120 - M10: 0.0743 - p20: 0.0253 - G20: 0.1411 - M20: 0.0791\n",
      "Epoch 2/500\n",
      "577/577 [==============================] - 319s 552ms/step - d_loss: -115.2350 - g_loss: 114.8199 - p10: 0.0200 - G10: 0.0880 - M10: 0.0634 - p20: 0.0214 - G20: 0.1285 - M20: 0.0738\n",
      "Epoch 3/500\n",
      "577/577 [==============================] - 319s 554ms/step - d_loss: -203.8494 - g_loss: 202.8002 - p10: 0.0260 - G10: 0.1047 - M10: 0.0699 - p20: 0.0256 - G20: 0.1461 - M20: 0.0794\n",
      "Epoch 4/500\n",
      "577/577 [==============================] - 318s 552ms/step - d_loss: -294.8037 - g_loss: 293.0713 - p10: 0.0270 - G10: 0.1058 - M10: 0.0707 - p20: 0.0258 - G20: 0.1478 - M20: 0.0803\n",
      "Epoch 5/500\n",
      "577/577 [==============================] - 320s 555ms/step - d_loss: -386.7396 - g_loss: 384.7028 - p10: 0.0270 - G10: 0.1101 - M10: 0.0730 - p20: 0.0267 - G20: 0.1470 - M20: 0.0794\n",
      "Epoch 6/500\n",
      "577/577 [==============================] - 319s 553ms/step - d_loss: -480.0174 - g_loss: 477.7333 - p10: 0.0270 - G10: 0.1117 - M10: 0.0742 - p20: 0.0256 - G20: 0.1436 - M20: 0.0803\n",
      "Epoch 7/500\n",
      "577/577 [==============================] - 321s 556ms/step - d_loss: -573.2464 - g_loss: 570.7737 - p10: 0.0284 - G10: 0.1126 - M10: 0.0727 - p20: 0.0263 - G20: 0.1461 - M20: 0.0786\n",
      "Epoch 8/500\n",
      "577/577 [==============================] - 322s 559ms/step - d_loss: -666.1812 - g_loss: 663.5163 - p10: 0.0358 - G10: 0.1317 - M10: 0.0825 - p20: 0.0309 - G20: 0.1673 - M20: 0.0903\n",
      "Epoch 9/500\n",
      "577/577 [==============================] - 320s 555ms/step - d_loss: -759.0845 - g_loss: 756.2144 - p10: 0.0395 - G10: 0.1420 - M10: 0.0887 - p20: 0.0328 - G20: 0.1670 - M20: 0.0906\n",
      "Epoch 10/500\n",
      "577/577 [==============================] - 323s 559ms/step - d_loss: -852.0047 - g_loss: 848.9302 - p10: 0.0391 - G10: 0.1511 - M10: 0.1009 - p20: 0.0365 - G20: 0.1782 - M20: 0.0964\n",
      "Epoch 11/500\n",
      "577/577 [==============================] - 323s 560ms/step - d_loss: -944.9404 - g_loss: 941.6520 - p10: 0.0381 - G10: 0.1555 - M10: 0.1059 - p20: 0.0358 - G20: 0.1840 - M20: 0.1051\n",
      "Epoch 12/500\n",
      "577/577 [==============================] - 321s 556ms/step - d_loss: -1037.8804 - g_loss: 1034.3645 - p10: 0.0377 - G10: 0.1520 - M10: 0.1055 - p20: 0.0365 - G20: 0.1941 - M20: 0.1084\n",
      "Epoch 13/500\n",
      "577/577 [==============================] - 323s 559ms/step - d_loss: -1130.8260 - g_loss: 1127.0841 - p10: 0.0377 - G10: 0.1524 - M10: 0.1053 - p20: 0.0342 - G20: 0.1909 - M20: 0.1110\n",
      "Epoch 14/500\n",
      "577/577 [==============================] - 323s 559ms/step - d_loss: -1223.7842 - g_loss: 1219.8108 - p10: 0.0377 - G10: 0.1419 - M10: 0.0934 - p20: 0.0328 - G20: 0.1768 - M20: 0.0962\n",
      "Epoch 15/500\n",
      "577/577 [==============================] - 324s 562ms/step - d_loss: -1316.7481 - g_loss: 1312.5277 - p10: 0.0358 - G10: 0.1398 - M10: 0.0935 - p20: 0.0335 - G20: 0.1800 - M20: 0.0999\n",
      "Epoch 16/500\n",
      "577/577 [==============================] - 323s 561ms/step - d_loss: -1409.7150 - g_loss: 1405.2393 - p10: 0.0335 - G10: 0.1357 - M10: 0.0930 - p20: 0.0328 - G20: 0.1760 - M20: 0.0972\n",
      "Epoch 17/500\n",
      "577/577 [==============================] - 322s 558ms/step - d_loss: -1502.6802 - g_loss: 1497.9613 - p10: 0.0363 - G10: 0.1397 - M10: 0.0938 - p20: 0.0340 - G20: 0.1824 - M20: 0.1018\n",
      "Epoch 18/500\n",
      "577/577 [==============================] - 323s 560ms/step - d_loss: -1595.6607 - g_loss: 1590.6598 - p10: 0.0367 - G10: 0.1462 - M10: 0.0996 - p20: 0.0328 - G20: 0.1764 - M20: 0.1012\n",
      "Epoch 19/500\n",
      "577/577 [==============================] - 332s 575ms/step - d_loss: -1688.6475 - g_loss: 1683.3443 - p10: 0.0363 - G10: 0.1452 - M10: 0.0983 - p20: 0.0379 - G20: 0.1905 - M20: 0.1044\n",
      "Epoch 20/500\n",
      "577/577 [==============================] - 332s 575ms/step - d_loss: -1781.6362 - g_loss: 1776.0159 - p10: 0.0386 - G10: 0.1416 - M10: 0.0914 - p20: 0.0363 - G20: 0.1828 - M20: 0.0987\n",
      "Epoch 21/500\n",
      "577/577 [==============================] - 330s 573ms/step - d_loss: -1874.6238 - g_loss: 1868.6723 - p10: 0.0400 - G10: 0.1550 - M10: 0.1055 - p20: 0.0398 - G20: 0.1984 - M20: 0.1124\n",
      "Epoch 22/500\n",
      "577/577 [==============================] - 330s 571ms/step - d_loss: -1967.6038 - g_loss: 1961.3183 - p10: 0.0377 - G10: 0.1496 - M10: 0.0973 - p20: 0.0377 - G20: 0.1942 - M20: 0.1050\n",
      "Epoch 23/500\n",
      "577/577 [==============================] - 332s 576ms/step - d_loss: -2060.6045 - g_loss: 2053.9736 - p10: 0.0372 - G10: 0.1419 - M10: 0.0950 - p20: 0.0356 - G20: 0.1819 - M20: 0.0992\n",
      "Epoch 24/500\n",
      "577/577 [==============================] - 330s 572ms/step - d_loss: -2153.5875 - g_loss: 2146.6006 - p10: 0.0358 - G10: 0.1418 - M10: 0.0966 - p20: 0.0367 - G20: 0.1882 - M20: 0.1047\n",
      "Epoch 25/500\n",
      "577/577 [==============================] - 330s 571ms/step - d_loss: -2246.5787 - g_loss: 2239.2355 - p10: 0.0372 - G10: 0.1397 - M10: 0.0954 - p20: 0.0335 - G20: 0.1779 - M20: 0.1012\n",
      "Epoch 26/500\n",
      "577/577 [==============================] - 330s 573ms/step - d_loss: -2339.5709 - g_loss: 2331.8573 - p10: 0.0372 - G10: 0.1558 - M10: 0.1140 - p20: 0.0344 - G20: 0.1904 - M20: 0.1191\n",
      "Epoch 27/500\n",
      "577/577 [==============================] - 332s 576ms/step - d_loss: -2432.5452 - g_loss: 2424.4776 - p10: 0.0363 - G10: 0.1613 - M10: 0.1132 - p20: 0.0379 - G20: 0.2068 - M20: 0.1167\n",
      "Epoch 28/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -2525.5561 - g_loss: 2517.1168 - p10: 0.0330 - G10: 0.1365 - M10: 0.0954 - p20: 0.0330 - G20: 0.1847 - M20: 0.1051\n",
      "Epoch 29/500\n",
      "577/577 [==============================] - 330s 572ms/step - d_loss: -2618.5409 - g_loss: 2609.7528 - p10: 0.0340 - G10: 0.1310 - M10: 0.0901 - p20: 0.0326 - G20: 0.1708 - M20: 0.0963\n",
      "Epoch 30/500\n",
      "577/577 [==============================] - 330s 572ms/step - d_loss: -2711.5504 - g_loss: 2702.3882 - p10: 0.0400 - G10: 0.1498 - M10: 0.1047 - p20: 0.0349 - G20: 0.1964 - M20: 0.1147\n",
      "Epoch 31/500\n",
      "577/577 [==============================] - 327s 567ms/step - d_loss: -2804.5194 - g_loss: 2794.9604 - p10: 0.0363 - G10: 0.1399 - M10: 0.0909 - p20: 0.0358 - G20: 0.1771 - M20: 0.0950\n",
      "Epoch 32/500\n",
      "577/577 [==============================] - 330s 571ms/step - d_loss: -2897.5232 - g_loss: 2887.6063 - p10: 0.0353 - G10: 0.1323 - M10: 0.0911 - p20: 0.0337 - G20: 0.1719 - M20: 0.0953\n",
      "Epoch 33/500\n",
      "577/577 [==============================] - 328s 569ms/step - d_loss: -2990.5123 - g_loss: 2980.2357 - p10: 0.0367 - G10: 0.1346 - M10: 0.0887 - p20: 0.0340 - G20: 0.1734 - M20: 0.0957\n",
      "Epoch 34/500\n",
      "577/577 [==============================] - 329s 570ms/step - d_loss: -3083.5137 - g_loss: 3072.8653 - p10: 0.0358 - G10: 0.1255 - M10: 0.0776 - p20: 0.0347 - G20: 0.1673 - M20: 0.0860\n",
      "Epoch 35/500\n",
      "577/577 [==============================] - 330s 571ms/step - d_loss: -3176.4795 - g_loss: 3165.4583 - p10: 0.0349 - G10: 0.1461 - M10: 0.1031 - p20: 0.0335 - G20: 0.1833 - M20: 0.1053\n",
      "Epoch 36/500\n",
      "577/577 [==============================] - 330s 571ms/step - d_loss: -3269.4804 - g_loss: 3258.0950 - p10: 0.0409 - G10: 0.1532 - M10: 0.1033 - p20: 0.0370 - G20: 0.1894 - M20: 0.1102\n",
      "Epoch 37/500\n",
      "577/577 [==============================] - 329s 570ms/step - d_loss: -3362.4601 - g_loss: 3350.7049 - p10: 0.0405 - G10: 0.1466 - M10: 0.0991 - p20: 0.0379 - G20: 0.1796 - M20: 0.1011\n",
      "Epoch 38/500\n",
      "577/577 [==============================] - 330s 572ms/step - d_loss: -3455.4877 - g_loss: 3443.3711 - p10: 0.0363 - G10: 0.1339 - M10: 0.0880 - p20: 0.0316 - G20: 0.1717 - M20: 0.0961\n",
      "Epoch 39/500\n",
      "577/577 [==============================] - 330s 572ms/step - d_loss: -3548.4522 - g_loss: 3535.9572 - p10: 0.0372 - G10: 0.1446 - M10: 0.0956 - p20: 0.0342 - G20: 0.1740 - M20: 0.0960\n",
      "Epoch 40/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -3641.4554 - g_loss: 3628.5961 - p10: 0.0344 - G10: 0.1532 - M10: 0.1154 - p20: 0.0337 - G20: 0.1927 - M20: 0.1159\n",
      "Epoch 41/500\n",
      "577/577 [==============================] - 329s 570ms/step - d_loss: -3734.4322 - g_loss: 3721.2255 - p10: 0.0423 - G10: 0.1678 - M10: 0.1153 - p20: 0.0393 - G20: 0.2073 - M20: 0.1192\n",
      "Epoch 42/500\n",
      "577/577 [==============================] - 329s 569ms/step - d_loss: -3827.4136 - g_loss: 3813.8202 - p10: 0.0381 - G10: 0.1276 - M10: 0.0833 - p20: 0.0349 - G20: 0.1701 - M20: 0.0899\n",
      "Epoch 43/500\n",
      "577/577 [==============================] - 332s 575ms/step - d_loss: -3920.4386 - g_loss: 3906.4984 - p10: 0.0316 - G10: 0.1305 - M10: 0.0878 - p20: 0.0300 - G20: 0.1643 - M20: 0.0961\n",
      "Epoch 44/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -4013.3926 - g_loss: 3999.0759 - p10: 0.0386 - G10: 0.1473 - M10: 0.0974 - p20: 0.0335 - G20: 0.1780 - M20: 0.0978\n",
      "Epoch 45/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -4106.5866 - g_loss: 4091.9212 - p10: 0.0335 - G10: 0.1315 - M10: 0.0924 - p20: 0.0344 - G20: 0.1756 - M20: 0.0971\n",
      "Epoch 46/500\n",
      "577/577 [==============================] - 332s 575ms/step - d_loss: -4199.8310 - g_loss: 4184.9956 - p10: 0.0377 - G10: 0.1489 - M10: 0.1002 - p20: 0.0377 - G20: 0.1788 - M20: 0.0963\n",
      "Epoch 47/500\n",
      "577/577 [==============================] - 329s 571ms/step - d_loss: -4293.5599 - g_loss: 4278.0791 - p10: 0.0442 - G10: 0.1470 - M10: 0.0902 - p20: 0.0347 - G20: 0.1809 - M20: 0.0973\n",
      "Epoch 48/500\n",
      "577/577 [==============================] - 332s 576ms/step - d_loss: -4387.3141 - g_loss: 4371.4814 - p10: 0.0395 - G10: 0.1567 - M10: 0.1080 - p20: 0.0335 - G20: 0.1865 - M20: 0.1123\n",
      "Epoch 49/500\n",
      "577/577 [==============================] - 330s 572ms/step - d_loss: -4481.0608 - g_loss: 4464.8787 - p10: 0.0358 - G10: 0.1514 - M10: 0.1112 - p20: 0.0305 - G20: 0.1770 - M20: 0.1085\n",
      "Epoch 50/500\n",
      "577/577 [==============================] - 330s 571ms/step - d_loss: -4574.7399 - g_loss: 4558.1934 - p10: 0.0372 - G10: 0.1317 - M10: 0.0881 - p20: 0.0351 - G20: 0.1742 - M20: 0.0960\n",
      "Epoch 51/500\n",
      "577/577 [==============================] - 330s 573ms/step - d_loss: -4668.4751 - g_loss: 4651.5798 - p10: 0.0409 - G10: 0.1575 - M10: 0.1071 - p20: 0.0405 - G20: 0.1901 - M20: 0.1053\n",
      "Epoch 52/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -4762.1197 - g_loss: 4744.8690 - p10: 0.0367 - G10: 0.1402 - M10: 0.0956 - p20: 0.0335 - G20: 0.1717 - M20: 0.0962\n",
      "Epoch 53/500\n",
      "577/577 [==============================] - 330s 573ms/step - d_loss: -4855.8158 - g_loss: 4838.1947 - p10: 0.0363 - G10: 0.1415 - M10: 0.0966 - p20: 0.0291 - G20: 0.1708 - M20: 0.1037\n",
      "Epoch 54/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -4949.5406 - g_loss: 4931.5701 - p10: 0.0386 - G10: 0.1461 - M10: 0.1002 - p20: 0.0337 - G20: 0.1780 - M20: 0.1049\n",
      "Epoch 55/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -5043.2364 - g_loss: 5024.8928 - p10: 0.0358 - G10: 0.1275 - M10: 0.0806 - p20: 0.0356 - G20: 0.1652 - M20: 0.0859\n",
      "Epoch 56/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -5136.9850 - g_loss: 5118.2939 - p10: 0.0381 - G10: 0.1456 - M10: 0.1022 - p20: 0.0337 - G20: 0.1772 - M20: 0.1003\n",
      "Epoch 57/500\n",
      "577/577 [==============================] - 330s 572ms/step - d_loss: -5230.7132 - g_loss: 5211.6703 - p10: 0.0386 - G10: 0.1379 - M10: 0.0881 - p20: 0.0351 - G20: 0.1718 - M20: 0.0933\n",
      "Epoch 58/500\n",
      "577/577 [==============================] - 330s 571ms/step - d_loss: -5324.3099 - g_loss: 5305.0131 - p10: 0.0391 - G10: 0.1585 - M10: 0.1093 - p20: 0.0356 - G20: 0.1827 - M20: 0.1078\n",
      "Epoch 59/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -5418.0336 - g_loss: 5398.2957 - p10: 0.0372 - G10: 0.1455 - M10: 0.0986 - p20: 0.0328 - G20: 0.1740 - M20: 0.1006\n",
      "Epoch 60/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -5511.7750 - g_loss: 5491.6515 - p10: 0.0344 - G10: 0.1364 - M10: 0.0936 - p20: 0.0314 - G20: 0.1677 - M20: 0.0952\n",
      "Epoch 61/500\n",
      "577/577 [==============================] - 330s 572ms/step - d_loss: -5605.4779 - g_loss: 5585.0080 - p10: 0.0386 - G10: 0.1579 - M10: 0.1109 - p20: 0.0328 - G20: 0.1847 - M20: 0.1108\n",
      "Epoch 62/500\n",
      "577/577 [==============================] - 331s 573ms/step - d_loss: -5699.1447 - g_loss: 5678.3417 - p10: 0.0386 - G10: 0.1457 - M10: 0.0985 - p20: 0.0363 - G20: 0.1802 - M20: 0.1012\n",
      "Epoch 63/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -5791.7442 - g_loss: 5771.3272 - p10: 0.0349 - G10: 0.1307 - M10: 0.0881 - p20: 0.0305 - G20: 0.1572 - M20: 0.0924\n",
      "Epoch 64/500\n",
      "577/577 [==============================] - 329s 571ms/step - d_loss: -5884.9988 - g_loss: 5864.4721 - p10: 0.0307 - G10: 0.1215 - M10: 0.0790 - p20: 0.0279 - G20: 0.1546 - M20: 0.0857\n",
      "Epoch 65/500\n",
      "577/577 [==============================] - 331s 573ms/step - d_loss: -5978.8025 - g_loss: 5957.5339 - p10: 0.0307 - G10: 0.1340 - M10: 0.0954 - p20: 0.0300 - G20: 0.1668 - M20: 0.0973\n",
      "Epoch 66/500\n",
      "577/577 [==============================] - 333s 577ms/step - d_loss: -6072.6928 - g_loss: 6050.8266 - p10: 0.0284 - G10: 0.1098 - M10: 0.0774 - p20: 0.0305 - G20: 0.1553 - M20: 0.0880\n",
      "Epoch 67/500\n",
      "577/577 [==============================] - 332s 575ms/step - d_loss: -6166.5038 - g_loss: 6144.1507 - p10: 0.0386 - G10: 0.1438 - M10: 0.1018 - p20: 0.0347 - G20: 0.1797 - M20: 0.1031\n",
      "Epoch 68/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -6260.3468 - g_loss: 6237.5384 - p10: 0.0381 - G10: 0.1528 - M10: 0.1030 - p20: 0.0321 - G20: 0.1756 - M20: 0.1008\n",
      "Epoch 69/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -6354.2068 - g_loss: 6331.0281 - p10: 0.0274 - G10: 0.1267 - M10: 0.0910 - p20: 0.0270 - G20: 0.1536 - M20: 0.0887\n",
      "Epoch 70/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -6447.9707 - g_loss: 6424.4014 - p10: 0.0284 - G10: 0.1146 - M10: 0.0748 - p20: 0.0281 - G20: 0.1463 - M20: 0.0793\n",
      "Epoch 71/500\n",
      "577/577 [==============================] - 331s 573ms/step - d_loss: -6541.7779 - g_loss: 6517.8125 - p10: 0.0409 - G10: 0.1672 - M10: 0.1190 - p20: 0.0328 - G20: 0.1915 - M20: 0.1188\n",
      "Epoch 72/500\n",
      "577/577 [==============================] - 329s 570ms/step - d_loss: -6635.6141 - g_loss: 6611.2841 - p10: 0.0349 - G10: 0.1311 - M10: 0.0882 - p20: 0.0288 - G20: 0.1598 - M20: 0.0927\n",
      "Epoch 73/500\n",
      "577/577 [==============================] - 330s 572ms/step - d_loss: -6729.3311 - g_loss: 6704.6610 - p10: 0.0302 - G10: 0.1217 - M10: 0.0853 - p20: 0.0314 - G20: 0.1627 - M20: 0.0932\n",
      "Epoch 74/500\n",
      "577/577 [==============================] - 329s 571ms/step - d_loss: -6823.0166 - g_loss: 6798.0004 - p10: 0.0312 - G10: 0.1364 - M10: 0.0976 - p20: 0.0312 - G20: 0.1713 - M20: 0.0999\n",
      "Epoch 75/500\n",
      "577/577 [==============================] - 330s 573ms/step - d_loss: -6916.7700 - g_loss: 6891.3750 - p10: 0.0372 - G10: 0.1358 - M10: 0.0947 - p20: 0.0335 - G20: 0.1711 - M20: 0.0973\n",
      "Epoch 76/500\n",
      "577/577 [==============================] - 333s 577ms/step - d_loss: -7010.4713 - g_loss: 6984.7152 - p10: 0.0298 - G10: 0.1137 - M10: 0.0822 - p20: 0.0291 - G20: 0.1496 - M20: 0.0864\n",
      "Epoch 77/500\n",
      "577/577 [==============================] - 329s 570ms/step - d_loss: -7104.2560 - g_loss: 7078.1446 - p10: 0.0312 - G10: 0.1212 - M10: 0.0826 - p20: 0.0351 - G20: 0.1708 - M20: 0.0878\n",
      "Epoch 78/500\n",
      "577/577 [==============================] - 335s 581ms/step - d_loss: -7197.9596 - g_loss: 7171.5136 - p10: 0.0372 - G10: 0.1430 - M10: 0.1021 - p20: 0.0353 - G20: 0.1762 - M20: 0.1024\n",
      "Epoch 79/500\n",
      "577/577 [==============================] - 325s 563ms/step - d_loss: -7291.6423 - g_loss: 7264.8517 - p10: 0.0358 - G10: 0.1297 - M10: 0.0814 - p20: 0.0326 - G20: 0.1653 - M20: 0.0864\n",
      "Epoch 80/500\n",
      "577/577 [==============================] - 326s 565ms/step - d_loss: -7385.1499 - g_loss: 7358.0546 - p10: 0.0353 - G10: 0.1510 - M10: 0.1081 - p20: 0.0328 - G20: 0.1871 - M20: 0.1124\n",
      "Epoch 81/500\n",
      "577/577 [==============================] - 324s 562ms/step - d_loss: -7478.9810 - g_loss: 7451.4824 - p10: 0.0349 - G10: 0.1475 - M10: 0.1025 - p20: 0.0326 - G20: 0.1836 - M20: 0.1059\n",
      "Epoch 82/500\n",
      "577/577 [==============================] - 327s 568ms/step - d_loss: -7572.7619 - g_loss: 7544.9236 - p10: 0.0338 - G10: 0.1348 - M10: 0.0947 - p20: 0.0316 - G20: 0.1700 - M20: 0.1005\n",
      "Epoch 83/500\n",
      "577/577 [==============================] - 323s 560ms/step - d_loss: -7666.4758 - g_loss: 7638.2935 - p10: 0.0330 - G10: 0.1316 - M10: 0.0896 - p20: 0.0337 - G20: 0.1747 - M20: 0.0967\n",
      "Epoch 84/500\n",
      "577/577 [==============================] - 324s 562ms/step - d_loss: -7760.1675 - g_loss: 7731.6223 - p10: 0.0372 - G10: 0.1328 - M10: 0.0832 - p20: 0.0328 - G20: 0.1698 - M20: 0.0913\n",
      "Epoch 85/500\n",
      "577/577 [==============================] - 324s 561ms/step - d_loss: -7853.9022 - g_loss: 7825.0017 - p10: 0.0363 - G10: 0.1274 - M10: 0.0806 - p20: 0.0300 - G20: 0.1553 - M20: 0.0859\n",
      "Epoch 86/500\n",
      "577/577 [==============================] - 324s 561ms/step - d_loss: -7947.3876 - g_loss: 7918.2239 - p10: 0.0358 - G10: 0.1331 - M10: 0.0868 - p20: 0.0323 - G20: 0.1669 - M20: 0.0932\n",
      "Epoch 87/500\n",
      "577/577 [==============================] - 323s 560ms/step - d_loss: -8041.0782 - g_loss: 8011.4913 - p10: 0.0372 - G10: 0.1273 - M10: 0.0809 - p20: 0.0326 - G20: 0.1596 - M20: 0.0885\n",
      "Epoch 88/500\n",
      "577/577 [==============================] - 323s 559ms/step - d_loss: -8134.7846 - g_loss: 8104.8859 - p10: 0.0335 - G10: 0.1317 - M10: 0.0887 - p20: 0.0316 - G20: 0.1655 - M20: 0.0928\n",
      "Epoch 89/500\n",
      "577/577 [==============================] - 324s 562ms/step - d_loss: -8228.5020 - g_loss: 8198.2070 - p10: 0.0353 - G10: 0.1408 - M10: 0.0950 - p20: 0.0323 - G20: 0.1679 - M20: 0.0953\n",
      "Epoch 90/500\n",
      "577/577 [==============================] - 323s 560ms/step - d_loss: -8322.2697 - g_loss: 8291.6334 - p10: 0.0349 - G10: 0.1197 - M10: 0.0801 - p20: 0.0333 - G20: 0.1507 - M20: 0.0798\n",
      "Epoch 91/500\n",
      "577/577 [==============================] - 323s 560ms/step - d_loss: -8415.9914 - g_loss: 8385.0232 - p10: 0.0386 - G10: 0.1334 - M10: 0.0885 - p20: 0.0333 - G20: 0.1692 - M20: 0.0975\n",
      "Epoch 92/500\n",
      "577/577 [==============================] - 323s 560ms/step - d_loss: -8509.6357 - g_loss: 8478.3078 - p10: 0.0395 - G10: 0.1512 - M10: 0.1091 - p20: 0.0377 - G20: 0.1951 - M20: 0.1166\n",
      "Epoch 93/500\n",
      "577/577 [==============================] - 324s 562ms/step - d_loss: -8603.3732 - g_loss: 8571.6815 - p10: 0.0353 - G10: 0.1334 - M10: 0.0926 - p20: 0.0321 - G20: 0.1579 - M20: 0.0932\n",
      "Epoch 94/500\n",
      "577/577 [==============================] - 322s 558ms/step - d_loss: -8697.1242 - g_loss: 8665.0741 - p10: 0.0293 - G10: 0.1128 - M10: 0.0775 - p20: 0.0335 - G20: 0.1676 - M20: 0.0910\n",
      "Epoch 95/500\n",
      "577/577 [==============================] - 323s 560ms/step - d_loss: -8790.8135 - g_loss: 8758.4159 - p10: 0.0316 - G10: 0.1226 - M10: 0.0849 - p20: 0.0328 - G20: 0.1641 - M20: 0.0939\n",
      "Epoch 96/500\n",
      "577/577 [==============================] - 323s 560ms/step - d_loss: -8884.2041 - g_loss: 8851.5745 - p10: 0.0284 - G10: 0.1114 - M10: 0.0788 - p20: 0.0272 - G20: 0.1478 - M20: 0.0872\n",
      "Epoch 97/500\n",
      "577/577 [==============================] - 327s 567ms/step - d_loss: -8977.7370 - g_loss: 8944.8575 - p10: 0.0363 - G10: 0.1423 - M10: 0.0949 - p20: 0.0307 - G20: 0.1730 - M20: 0.1005\n",
      "Epoch 98/500\n",
      "577/577 [==============================] - 328s 569ms/step - d_loss: -9071.6271 - g_loss: 9038.2956 - p10: 0.0405 - G10: 0.1447 - M10: 0.0969 - p20: 0.0340 - G20: 0.1705 - M20: 0.0985\n",
      "Epoch 99/500\n",
      "577/577 [==============================] - 323s 560ms/step - d_loss: -9165.3546 - g_loss: 9131.6850 - p10: 0.0381 - G10: 0.1381 - M10: 0.0967 - p20: 0.0353 - G20: 0.1711 - M20: 0.1027\n",
      "Epoch 100/500\n",
      "577/577 [==============================] - 322s 559ms/step - d_loss: -9259.0745 - g_loss: 9225.0439 - p10: 0.0377 - G10: 0.1484 - M10: 0.1015 - p20: 0.0323 - G20: 0.1803 - M20: 0.1066\n",
      "Epoch 101/500\n",
      "577/577 [==============================] - 323s 559ms/step - d_loss: -9352.8156 - g_loss: 9318.4326 - p10: 0.0367 - G10: 0.1334 - M10: 0.0883 - p20: 0.0353 - G20: 0.1722 - M20: 0.0929\n",
      "Epoch 102/500\n",
      "577/577 [==============================] - 322s 557ms/step - d_loss: -9446.4912 - g_loss: 9411.7666 - p10: 0.0302 - G10: 0.1136 - M10: 0.0823 - p20: 0.0316 - G20: 0.1520 - M20: 0.0845\n",
      "Epoch 103/500\n",
      "577/577 [==============================] - 324s 561ms/step - d_loss: -9540.2317 - g_loss: 9505.1344 - p10: 0.0353 - G10: 0.1291 - M10: 0.0901 - p20: 0.0326 - G20: 0.1634 - M20: 0.0943\n",
      "Epoch 104/500\n",
      "577/577 [==============================] - 323s 561ms/step - d_loss: -9633.9350 - g_loss: 9598.4890 - p10: 0.0405 - G10: 0.1470 - M10: 0.1000 - p20: 0.0367 - G20: 0.1792 - M20: 0.1062\n",
      "Epoch 105/500\n",
      "577/577 [==============================] - 323s 560ms/step - d_loss: -9727.5935 - g_loss: 9691.8046 - p10: 0.0307 - G10: 0.1398 - M10: 0.1010 - p20: 0.0344 - G20: 0.1709 - M20: 0.0920\n",
      "Epoch 106/500\n",
      "577/577 [==============================] - 324s 562ms/step - d_loss: -9821.3708 - g_loss: 9785.2150 - p10: 0.0307 - G10: 0.1280 - M10: 0.0910 - p20: 0.0314 - G20: 0.1697 - M20: 0.0948\n",
      "Epoch 107/500\n",
      "577/577 [==============================] - 323s 559ms/step - d_loss: -9915.1077 - g_loss: 9878.6069 - p10: 0.0344 - G10: 0.1363 - M10: 0.0926 - p20: 0.0328 - G20: 0.1736 - M20: 0.0960\n",
      "Epoch 108/500\n",
      "577/577 [==============================] - 322s 559ms/step - d_loss: -10008.8180 - g_loss: 9971.9708 - p10: 0.0316 - G10: 0.1508 - M10: 0.1116 - p20: 0.0335 - G20: 0.1891 - M20: 0.1121\n",
      "Epoch 109/500\n",
      "577/577 [==============================] - 325s 562ms/step - d_loss: -10102.4665 - g_loss: 10065.2483 - p10: 0.0274 - G10: 0.1112 - M10: 0.0784 - p20: 0.0321 - G20: 0.1589 - M20: 0.0865\n",
      "Epoch 110/500\n",
      "577/577 [==============================] - 322s 558ms/step - d_loss: -10196.1747 - g_loss: 10158.6387 - p10: 0.0335 - G10: 0.1288 - M10: 0.0935 - p20: 0.0337 - G20: 0.1640 - M20: 0.0954\n",
      "Epoch 111/500\n",
      "577/577 [==============================] - 323s 560ms/step - d_loss: -10289.9217 - g_loss: 10252.0242 - p10: 0.0326 - G10: 0.1302 - M10: 0.0928 - p20: 0.0281 - G20: 0.1619 - M20: 0.0972\n",
      "Epoch 112/500\n",
      "577/577 [==============================] - 322s 558ms/step - d_loss: -10383.6460 - g_loss: 10345.4016 - p10: 0.0288 - G10: 0.1195 - M10: 0.0845 - p20: 0.0316 - G20: 0.1564 - M20: 0.0883\n",
      "Epoch 113/500\n",
      "577/577 [==============================] - 322s 559ms/step - d_loss: -10477.3250 - g_loss: 10438.7176 - p10: 0.0316 - G10: 0.1278 - M10: 0.0943 - p20: 0.0314 - G20: 0.1600 - M20: 0.0956\n",
      "Epoch 114/500\n",
      "577/577 [==============================] - 322s 558ms/step - d_loss: -10571.0465 - g_loss: 10532.0822 - p10: 0.0372 - G10: 0.1304 - M10: 0.0926 - p20: 0.0330 - G20: 0.1580 - M20: 0.0881\n",
      "Epoch 115/500\n",
      "577/577 [==============================] - 321s 556ms/step - d_loss: -10663.9160 - g_loss: 10625.1754 - p10: 0.0293 - G10: 0.1170 - M10: 0.0814 - p20: 0.0286 - G20: 0.1509 - M20: 0.0863\n",
      "Epoch 116/500\n",
      "577/577 [==============================] - 320s 555ms/step - d_loss: -10757.5832 - g_loss: 10718.4558 - p10: 0.0298 - G10: 0.1215 - M10: 0.0865 - p20: 0.0298 - G20: 0.1552 - M20: 0.0922\n",
      "Epoch 117/500\n",
      "577/577 [==============================] - 321s 557ms/step - d_loss: -10851.5479 - g_loss: 10811.8232 - p10: 0.0340 - G10: 0.1337 - M10: 0.0909 - p20: 0.0326 - G20: 0.1770 - M20: 0.0978\n",
      "Epoch 118/500\n",
      "577/577 [==============================] - 320s 555ms/step - d_loss: -10945.1283 - g_loss: 10905.0811 - p10: 0.0340 - G10: 0.1487 - M10: 0.1061 - p20: 0.0298 - G20: 0.1823 - M20: 0.1106\n",
      "Epoch 119/500\n",
      "577/577 [==============================] - 320s 554ms/step - d_loss: -11038.9679 - g_loss: 10998.4254 - p10: 0.0367 - G10: 0.1573 - M10: 0.1122 - p20: 0.0300 - G20: 0.1911 - M20: 0.1189\n",
      "Epoch 120/500\n",
      "577/577 [==============================] - 323s 559ms/step - d_loss: -11132.8577 - g_loss: 11091.8674 - p10: 0.0349 - G10: 0.1486 - M10: 0.1044 - p20: 0.0323 - G20: 0.1788 - M20: 0.1059\n",
      "Epoch 121/500\n",
      "577/577 [==============================] - 322s 559ms/step - d_loss: -11226.6008 - g_loss: 11185.2165 - p10: 0.0414 - G10: 0.1601 - M10: 0.1124 - p20: 0.0340 - G20: 0.1880 - M20: 0.1135\n",
      "Epoch 122/500\n",
      "577/577 [==============================] - 320s 555ms/step - d_loss: -11320.3515 - g_loss: 11278.6056 - p10: 0.0353 - G10: 0.1356 - M10: 0.0883 - p20: 0.0342 - G20: 0.1760 - M20: 0.0980\n",
      "Epoch 123/500\n",
      "577/577 [==============================] - 321s 556ms/step - d_loss: -11414.0731 - g_loss: 11371.9656 - p10: 0.0321 - G10: 0.1445 - M10: 0.1123 - p20: 0.0314 - G20: 0.1842 - M20: 0.1128\n",
      "Epoch 124/500\n",
      "577/577 [==============================] - 321s 556ms/step - d_loss: -11507.7729 - g_loss: 11465.3204 - p10: 0.0349 - G10: 0.1547 - M10: 0.1131 - p20: 0.0328 - G20: 0.1873 - M20: 0.1103\n",
      "Epoch 125/500\n",
      "577/577 [==============================] - 322s 557ms/step - d_loss: -11601.4903 - g_loss: 11558.7069 - p10: 0.0307 - G10: 0.1262 - M10: 0.0888 - p20: 0.0319 - G20: 0.1642 - M20: 0.0933\n",
      "Epoch 126/500\n",
      "577/577 [==============================] - 322s 558ms/step - d_loss: -11695.2005 - g_loss: 11652.0607 - p10: 0.0358 - G10: 0.1438 - M10: 0.1020 - p20: 0.0330 - G20: 0.1841 - M20: 0.1086\n",
      "Epoch 127/500\n",
      "577/577 [==============================] - 320s 555ms/step - d_loss: -11788.9166 - g_loss: 11745.4225 - p10: 0.0326 - G10: 0.1205 - M10: 0.0813 - p20: 0.0314 - G20: 0.1631 - M20: 0.0891\n",
      "Epoch 128/500\n",
      "577/577 [==============================] - 321s 556ms/step - d_loss: -11882.2488 - g_loss: 11838.6165 - p10: 0.0307 - G10: 0.1097 - M10: 0.0699 - p20: 0.0330 - G20: 0.1524 - M20: 0.0775\n",
      "Epoch 129/500\n",
      "577/577 [==============================] - 322s 557ms/step - d_loss: -11975.7020 - g_loss: 11931.6703 - p10: 0.0358 - G10: 0.1334 - M10: 0.0906 - p20: 0.0330 - G20: 0.1632 - M20: 0.0928\n",
      "Epoch 130/500\n",
      "577/577 [==============================] - 320s 554ms/step - d_loss: -12069.3840 - g_loss: 12024.9505 - p10: 0.0288 - G10: 0.1101 - M10: 0.0810 - p20: 0.0258 - G20: 0.1383 - M20: 0.0843\n",
      "Epoch 131/500\n",
      "577/577 [==============================] - 321s 556ms/step - d_loss: -12163.1162 - g_loss: 12118.2747 - p10: 0.0298 - G10: 0.1181 - M10: 0.0858 - p20: 0.0277 - G20: 0.1433 - M20: 0.0838\n",
      "Epoch 132/500\n",
      "577/577 [==============================] - 321s 557ms/step - d_loss: -12256.6472 - g_loss: 12211.5738 - p10: 0.0270 - G10: 0.1164 - M10: 0.0824 - p20: 0.0270 - G20: 0.1498 - M20: 0.0889\n",
      "Epoch 133/500\n",
      "577/577 [==============================] - 321s 557ms/step - d_loss: -12350.4237 - g_loss: 12304.9183 - p10: 0.0260 - G10: 0.1110 - M10: 0.0806 - p20: 0.0293 - G20: 0.1521 - M20: 0.0832\n",
      "Epoch 134/500\n",
      "577/577 [==============================] - 321s 557ms/step - d_loss: -12444.1738 - g_loss: 12398.2843 - p10: 0.0298 - G10: 0.1230 - M10: 0.0865 - p20: 0.0281 - G20: 0.1521 - M20: 0.0866\n",
      "Epoch 135/500\n",
      "577/577 [==============================] - 322s 558ms/step - d_loss: -12537.9475 - g_loss: 12491.7128 - p10: 0.0270 - G10: 0.1090 - M10: 0.0784 - p20: 0.0258 - G20: 0.1353 - M20: 0.0769\n",
      "Epoch 136/500\n",
      "577/577 [==============================] - 321s 556ms/step - d_loss: -12631.7012 - g_loss: 12585.1484 - p10: 0.0288 - G10: 0.1061 - M10: 0.0685 - p20: 0.0307 - G20: 0.1507 - M20: 0.0783\n",
      "Epoch 137/500\n",
      "577/577 [==============================] - 321s 556ms/step - d_loss: -12725.3809 - g_loss: 12678.4846 - p10: 0.0386 - G10: 0.1435 - M10: 0.0998 - p20: 0.0337 - G20: 0.1809 - M20: 0.1055\n",
      "Epoch 138/500\n",
      "577/577 [==============================] - 324s 561ms/step - d_loss: -12819.0518 - g_loss: 12771.8494 - p10: 0.0340 - G10: 0.1270 - M10: 0.0850 - p20: 0.0321 - G20: 0.1669 - M20: 0.0919\n",
      "Epoch 139/500\n",
      "577/577 [==============================] - 322s 558ms/step - d_loss: -12912.7827 - g_loss: 12865.2438 - p10: 0.0307 - G10: 0.1180 - M10: 0.0819 - p20: 0.0279 - G20: 0.1520 - M20: 0.0853\n",
      "Epoch 140/500\n",
      "577/577 [==============================] - 321s 557ms/step - d_loss: -13006.4454 - g_loss: 12958.6094 - p10: 0.0400 - G10: 0.1536 - M10: 0.1057 - p20: 0.0335 - G20: 0.1778 - M20: 0.1054\n",
      "Epoch 141/500\n",
      "577/577 [==============================] - 320s 554ms/step - d_loss: -13100.2146 - g_loss: 13051.9599 - p10: 0.0353 - G10: 0.1345 - M10: 0.0979 - p20: 0.0347 - G20: 0.1804 - M20: 0.1043\n",
      "Epoch 142/500\n",
      "577/577 [==============================] - 320s 555ms/step - d_loss: -13193.9172 - g_loss: 13145.3357 - p10: 0.0358 - G10: 0.1287 - M10: 0.0868 - p20: 0.0340 - G20: 0.1650 - M20: 0.0901\n",
      "Epoch 143/500\n",
      "577/577 [==============================] - 324s 561ms/step - d_loss: -13287.6479 - g_loss: 13238.6963 - p10: 0.0349 - G10: 0.1193 - M10: 0.0812 - p20: 0.0319 - G20: 0.1543 - M20: 0.0855\n",
      "Epoch 144/500\n",
      "577/577 [==============================] - 322s 557ms/step - d_loss: -13381.2506 - g_loss: 13331.9676 - p10: 0.0293 - G10: 0.1241 - M10: 0.0902 - p20: 0.0309 - G20: 0.1626 - M20: 0.0964\n",
      "Epoch 145/500\n",
      "577/577 [==============================] - 323s 560ms/step - d_loss: -13474.9114 - g_loss: 13425.3370 - p10: 0.0363 - G10: 0.1334 - M10: 0.0947 - p20: 0.0333 - G20: 0.1689 - M20: 0.0996\n",
      "Epoch 146/500\n",
      "577/577 [==============================] - 323s 560ms/step - d_loss: -13568.7117 - g_loss: 13518.7421 - p10: 0.0288 - G10: 0.1099 - M10: 0.0753 - p20: 0.0298 - G20: 0.1466 - M20: 0.0785\n",
      "Epoch 147/500\n",
      "577/577 [==============================] - 322s 557ms/step - d_loss: -13662.3554 - g_loss: 13612.0445 - p10: 0.0330 - G10: 0.1245 - M10: 0.0816 - p20: 0.0302 - G20: 0.1606 - M20: 0.0884\n",
      "Epoch 148/500\n",
      "577/577 [==============================] - 320s 554ms/step - d_loss: -13756.1008 - g_loss: 13705.4116 - p10: 0.0293 - G10: 0.1230 - M10: 0.0858 - p20: 0.0319 - G20: 0.1676 - M20: 0.0898\n",
      "Epoch 149/500\n",
      "577/577 [==============================] - 320s 555ms/step - d_loss: -13849.8173 - g_loss: 13798.8079 - p10: 0.0312 - G10: 0.1279 - M10: 0.0937 - p20: 0.0307 - G20: 0.1578 - M20: 0.0928\n",
      "Epoch 150/500\n",
      "577/577 [==============================] - 321s 556ms/step - d_loss: -13943.5418 - g_loss: 13892.1686 - p10: 0.0302 - G10: 0.1205 - M10: 0.0876 - p20: 0.0302 - G20: 0.1564 - M20: 0.0879\n",
      "Epoch 151/500\n",
      "577/577 [==============================] - 322s 558ms/step - d_loss: -14037.2755 - g_loss: 13985.5501 - p10: 0.0330 - G10: 0.1327 - M10: 0.0927 - p20: 0.0309 - G20: 0.1616 - M20: 0.0919\n",
      "Epoch 152/500\n",
      "577/577 [==============================] - 321s 557ms/step - d_loss: -14130.9086 - g_loss: 14078.8508 - p10: 0.0288 - G10: 0.1057 - M10: 0.0732 - p20: 0.0314 - G20: 0.1543 - M20: 0.0824\n",
      "Epoch 153/500\n",
      "577/577 [==============================] - 321s 555ms/step - d_loss: -14224.5387 - g_loss: 14172.1183 - p10: 0.0335 - G10: 0.1192 - M10: 0.0798 - p20: 0.0335 - G20: 0.1591 - M20: 0.0849\n",
      "Epoch 154/500\n",
      "577/577 [==============================] - 323s 559ms/step - d_loss: -14318.3209 - g_loss: 14265.5216 - p10: 0.0312 - G10: 0.1327 - M10: 0.0931 - p20: 0.0360 - G20: 0.1802 - M20: 0.1006\n",
      "Epoch 155/500\n",
      "577/577 [==============================] - 320s 555ms/step - d_loss: -14412.0597 - g_loss: 14358.9314 - p10: 0.0377 - G10: 0.1500 - M10: 0.1025 - p20: 0.0330 - G20: 0.1685 - M20: 0.0995\n",
      "Epoch 156/500\n",
      "577/577 [==============================] - 322s 558ms/step - d_loss: -14505.7648 - g_loss: 14452.2817 - p10: 0.0400 - G10: 0.1452 - M10: 0.0970 - p20: 0.0326 - G20: 0.1734 - M20: 0.1005\n",
      "Epoch 157/500\n",
      "577/577 [==============================] - 322s 559ms/step - d_loss: -14599.4625 - g_loss: 14545.6293 - p10: 0.0344 - G10: 0.1332 - M10: 0.0923 - p20: 0.0316 - G20: 0.1687 - M20: 0.0975\n",
      "Epoch 158/500\n",
      "577/577 [==============================] - 321s 556ms/step - d_loss: -14693.1175 - g_loss: 14638.9702 - p10: 0.0284 - G10: 0.1246 - M10: 0.0833 - p20: 0.0284 - G20: 0.1521 - M20: 0.0823\n",
      "Epoch 159/500\n",
      "577/577 [==============================] - 322s 558ms/step - d_loss: -14786.8172 - g_loss: 14732.3127 - p10: 0.0260 - G10: 0.0998 - M10: 0.0638 - p20: 0.0253 - G20: 0.1319 - M20: 0.0660\n",
      "Epoch 160/500\n",
      "577/577 [==============================] - 324s 561ms/step - d_loss: -14880.4364 - g_loss: 14825.7150 - p10: 0.0288 - G10: 0.1063 - M10: 0.0713 - p20: 0.0309 - G20: 0.1368 - M20: 0.0680\n",
      "Epoch 161/500\n",
      "577/577 [==============================] - 320s 555ms/step - d_loss: -14974.1955 - g_loss: 14919.0687 - p10: 0.0340 - G10: 0.1175 - M10: 0.0732 - p20: 0.0328 - G20: 0.1524 - M20: 0.0789\n",
      "Epoch 162/500\n",
      "577/577 [==============================] - 321s 557ms/step - d_loss: -15067.9474 - g_loss: 15012.4406 - p10: 0.0312 - G10: 0.1186 - M10: 0.0766 - p20: 0.0298 - G20: 0.1456 - M20: 0.0791\n",
      "Epoch 163/500\n",
      "577/577 [==============================] - 325s 563ms/step - d_loss: -15161.6519 - g_loss: 15105.7914 - p10: 0.0319 - G10: 0.1165 - M10: 0.0759 - p20: 0.0342 - G20: 0.1599 - M20: 0.0800\n",
      "Epoch 164/500\n",
      "577/577 [==============================] - 321s 556ms/step - d_loss: -15255.3799 - g_loss: 15199.1502 - p10: 0.0340 - G10: 0.1210 - M10: 0.0757 - p20: 0.0323 - G20: 0.1537 - M20: 0.0816\n",
      "Epoch 165/500\n",
      "577/577 [==============================] - 322s 558ms/step - d_loss: -15348.9482 - g_loss: 15292.4385 - p10: 0.0363 - G10: 0.1363 - M10: 0.0912 - p20: 0.0347 - G20: 0.1716 - M20: 0.0914\n",
      "Epoch 166/500\n",
      "577/577 [==============================] - 321s 556ms/step - d_loss: -15442.6405 - g_loss: 15385.7988 - p10: 0.0335 - G10: 0.1230 - M10: 0.0786 - p20: 0.0328 - G20: 0.1639 - M20: 0.0864\n",
      "Epoch 167/500\n",
      "577/577 [==============================] - 323s 559ms/step - d_loss: -15536.3819 - g_loss: 15479.1196 - p10: 0.0316 - G10: 0.1037 - M10: 0.0635 - p20: 0.0279 - G20: 0.1342 - M20: 0.0672\n",
      "Epoch 168/500\n",
      "577/577 [==============================] - 321s 556ms/step - d_loss: -15628.8235 - g_loss: 15572.5041 - p10: 0.0302 - G10: 0.1075 - M10: 0.0692 - p20: 0.0291 - G20: 0.1409 - M20: 0.0736\n",
      "Epoch 169/500\n",
      "577/577 [==============================] - 322s 558ms/step - d_loss: -15723.7708 - g_loss: 15665.8702 - p10: 0.0363 - G10: 0.1294 - M10: 0.0855 - p20: 0.0330 - G20: 0.1630 - M20: 0.0863\n",
      "Epoch 170/500\n",
      "577/577 [==============================] - 322s 558ms/step - d_loss: -15817.4849 - g_loss: 15759.2265 - p10: 0.0358 - G10: 0.1381 - M10: 0.0928 - p20: 0.0323 - G20: 0.1659 - M20: 0.0901\n",
      "Epoch 171/500\n",
      "577/577 [==============================] - 322s 558ms/step - d_loss: -15909.6061 - g_loss: 15852.4993 - p10: 0.0363 - G10: 0.1234 - M10: 0.0789 - p20: 0.0347 - G20: 0.1632 - M20: 0.0877\n",
      "Epoch 172/500\n",
      "577/577 [==============================] - 322s 559ms/step - d_loss: -16004.7627 - g_loss: 15945.8279 - p10: 0.0335 - G10: 0.1138 - M10: 0.0731 - p20: 0.0295 - G20: 0.1417 - M20: 0.0783\n",
      "Epoch 173/500\n",
      "577/577 [==============================] - 321s 556ms/step - d_loss: -16098.4043 - g_loss: 16039.2266 - p10: 0.0363 - G10: 0.1421 - M10: 0.0995 - p20: 0.0314 - G20: 0.1674 - M20: 0.0986\n",
      "Epoch 174/500\n",
      "577/577 [==============================] - 324s 561ms/step - d_loss: -16192.1688 - g_loss: 16132.5205 - p10: 0.0353 - G10: 0.1435 - M10: 0.0982 - p20: 0.0330 - G20: 0.1755 - M20: 0.0998\n",
      "Epoch 175/500\n",
      "577/577 [==============================] - 321s 557ms/step - d_loss: -16285.9596 - g_loss: 16225.8894 - p10: 0.0307 - G10: 0.1237 - M10: 0.0878 - p20: 0.0321 - G20: 0.1655 - M20: 0.0927\n",
      "Epoch 176/500\n",
      "577/577 [==============================] - 324s 561ms/step - d_loss: -16379.6912 - g_loss: 16319.2456 - p10: 0.0321 - G10: 0.1227 - M10: 0.0826 - p20: 0.0360 - G20: 0.1740 - M20: 0.0890\n",
      "Epoch 177/500\n",
      "577/577 [==============================] - 324s 561ms/step - d_loss: -16473.1178 - g_loss: 16412.3290 - p10: 0.0321 - G10: 0.1297 - M10: 0.0937 - p20: 0.0353 - G20: 0.1708 - M20: 0.0896\n",
      "Epoch 178/500\n",
      "577/577 [==============================] - 324s 561ms/step - d_loss: -16564.8767 - g_loss: 16503.7546 - p10: 0.0377 - G10: 0.1427 - M10: 0.1002 - p20: 0.0342 - G20: 0.1676 - M20: 0.0991\n",
      "Epoch 179/500\n",
      "577/577 [==============================] - 323s 559ms/step - d_loss: -16656.0017 - g_loss: 16594.5424 - p10: 0.0372 - G10: 0.1391 - M10: 0.0924 - p20: 0.0347 - G20: 0.1703 - M20: 0.0934\n",
      "Epoch 180/500\n",
      "577/577 [==============================] - 323s 560ms/step - d_loss: -16746.8932 - g_loss: 16685.1796 - p10: 0.0349 - G10: 0.1182 - M10: 0.0806 - p20: 0.0353 - G20: 0.1542 - M20: 0.0839\n",
      "Epoch 181/500\n",
      "577/577 [==============================] - 322s 558ms/step - d_loss: -16837.9047 - g_loss: 16775.8379 - p10: 0.0335 - G10: 0.1182 - M10: 0.0808 - p20: 0.0300 - G20: 0.1540 - M20: 0.0858\n",
      "Epoch 182/500\n",
      "577/577 [==============================] - 322s 558ms/step - d_loss: -16928.8237 - g_loss: 16866.4456 - p10: 0.0344 - G10: 0.1250 - M10: 0.0825 - p20: 0.0342 - G20: 0.1560 - M20: 0.0795\n",
      "Epoch 183/500\n",
      "577/577 [==============================] - 322s 558ms/step - d_loss: -17019.7331 - g_loss: 16956.9615 - p10: 0.0316 - G10: 0.1189 - M10: 0.0833 - p20: 0.0328 - G20: 0.1616 - M20: 0.0917\n",
      "Epoch 184/500\n",
      "577/577 [==============================] - 321s 557ms/step - d_loss: -17110.6288 - g_loss: 17047.5130 - p10: 0.0340 - G10: 0.1196 - M10: 0.0797 - p20: 0.0316 - G20: 0.1520 - M20: 0.0874\n",
      "Epoch 185/500\n",
      "577/577 [==============================] - 323s 559ms/step - d_loss: -17201.5213 - g_loss: 17138.0612 - p10: 0.0312 - G10: 0.1157 - M10: 0.0725 - p20: 0.0342 - G20: 0.1547 - M20: 0.0735\n",
      "Epoch 186/500\n",
      "577/577 [==============================] - 322s 559ms/step - d_loss: -17292.3608 - g_loss: 17228.5963 - p10: 0.0330 - G10: 0.1251 - M10: 0.0818 - p20: 0.0367 - G20: 0.1719 - M20: 0.0883\n",
      "Epoch 187/500\n",
      "577/577 [==============================] - 322s 558ms/step - d_loss: -17383.2147 - g_loss: 17319.1114 - p10: 0.0298 - G10: 0.1035 - M10: 0.0650 - p20: 0.0321 - G20: 0.1508 - M20: 0.0759\n",
      "Epoch 188/500\n",
      "577/577 [==============================] - 322s 559ms/step - d_loss: -17474.1237 - g_loss: 17409.6193 - p10: 0.0358 - G10: 0.1260 - M10: 0.0768 - p20: 0.0349 - G20: 0.1640 - M20: 0.0811\n",
      "Epoch 189/500\n",
      "577/577 [==============================] - 322s 558ms/step - d_loss: -17565.0087 - g_loss: 17500.1476 - p10: 0.0377 - G10: 0.1441 - M10: 0.0985 - p20: 0.0323 - G20: 0.1700 - M20: 0.0978\n",
      "Epoch 190/500\n",
      "577/577 [==============================] - 323s 560ms/step - d_loss: -17655.8502 - g_loss: 17590.6761 - p10: 0.0307 - G10: 0.1210 - M10: 0.0822 - p20: 0.0302 - G20: 0.1595 - M20: 0.0847\n",
      "Epoch 191/500\n",
      "577/577 [==============================] - 321s 557ms/step - d_loss: -17746.6780 - g_loss: 17681.1333 - p10: 0.0307 - G10: 0.1103 - M10: 0.0724 - p20: 0.0307 - G20: 0.1474 - M20: 0.0769\n",
      "Epoch 192/500\n",
      "577/577 [==============================] - 321s 556ms/step - d_loss: -17837.5787 - g_loss: 17771.7168 - p10: 0.0358 - G10: 0.1383 - M10: 0.0941 - p20: 0.0323 - G20: 0.1667 - M20: 0.0917\n",
      "Epoch 193/500\n",
      "577/577 [==============================] - 323s 561ms/step - d_loss: -17928.4233 - g_loss: 17862.2349 - p10: 0.0335 - G10: 0.1228 - M10: 0.0815 - p20: 0.0337 - G20: 0.1551 - M20: 0.0838\n",
      "Epoch 194/500\n",
      "577/577 [==============================] - 323s 560ms/step - d_loss: -18019.2115 - g_loss: 17952.6851 - p10: 0.0293 - G10: 0.1041 - M10: 0.0599 - p20: 0.0298 - G20: 0.1365 - M20: 0.0651\n",
      "Epoch 195/500\n",
      "577/577 [==============================] - 324s 562ms/step - d_loss: -18110.0961 - g_loss: 18043.2264 - p10: 0.0372 - G10: 0.1518 - M10: 0.1077 - p20: 0.0356 - G20: 0.1900 - M20: 0.1087\n",
      "Epoch 196/500\n",
      "577/577 [==============================] - 321s 556ms/step - d_loss: -18201.0068 - g_loss: 18133.7924 - p10: 0.0344 - G10: 0.1382 - M10: 0.0968 - p20: 0.0358 - G20: 0.1750 - M20: 0.0945\n",
      "Epoch 197/500\n",
      "577/577 [==============================] - 323s 559ms/step - d_loss: -18291.8665 - g_loss: 18224.2848 - p10: 0.0335 - G10: 0.1315 - M10: 0.0882 - p20: 0.0337 - G20: 0.1703 - M20: 0.0926\n",
      "Epoch 198/500\n",
      "577/577 [==============================] - 323s 560ms/step - d_loss: -18382.6921 - g_loss: 18314.7687 - p10: 0.0419 - G10: 0.1524 - M10: 0.1011 - p20: 0.0407 - G20: 0.1979 - M20: 0.1102\n",
      "Epoch 199/500\n",
      "577/577 [==============================] - 323s 561ms/step - d_loss: -18473.6098 - g_loss: 18405.3631 - p10: 0.0349 - G10: 0.1382 - M10: 0.0959 - p20: 0.0351 - G20: 0.1780 - M20: 0.1048\n",
      "Epoch 200/500\n",
      "577/577 [==============================] - 324s 562ms/step - d_loss: -18564.4423 - g_loss: 18495.9506 - p10: 0.0367 - G10: 0.1440 - M10: 0.0965 - p20: 0.0374 - G20: 0.1868 - M20: 0.1030\n",
      "Epoch 201/500\n",
      "577/577 [==============================] - 323s 560ms/step - d_loss: -18655.2984 - g_loss: 18586.4213 - p10: 0.0363 - G10: 0.1310 - M10: 0.0886 - p20: 0.0319 - G20: 0.1573 - M20: 0.0936\n",
      "Epoch 202/500\n",
      "577/577 [==============================] - 323s 560ms/step - d_loss: -18746.1614 - g_loss: 18676.9897 - p10: 0.0386 - G10: 0.1418 - M10: 0.0958 - p20: 0.0353 - G20: 0.1725 - M20: 0.0956\n",
      "Epoch 203/500\n",
      "577/577 [==============================] - 323s 559ms/step - d_loss: -18837.0370 - g_loss: 18767.5105 - p10: 0.0386 - G10: 0.1363 - M10: 0.0843 - p20: 0.0360 - G20: 0.1764 - M20: 0.0885\n",
      "Epoch 204/500\n",
      "577/577 [==============================] - 324s 561ms/step - d_loss: -18927.9091 - g_loss: 18858.0204 - p10: 0.0335 - G10: 0.1329 - M10: 0.0946 - p20: 0.0344 - G20: 0.1735 - M20: 0.0974\n",
      "Epoch 205/500\n",
      "577/577 [==============================] - 324s 562ms/step - d_loss: -19018.7888 - g_loss: 18948.5554 - p10: 0.0367 - G10: 0.1312 - M10: 0.0862 - p20: 0.0353 - G20: 0.1724 - M20: 0.0914\n",
      "Epoch 206/500\n",
      "577/577 [==============================] - 325s 563ms/step - d_loss: -19109.6122 - g_loss: 19039.0766 - p10: 0.0395 - G10: 0.1470 - M10: 0.0997 - p20: 0.0337 - G20: 0.1780 - M20: 0.1032\n",
      "Epoch 207/500\n",
      "577/577 [==============================] - 323s 559ms/step - d_loss: -19200.3678 - g_loss: 19129.6124 - p10: 0.0316 - G10: 0.1346 - M10: 0.0931 - p20: 0.0353 - G20: 0.1750 - M20: 0.0996\n",
      "Epoch 208/500\n",
      "577/577 [==============================] - 250s 434ms/step - d_loss: -19291.2323 - g_loss: 19220.1029 - p10: 0.0344 - G10: 0.1212 - M10: 0.0817 - p20: 0.0333 - G20: 0.1567 - M20: 0.0848\n",
      "Epoch 209/500\n",
      "577/577 [==============================] - 220s 381ms/step - d_loss: -19382.1554 - g_loss: 19310.6561 - p10: 0.0400 - G10: 0.1471 - M10: 0.0980 - p20: 0.0344 - G20: 0.1677 - M20: 0.0957\n",
      "Epoch 210/500\n",
      "577/577 [==============================] - 220s 382ms/step - d_loss: -19473.0270 - g_loss: 19401.1643 - p10: 0.0326 - G10: 0.1227 - M10: 0.0802 - p20: 0.0293 - G20: 0.1503 - M20: 0.0821\n",
      "Epoch 211/500\n",
      "577/577 [==============================] - 221s 384ms/step - d_loss: -19563.9245 - g_loss: 19491.7152 - p10: 0.0372 - G10: 0.1236 - M10: 0.0816 - p20: 0.0353 - G20: 0.1553 - M20: 0.0815\n",
      "Epoch 212/500\n",
      "577/577 [==============================] - 220s 382ms/step - d_loss: -19654.7795 - g_loss: 19582.2466 - p10: 0.0335 - G10: 0.1226 - M10: 0.0842 - p20: 0.0337 - G20: 0.1574 - M20: 0.0841\n",
      "Epoch 213/500\n",
      "577/577 [==============================] - 220s 381ms/step - d_loss: -19745.6086 - g_loss: 19672.7324 - p10: 0.0316 - G10: 0.1201 - M10: 0.0832 - p20: 0.0342 - G20: 0.1693 - M20: 0.0873\n",
      "Epoch 214/500\n",
      "577/577 [==============================] - 222s 384ms/step - d_loss: -19834.7490 - g_loss: 19763.3265 - p10: 0.0349 - G10: 0.1276 - M10: 0.0843 - p20: 0.0316 - G20: 0.1637 - M20: 0.0895\n",
      "Epoch 215/500\n",
      "577/577 [==============================] - 222s 384ms/step - d_loss: -19927.0422 - g_loss: 19853.8925 - p10: 0.0279 - G10: 0.1034 - M10: 0.0695 - p20: 0.0333 - G20: 0.1492 - M20: 0.0781\n",
      "Epoch 216/500\n",
      "577/577 [==============================] - 220s 382ms/step - d_loss: -20017.9364 - g_loss: 19944.4245 - p10: 0.0326 - G10: 0.1197 - M10: 0.0761 - p20: 0.0302 - G20: 0.1548 - M20: 0.0821\n",
      "Epoch 217/500\n",
      "577/577 [==============================] - 221s 383ms/step - d_loss: -20108.8980 - g_loss: 20034.8581 - p10: 0.0386 - G10: 0.1392 - M10: 0.0976 - p20: 0.0347 - G20: 0.1678 - M20: 0.0978\n",
      "Epoch 218/500\n",
      "577/577 [==============================] - 222s 384ms/step - d_loss: -20199.7956 - g_loss: 20125.4779 - p10: 0.0353 - G10: 0.1246 - M10: 0.0772 - p20: 0.0307 - G20: 0.1524 - M20: 0.0780\n",
      "Epoch 219/500\n",
      "577/577 [==============================] - 221s 383ms/step - d_loss: -20290.6961 - g_loss: 20216.0080 - p10: 0.0340 - G10: 0.1299 - M10: 0.0844 - p20: 0.0314 - G20: 0.1667 - M20: 0.0921\n",
      "Epoch 220/500\n",
      "577/577 [==============================] - 224s 388ms/step - d_loss: -20381.6086 - g_loss: 20306.5204 - p10: 0.0358 - G10: 0.1406 - M10: 0.0965 - p20: 0.0314 - G20: 0.1654 - M20: 0.0927\n",
      "Epoch 221/500\n",
      "577/577 [==============================] - 228s 395ms/step - d_loss: -20472.4113 - g_loss: 20397.0295 - p10: 0.0353 - G10: 0.1413 - M10: 0.0995 - p20: 0.0333 - G20: 0.1770 - M20: 0.1028\n",
      "Epoch 222/500\n",
      "577/577 [==============================] - 227s 393ms/step - d_loss: -20562.9060 - g_loss: 20487.5447 - p10: 0.0409 - G10: 0.1591 - M10: 0.1042 - p20: 0.0344 - G20: 0.1836 - M20: 0.0987\n",
      "Epoch 223/500\n",
      "577/577 [==============================] - 226s 391ms/step - d_loss: -20653.9889 - g_loss: 20578.1140 - p10: 0.0358 - G10: 0.1314 - M10: 0.0847 - p20: 0.0309 - G20: 0.1621 - M20: 0.0894\n",
      "Epoch 224/500\n",
      "577/577 [==============================] - 226s 392ms/step - d_loss: -20744.9383 - g_loss: 20668.5922 - p10: 0.0316 - G10: 0.1294 - M10: 0.0876 - p20: 0.0286 - G20: 0.1589 - M20: 0.0875\n",
      "Epoch 225/500\n",
      "577/577 [==============================] - 226s 393ms/step - d_loss: -20835.8760 - g_loss: 20759.1107 - p10: 0.0381 - G10: 0.1435 - M10: 0.0963 - p20: 0.0342 - G20: 0.1693 - M20: 0.0932\n",
      "Epoch 226/500\n",
      "577/577 [==============================] - 227s 394ms/step - d_loss: -20926.7804 - g_loss: 20849.6432 - p10: 0.0353 - G10: 0.1504 - M10: 0.1063 - p20: 0.0351 - G20: 0.1870 - M20: 0.1033\n",
      "Epoch 227/500\n",
      "577/577 [==============================] - 227s 394ms/step - d_loss: -21017.6527 - g_loss: 20940.1854 - p10: 0.0335 - G10: 0.1183 - M10: 0.0709 - p20: 0.0342 - G20: 0.1617 - M20: 0.0782\n",
      "Epoch 228/500\n",
      "577/577 [==============================] - 226s 391ms/step - d_loss: -21108.5438 - g_loss: 21030.7678 - p10: 0.0372 - G10: 0.1438 - M10: 0.1020 - p20: 0.0356 - G20: 0.1774 - M20: 0.1004\n",
      "Epoch 229/500\n",
      "577/577 [==============================] - 225s 390ms/step - d_loss: -21199.3788 - g_loss: 21121.2848 - p10: 0.0349 - G10: 0.1421 - M10: 0.1029 - p20: 0.0333 - G20: 0.1723 - M20: 0.1005\n",
      "Epoch 230/500\n",
      "577/577 [==============================] - 228s 396ms/step - d_loss: -21290.2333 - g_loss: 21211.8088 - p10: 0.0391 - G10: 0.1417 - M10: 0.0986 - p20: 0.0372 - G20: 0.1789 - M20: 0.0992\n",
      "Epoch 231/500\n",
      "577/577 [==============================] - 227s 394ms/step - d_loss: -21381.0859 - g_loss: 21302.2991 - p10: 0.0358 - G10: 0.1223 - M10: 0.0836 - p20: 0.0356 - G20: 0.1568 - M20: 0.0869\n",
      "Epoch 232/500\n",
      "577/577 [==============================] - 226s 391ms/step - d_loss: -21471.9749 - g_loss: 21392.7857 - p10: 0.0340 - G10: 0.1205 - M10: 0.0796 - p20: 0.0326 - G20: 0.1542 - M20: 0.0826\n",
      "Epoch 233/500\n",
      "577/577 [==============================] - 226s 392ms/step - d_loss: -21562.6836 - g_loss: 21483.3827 - p10: 0.0344 - G10: 0.1243 - M10: 0.0860 - p20: 0.0340 - G20: 0.1554 - M20: 0.0859\n",
      "Epoch 234/500\n",
      "577/577 [==============================] - 226s 392ms/step - d_loss: -21653.6353 - g_loss: 21573.8027 - p10: 0.0326 - G10: 0.1234 - M10: 0.0818 - p20: 0.0335 - G20: 0.1664 - M20: 0.0878\n",
      "Epoch 235/500\n",
      "577/577 [==============================] - 229s 397ms/step - d_loss: -21744.4427 - g_loss: 21664.5001 - p10: 0.0340 - G10: 0.1310 - M10: 0.0867 - p20: 0.0300 - G20: 0.1559 - M20: 0.0870\n",
      "Epoch 236/500\n",
      "577/577 [==============================] - 226s 392ms/step - d_loss: -21835.3884 - g_loss: 21754.9446 - p10: 0.0447 - G10: 0.1615 - M10: 0.1069 - p20: 0.0391 - G20: 0.1911 - M20: 0.1049\n",
      "Epoch 237/500\n",
      "577/577 [==============================] - 226s 392ms/step - d_loss: -21926.2339 - g_loss: 21845.5018 - p10: 0.0381 - G10: 0.1414 - M10: 0.0972 - p20: 0.0370 - G20: 0.1789 - M20: 0.1020\n",
      "Epoch 238/500\n",
      "577/577 [==============================] - 226s 392ms/step - d_loss: -22017.1006 - g_loss: 21936.0177 - p10: 0.0330 - G10: 0.1156 - M10: 0.0744 - p20: 0.0333 - G20: 0.1623 - M20: 0.0806\n",
      "Epoch 239/500\n",
      "577/577 [==============================] - 228s 395ms/step - d_loss: -22107.9433 - g_loss: 22026.5537 - p10: 0.0349 - G10: 0.1261 - M10: 0.0867 - p20: 0.0340 - G20: 0.1666 - M20: 0.0935\n",
      "Epoch 240/500\n",
      "577/577 [==============================] - 227s 393ms/step - d_loss: -22198.8339 - g_loss: 22117.0865 - p10: 0.0335 - G10: 0.1224 - M10: 0.0813 - p20: 0.0349 - G20: 0.1684 - M20: 0.0873\n",
      "Epoch 241/500\n",
      "577/577 [==============================] - 226s 392ms/step - d_loss: -22289.6791 - g_loss: 22207.5211 - p10: 0.0302 - G10: 0.1254 - M10: 0.0921 - p20: 0.0293 - G20: 0.1563 - M20: 0.0927\n",
      "Epoch 242/500\n",
      "577/577 [==============================] - 228s 395ms/step - d_loss: -22380.4207 - g_loss: 22298.2629 - p10: 0.0321 - G10: 0.1190 - M10: 0.0814 - p20: 0.0309 - G20: 0.1496 - M20: 0.0797\n",
      "Epoch 243/500\n",
      "577/577 [==============================] - 229s 397ms/step - d_loss: -22471.3864 - g_loss: 22388.7005 - p10: 0.0307 - G10: 0.1149 - M10: 0.0718 - p20: 0.0307 - G20: 0.1470 - M20: 0.0762\n",
      "Epoch 244/500\n",
      "577/577 [==============================] - 227s 394ms/step - d_loss: -22562.2339 - g_loss: 22479.2372 - p10: 0.0326 - G10: 0.1247 - M10: 0.0804 - p20: 0.0333 - G20: 0.1627 - M20: 0.0851\n",
      "Epoch 245/500\n",
      "577/577 [==============================] - 230s 398ms/step - d_loss: -22653.1180 - g_loss: 22569.6938 - p10: 0.0365 - G10: 0.1257 - M10: 0.0833 - p20: 0.0343 - G20: 0.1595 - M20: 0.0873\n",
      "Epoch 246/500\n",
      "577/577 [==============================] - 227s 393ms/step - d_loss: -22744.0040 - g_loss: 22660.3039 - p10: 0.0302 - G10: 0.1212 - M10: 0.0857 - p20: 0.0309 - G20: 0.1550 - M20: 0.0866\n",
      "Epoch 247/500\n",
      "577/577 [==============================] - 227s 394ms/step - d_loss: -22834.8596 - g_loss: 22750.8307 - p10: 0.0363 - G10: 0.1206 - M10: 0.0774 - p20: 0.0337 - G20: 0.1550 - M20: 0.0787\n",
      "Epoch 248/500\n",
      "577/577 [==============================] - 228s 396ms/step - d_loss: -22925.7041 - g_loss: 22841.3049 - p10: 0.0340 - G10: 0.1171 - M10: 0.0734 - p20: 0.0328 - G20: 0.1416 - M20: 0.0715\n",
      "Epoch 249/500\n",
      "577/577 [==============================] - 229s 397ms/step - d_loss: -23016.4868 - g_loss: 22931.8704 - p10: 0.0302 - G10: 0.1153 - M10: 0.0870 - p20: 0.0344 - G20: 0.1617 - M20: 0.0898\n",
      "Epoch 250/500\n",
      "577/577 [==============================] - 228s 395ms/step - d_loss: -23107.2208 - g_loss: 23022.3981 - p10: 0.0367 - G10: 0.1268 - M10: 0.0820 - p20: 0.0360 - G20: 0.1725 - M20: 0.0925\n",
      "Epoch 251/500\n",
      "577/577 [==============================] - 229s 398ms/step - d_loss: -23197.6911 - g_loss: 23112.7966 - p10: 0.0344 - G10: 0.1293 - M10: 0.0850 - p20: 0.0321 - G20: 0.1611 - M20: 0.0880\n",
      "Epoch 252/500\n",
      "577/577 [==============================] - 229s 397ms/step - d_loss: -23288.3953 - g_loss: 23203.2309 - p10: 0.0344 - G10: 0.1395 - M10: 0.0992 - p20: 0.0316 - G20: 0.1686 - M20: 0.0999\n",
      "Epoch 253/500\n",
      "577/577 [==============================] - 227s 394ms/step - d_loss: -23378.7575 - g_loss: 23293.7492 - p10: 0.0358 - G10: 0.1422 - M10: 0.1001 - p20: 0.0323 - G20: 0.1776 - M20: 0.1022\n",
      "Epoch 254/500\n",
      "577/577 [==============================] - 228s 394ms/step - d_loss: -23470.3748 - g_loss: 23384.2159 - p10: 0.0349 - G10: 0.1475 - M10: 0.1065 - p20: 0.0347 - G20: 0.1763 - M20: 0.1028\n",
      "Epoch 255/500\n",
      "577/577 [==============================] - 228s 395ms/step - d_loss: -23561.2867 - g_loss: 23474.6613 - p10: 0.0335 - G10: 0.1374 - M10: 0.0985 - p20: 0.0330 - G20: 0.1773 - M20: 0.1028\n",
      "Epoch 256/500\n",
      "577/577 [==============================] - 228s 396ms/step - d_loss: -23652.2119 - g_loss: 23565.2141 - p10: 0.0386 - G10: 0.1387 - M10: 0.0909 - p20: 0.0351 - G20: 0.1722 - M20: 0.0937\n",
      "Epoch 257/500\n",
      "577/577 [==============================] - 227s 393ms/step - d_loss: -23743.1090 - g_loss: 23655.8261 - p10: 0.0358 - G10: 0.1634 - M10: 0.1216 - p20: 0.0351 - G20: 0.1984 - M20: 0.1198\n",
      "Epoch 258/500\n",
      "577/577 [==============================] - 233s 403ms/step - d_loss: -23834.0276 - g_loss: 23746.3338 - p10: 0.0367 - G10: 0.1516 - M10: 0.1111 - p20: 0.0388 - G20: 0.1919 - M20: 0.1107\n",
      "Epoch 259/500\n",
      "577/577 [==============================] - 234s 406ms/step - d_loss: -23924.8559 - g_loss: 23836.8871 - p10: 0.0386 - G10: 0.1558 - M10: 0.1087 - p20: 0.0347 - G20: 0.1847 - M20: 0.1066\n",
      "Epoch 260/500\n",
      "577/577 [==============================] - 228s 395ms/step - d_loss: -18175.8223 - g_loss: 23927.7046 - p10: 0.0358 - G10: 0.1429 - M10: 0.0963 - p20: 0.0353 - G20: 0.1747 - M20: 0.0955\n",
      "Epoch 261/500\n",
      "577/577 [==============================] - 230s 399ms/step - d_loss: -23850.1895 - g_loss: 24017.2232 - p10: 0.0353 - G10: 0.1460 - M10: 0.1036 - p20: 0.0302 - G20: 0.1684 - M20: 0.0995\n",
      "Epoch 262/500\n",
      "577/577 [==============================] - 229s 397ms/step - d_loss: -24095.2333 - g_loss: 24105.8205 - p10: 0.0349 - G10: 0.1393 - M10: 0.0965 - p20: 0.0307 - G20: 0.1663 - M20: 0.0968\n",
      "Epoch 263/500\n",
      "577/577 [==============================] - 230s 398ms/step - d_loss: -24259.5056 - g_loss: 24195.4016 - p10: 0.0372 - G10: 0.1484 - M10: 0.1016 - p20: 0.0342 - G20: 0.1805 - M20: 0.1010\n",
      "Epoch 264/500\n",
      "577/577 [==============================] - 229s 396ms/step - d_loss: -24364.4016 - g_loss: 24285.4992 - p10: 0.0419 - G10: 0.1582 - M10: 0.1078 - p20: 0.0360 - G20: 0.1830 - M20: 0.1076\n",
      "Epoch 265/500\n",
      "577/577 [==============================] - 236s 409ms/step - d_loss: -24455.0355 - g_loss: 24375.9625 - p10: 0.0358 - G10: 0.1490 - M10: 0.1057 - p20: 0.0314 - G20: 0.1764 - M20: 0.1048\n",
      "Epoch 266/500\n",
      "577/577 [==============================] - 229s 397ms/step - d_loss: -24550.2189 - g_loss: 24466.5104 - p10: 0.0330 - G10: 0.1348 - M10: 0.0956 - p20: 0.0340 - G20: 0.1714 - M20: 0.0977\n",
      "Epoch 267/500\n",
      "577/577 [==============================] - 234s 406ms/step - d_loss: -24640.9101 - g_loss: 24556.7931 - p10: 0.0353 - G10: 0.1399 - M10: 0.0990 - p20: 0.0328 - G20: 0.1668 - M20: 0.0958\n",
      "Epoch 268/500\n",
      "577/577 [==============================] - 239s 414ms/step - d_loss: -24733.4823 - g_loss: 24647.2468 - p10: 0.0367 - G10: 0.1448 - M10: 0.1069 - p20: 0.0351 - G20: 0.1839 - M20: 0.1106\n",
      "Epoch 269/500\n",
      "577/577 [==============================] - 237s 411ms/step - d_loss: -24824.5482 - g_loss: 24737.7463 - p10: 0.0316 - G10: 0.1256 - M10: 0.0873 - p20: 0.0340 - G20: 0.1593 - M20: 0.0874\n",
      "Epoch 270/500\n",
      "577/577 [==============================] - 230s 399ms/step - d_loss: -24913.7168 - g_loss: 24827.7835 - p10: 0.0395 - G10: 0.1524 - M10: 0.1069 - p20: 0.0347 - G20: 0.1844 - M20: 0.1101\n",
      "Epoch 271/500\n",
      "577/577 [==============================] - 232s 402ms/step - d_loss: -24996.3860 - g_loss: 24918.2819 - p10: 0.0335 - G10: 0.1260 - M10: 0.0903 - p20: 0.0347 - G20: 0.1697 - M20: 0.0955\n",
      "Epoch 272/500\n",
      "577/577 [==============================] - 228s 394ms/step - d_loss: -25096.5878 - g_loss: 25008.9143 - p10: 0.0372 - G10: 0.1595 - M10: 0.1161 - p20: 0.0309 - G20: 0.1835 - M20: 0.1163\n",
      "Epoch 273/500\n",
      "577/577 [==============================] - 229s 397ms/step - d_loss: -25188.6217 - g_loss: 25099.4550 - p10: 0.0321 - G10: 0.1236 - M10: 0.0869 - p20: 0.0342 - G20: 0.1649 - M20: 0.0901\n",
      "Epoch 274/500\n",
      "577/577 [==============================] - 233s 403ms/step - d_loss: -25278.2776 - g_loss: 25189.9541 - p10: 0.0321 - G10: 0.1430 - M10: 0.1072 - p20: 0.0312 - G20: 0.1768 - M20: 0.1071\n",
      "Epoch 275/500\n",
      "577/577 [==============================] - 228s 395ms/step - d_loss: -25370.5445 - g_loss: 25280.4460 - p10: 0.0279 - G10: 0.1053 - M10: 0.0707 - p20: 0.0314 - G20: 0.1529 - M20: 0.0767\n",
      "Epoch 276/500\n",
      "577/577 [==============================] - 227s 393ms/step - d_loss: -25461.4875 - g_loss: 25370.8167 - p10: 0.0284 - G10: 0.1064 - M10: 0.0720 - p20: 0.0305 - G20: 0.1484 - M20: 0.0803\n",
      "Epoch 277/500\n",
      "577/577 [==============================] - 228s 395ms/step - d_loss: -25552.4441 - g_loss: 25461.2185 - p10: 0.0326 - G10: 0.1136 - M10: 0.0702 - p20: 0.0295 - G20: 0.1513 - M20: 0.0793\n",
      "Epoch 278/500\n",
      "577/577 [==============================] - 228s 395ms/step - d_loss: -25643.3946 - g_loss: 25551.5407 - p10: 0.0293 - G10: 0.1068 - M10: 0.0694 - p20: 0.0288 - G20: 0.1451 - M20: 0.0764\n",
      "Epoch 279/500\n",
      "577/577 [==============================] - 227s 393ms/step - d_loss: -25734.3516 - g_loss: 25641.9341 - p10: 0.0326 - G10: 0.1334 - M10: 0.0915 - p20: 0.0316 - G20: 0.1729 - M20: 0.0970\n",
      "Epoch 280/500\n",
      "577/577 [==============================] - 227s 394ms/step - d_loss: -25825.2095 - g_loss: 25732.4792 - p10: 0.0312 - G10: 0.1358 - M10: 0.0990 - p20: 0.0305 - G20: 0.1780 - M20: 0.1040\n",
      "Epoch 281/500\n",
      "577/577 [==============================] - 229s 396ms/step - d_loss: -25916.0504 - g_loss: 25823.0359 - p10: 0.0335 - G10: 0.1151 - M10: 0.0752 - p20: 0.0319 - G20: 0.1554 - M20: 0.0827\n",
      "Epoch 282/500\n",
      "577/577 [==============================] - 227s 394ms/step - d_loss: -26006.9635 - g_loss: 25913.6665 - p10: 0.0349 - G10: 0.1268 - M10: 0.0861 - p20: 0.0323 - G20: 0.1625 - M20: 0.0899\n",
      "Epoch 283/500\n",
      "577/577 [==============================] - 227s 393ms/step - d_loss: -26097.8025 - g_loss: 26004.1719 - p10: 0.0293 - G10: 0.1162 - M10: 0.0764 - p20: 0.0270 - G20: 0.1415 - M20: 0.0755\n",
      "Epoch 284/500\n",
      "577/577 [==============================] - 227s 393ms/step - d_loss: -26188.7140 - g_loss: 26094.7085 - p10: 0.0279 - G10: 0.0997 - M10: 0.0639 - p20: 0.0281 - G20: 0.1340 - M20: 0.0660\n",
      "Epoch 285/500\n",
      "577/577 [==============================] - 228s 396ms/step - d_loss: -26279.5497 - g_loss: 26185.2276 - p10: 0.0260 - G10: 0.1031 - M10: 0.0700 - p20: 0.0270 - G20: 0.1367 - M20: 0.0726\n",
      "Epoch 286/500\n",
      "577/577 [==============================] - 227s 393ms/step - d_loss: -26370.4582 - g_loss: 26275.7840 - p10: 0.0302 - G10: 0.1144 - M10: 0.0747 - p20: 0.0323 - G20: 0.1622 - M20: 0.0820\n",
      "Epoch 287/500\n",
      "577/577 [==============================] - 227s 394ms/step - d_loss: -26461.0638 - g_loss: 26366.2765 - p10: 0.0293 - G10: 0.1102 - M10: 0.0720 - p20: 0.0286 - G20: 0.1447 - M20: 0.0761\n",
      "Epoch 288/500\n",
      "577/577 [==============================] - 229s 396ms/step - d_loss: -26551.2654 - g_loss: 26456.6916 - p10: 0.0284 - G10: 0.0990 - M10: 0.0606 - p20: 0.0314 - G20: 0.1499 - M20: 0.0739\n",
      "Epoch 289/500\n",
      "577/577 [==============================] - 259s 449ms/step - d_loss: -26642.6997 - g_loss: 26547.3507 - p10: 0.0326 - G10: 0.1132 - M10: 0.0703 - p20: 0.0305 - G20: 0.1536 - M20: 0.0795\n",
      "Epoch 290/500\n",
      "577/577 [==============================] - 333s 576ms/step - d_loss: -26733.5892 - g_loss: 26637.9802 - p10: 0.0260 - G10: 0.1090 - M10: 0.0772 - p20: 0.0286 - G20: 0.1506 - M20: 0.0826\n",
      "Epoch 291/500\n",
      "577/577 [==============================] - 333s 578ms/step - d_loss: -26824.5061 - g_loss: 26728.3325 - p10: 0.0256 - G10: 0.0993 - M10: 0.0635 - p20: 0.0288 - G20: 0.1438 - M20: 0.0715\n",
      "Epoch 292/500\n",
      "577/577 [==============================] - 335s 581ms/step - d_loss: -26915.4269 - g_loss: 26818.8346 - p10: 0.0321 - G10: 0.1133 - M10: 0.0700 - p20: 0.0328 - G20: 0.1505 - M20: 0.0780\n",
      "Epoch 293/500\n",
      "577/577 [==============================] - 330s 573ms/step - d_loss: -27006.3455 - g_loss: 26909.4704 - p10: 0.0316 - G10: 0.1168 - M10: 0.0788 - p20: 0.0314 - G20: 0.1546 - M20: 0.0843\n",
      "Epoch 294/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -27097.2516 - g_loss: 26999.8853 - p10: 0.0298 - G10: 0.1181 - M10: 0.0851 - p20: 0.0295 - G20: 0.1497 - M20: 0.0840\n",
      "Epoch 295/500\n",
      "577/577 [==============================] - 376s 652ms/step - d_loss: -27188.1334 - g_loss: 27090.4376 - p10: 0.0293 - G10: 0.1052 - M10: 0.0685 - p20: 0.0279 - G20: 0.1346 - M20: 0.0754\n",
      "Epoch 296/500\n",
      "577/577 [==============================] - 338s 586ms/step - d_loss: -27278.9798 - g_loss: 27180.9273 - p10: 0.0316 - G10: 0.1046 - M10: 0.0653 - p20: 0.0323 - G20: 0.1487 - M20: 0.0754\n",
      "Epoch 297/500\n",
      "577/577 [==============================] - 357s 618ms/step - d_loss: -27369.8994 - g_loss: 27271.4466 - p10: 0.0302 - G10: 0.1111 - M10: 0.0699 - p20: 0.0333 - G20: 0.1534 - M20: 0.0779\n",
      "Epoch 298/500\n",
      "577/577 [==============================] - 342s 592ms/step - d_loss: -27460.7636 - g_loss: 27361.9602 - p10: 0.0307 - G10: 0.1114 - M10: 0.0691 - p20: 0.0314 - G20: 0.1503 - M20: 0.0746\n",
      "Epoch 299/500\n",
      "577/577 [==============================] - 330s 572ms/step - d_loss: -27551.6253 - g_loss: 27452.4941 - p10: 0.0270 - G10: 0.1004 - M10: 0.0695 - p20: 0.0272 - G20: 0.1375 - M20: 0.0691\n",
      "Epoch 300/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -27642.5143 - g_loss: 27543.0384 - p10: 0.0330 - G10: 0.1358 - M10: 0.0965 - p20: 0.0309 - G20: 0.1634 - M20: 0.0948\n",
      "Epoch 301/500\n",
      "577/577 [==============================] - 340s 589ms/step - d_loss: -27733.3480 - g_loss: 27633.5144 - p10: 0.0302 - G10: 0.1132 - M10: 0.0794 - p20: 0.0300 - G20: 0.1503 - M20: 0.0817\n",
      "Epoch 302/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -27824.2400 - g_loss: 27724.0463 - p10: 0.0340 - G10: 0.1289 - M10: 0.0873 - p20: 0.0337 - G20: 0.1670 - M20: 0.0915\n",
      "Epoch 303/500\n",
      "577/577 [==============================] - 332s 576ms/step - d_loss: -27915.0982 - g_loss: 27814.5792 - p10: 0.0395 - G10: 0.1402 - M10: 0.0964 - p20: 0.0342 - G20: 0.1708 - M20: 0.0962\n",
      "Epoch 304/500\n",
      "577/577 [==============================] - 327s 567ms/step - d_loss: -28005.9701 - g_loss: 27905.1008 - p10: 0.0307 - G10: 0.1051 - M10: 0.0690 - p20: 0.0312 - G20: 0.1462 - M20: 0.0755\n",
      "Epoch 305/500\n",
      "577/577 [==============================] - 324s 562ms/step - d_loss: -28096.8483 - g_loss: 27995.6096 - p10: 0.0321 - G10: 0.1218 - M10: 0.0831 - p20: 0.0326 - G20: 0.1591 - M20: 0.0820\n",
      "Epoch 306/500\n",
      "577/577 [==============================] - 327s 567ms/step - d_loss: -28188.0875 - g_loss: 28086.2956 - p10: 0.0242 - G10: 0.1082 - M10: 0.0812 - p20: 0.0260 - G20: 0.1469 - M20: 0.0860\n",
      "Epoch 307/500\n",
      "577/577 [==============================] - 326s 565ms/step - d_loss: -28279.1489 - g_loss: 28176.8814 - p10: 0.0312 - G10: 0.1250 - M10: 0.0846 - p20: 0.0272 - G20: 0.1508 - M20: 0.0877\n",
      "Epoch 308/500\n",
      "577/577 [==============================] - 326s 564ms/step - d_loss: -28370.0580 - g_loss: 28267.3257 - p10: 0.0326 - G10: 0.1283 - M10: 0.0875 - p20: 0.0321 - G20: 0.1675 - M20: 0.0914\n",
      "Epoch 309/500\n",
      "577/577 [==============================] - 332s 576ms/step - d_loss: -28460.8781 - g_loss: 28357.7291 - p10: 0.0293 - G10: 0.1257 - M10: 0.0917 - p20: 0.0300 - G20: 0.1645 - M20: 0.0942\n",
      "Epoch 310/500\n",
      "577/577 [==============================] - 329s 570ms/step - d_loss: -28551.6816 - g_loss: 28448.4812 - p10: 0.0326 - G10: 0.1220 - M10: 0.0812 - p20: 0.0314 - G20: 0.1523 - M20: 0.0826\n",
      "Epoch 311/500\n",
      "577/577 [==============================] - 328s 569ms/step - d_loss: -28642.4691 - g_loss: 28538.8782 - p10: 0.0340 - G10: 0.1369 - M10: 0.0944 - p20: 0.0333 - G20: 0.1682 - M20: 0.0911\n",
      "Epoch 312/500\n",
      "577/577 [==============================] - 328s 568ms/step - d_loss: -28733.3954 - g_loss: 28629.4381 - p10: 0.0302 - G10: 0.1182 - M10: 0.0810 - p20: 0.0314 - G20: 0.1561 - M20: 0.0805\n",
      "Epoch 313/500\n",
      "577/577 [==============================] - 329s 570ms/step - d_loss: -28824.3349 - g_loss: 28719.8448 - p10: 0.0349 - G10: 0.1364 - M10: 0.0957 - p20: 0.0293 - G20: 0.1545 - M20: 0.0923\n",
      "Epoch 314/500\n",
      "577/577 [==============================] - 325s 563ms/step - d_loss: -28915.2630 - g_loss: 28810.3478 - p10: 0.0326 - G10: 0.1227 - M10: 0.0873 - p20: 0.0307 - G20: 0.1523 - M20: 0.0873\n",
      "Epoch 315/500\n",
      "577/577 [==============================] - 325s 563ms/step - d_loss: -29006.0734 - g_loss: 28900.9014 - p10: 0.0293 - G10: 0.1228 - M10: 0.0887 - p20: 0.0300 - G20: 0.1522 - M20: 0.0893\n",
      "Epoch 316/500\n",
      "577/577 [==============================] - 334s 579ms/step - d_loss: -29096.9722 - g_loss: 28991.4189 - p10: 0.0274 - G10: 0.1074 - M10: 0.0737 - p20: 0.0291 - G20: 0.1439 - M20: 0.0789\n",
      "Epoch 317/500\n",
      "577/577 [==============================] - 330s 572ms/step - d_loss: -29187.8784 - g_loss: 29081.9013 - p10: 0.0358 - G10: 0.1303 - M10: 0.0886 - p20: 0.0365 - G20: 0.1692 - M20: 0.0932\n",
      "Epoch 318/500\n",
      "577/577 [==============================] - 325s 563ms/step - d_loss: -29274.0641 - g_loss: 29172.4352 - p10: 0.0326 - G10: 0.1230 - M10: 0.0850 - p20: 0.0347 - G20: 0.1637 - M20: 0.0872\n",
      "Epoch 319/500\n",
      "577/577 [==============================] - 325s 564ms/step - d_loss: -29368.8520 - g_loss: 29262.6696 - p10: 0.0288 - G10: 0.1190 - M10: 0.0832 - p20: 0.0265 - G20: 0.1503 - M20: 0.0868\n",
      "Epoch 320/500\n",
      "577/577 [==============================] - 325s 563ms/step - d_loss: -29459.5680 - g_loss: 29353.1465 - p10: 0.0381 - G10: 0.1574 - M10: 0.1130 - p20: 0.0367 - G20: 0.1910 - M20: 0.1095\n",
      "Epoch 321/500\n",
      "577/577 [==============================] - 326s 566ms/step - d_loss: -29550.4305 - g_loss: 29443.7449 - p10: 0.0270 - G10: 0.1158 - M10: 0.0812 - p20: 0.0305 - G20: 0.1531 - M20: 0.0786\n",
      "Epoch 322/500\n",
      "577/577 [==============================] - 327s 567ms/step - d_loss: -29641.2803 - g_loss: 29534.2485 - p10: 0.0330 - G10: 0.1192 - M10: 0.0795 - p20: 0.0316 - G20: 0.1510 - M20: 0.0834\n",
      "Epoch 323/500\n",
      "577/577 [==============================] - 328s 568ms/step - d_loss: -29731.9207 - g_loss: 29624.6548 - p10: 0.0321 - G10: 0.1219 - M10: 0.0760 - p20: 0.0330 - G20: 0.1611 - M20: 0.0829\n",
      "Epoch 324/500\n",
      "577/577 [==============================] - 325s 564ms/step - d_loss: -29822.5719 - g_loss: 29715.0152 - p10: 0.0288 - G10: 0.1141 - M10: 0.0777 - p20: 0.0267 - G20: 0.1399 - M20: 0.0765\n",
      "Epoch 325/500\n",
      "577/577 [==============================] - 327s 566ms/step - d_loss: -29913.2724 - g_loss: 29805.4640 - p10: 0.0260 - G10: 0.1045 - M10: 0.0750 - p20: 0.0251 - G20: 0.1349 - M20: 0.0778\n",
      "Epoch 326/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -30004.2869 - g_loss: 29895.9997 - p10: 0.0347 - G10: 0.1314 - M10: 0.0923 - p20: 0.0334 - G20: 0.1642 - M20: 0.0921\n",
      "Epoch 327/500\n",
      "577/577 [==============================] - 327s 567ms/step - d_loss: -30095.1889 - g_loss: 29986.5372 - p10: 0.0344 - G10: 0.1244 - M10: 0.0839 - p20: 0.0340 - G20: 0.1609 - M20: 0.0884\n",
      "Epoch 328/500\n",
      "577/577 [==============================] - 328s 568ms/step - d_loss: -30186.0134 - g_loss: 30077.0034 - p10: 0.0307 - G10: 0.1206 - M10: 0.0810 - p20: 0.0335 - G20: 0.1656 - M20: 0.0874\n",
      "Epoch 329/500\n",
      "577/577 [==============================] - 336s 583ms/step - d_loss: -30276.9191 - g_loss: 30167.5112 - p10: 0.0335 - G10: 0.1307 - M10: 0.0916 - p20: 0.0323 - G20: 0.1731 - M20: 0.0986\n",
      "Epoch 330/500\n",
      "577/577 [==============================] - 335s 581ms/step - d_loss: -30367.8132 - g_loss: 30258.0060 - p10: 0.0293 - G10: 0.1353 - M10: 0.0994 - p20: 0.0300 - G20: 0.1697 - M20: 0.1018\n",
      "Epoch 331/500\n",
      "577/577 [==============================] - 332s 576ms/step - d_loss: -30458.6755 - g_loss: 30348.5422 - p10: 0.0335 - G10: 0.1383 - M10: 0.0970 - p20: 0.0353 - G20: 0.1759 - M20: 0.0971\n",
      "Epoch 332/500\n",
      "577/577 [==============================] - 332s 576ms/step - d_loss: -30549.5211 - g_loss: 30439.0500 - p10: 0.0330 - G10: 0.1389 - M10: 0.0988 - p20: 0.0300 - G20: 0.1664 - M20: 0.0978\n",
      "Epoch 333/500\n",
      "577/577 [==============================] - 333s 577ms/step - d_loss: -30640.3607 - g_loss: 30529.6392 - p10: 0.0349 - G10: 0.1368 - M10: 0.0931 - p20: 0.0326 - G20: 0.1682 - M20: 0.0967\n",
      "Epoch 334/500\n",
      "577/577 [==============================] - 333s 577ms/step - d_loss: -30731.2790 - g_loss: 30620.1430 - p10: 0.0321 - G10: 0.1311 - M10: 0.0973 - p20: 0.0335 - G20: 0.1695 - M20: 0.0974\n",
      "Epoch 335/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -30822.1686 - g_loss: 30710.6762 - p10: 0.0298 - G10: 0.1269 - M10: 0.0910 - p20: 0.0316 - G20: 0.1626 - M20: 0.0935\n",
      "Epoch 336/500\n",
      "577/577 [==============================] - 332s 576ms/step - d_loss: -30912.9886 - g_loss: 30801.1660 - p10: 0.0358 - G10: 0.1372 - M10: 0.0998 - p20: 0.0351 - G20: 0.1725 - M20: 0.1002\n",
      "Epoch 337/500\n",
      "577/577 [==============================] - 333s 577ms/step - d_loss: -31001.9481 - g_loss: 30891.7730 - p10: 0.0307 - G10: 0.1156 - M10: 0.0750 - p20: 0.0309 - G20: 0.1472 - M20: 0.0763\n",
      "Epoch 338/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -31094.5994 - g_loss: 30982.3476 - p10: 0.0307 - G10: 0.1335 - M10: 0.0950 - p20: 0.0319 - G20: 0.1713 - M20: 0.0964\n",
      "Epoch 339/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -31173.2219 - g_loss: 31072.7849 - p10: 0.0312 - G10: 0.1177 - M10: 0.0795 - p20: 0.0284 - G20: 0.1454 - M20: 0.0823\n",
      "Epoch 340/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -31276.1035 - g_loss: 31163.2437 - p10: 0.0391 - G10: 0.1358 - M10: 0.0890 - p20: 0.0347 - G20: 0.1671 - M20: 0.0889\n",
      "Epoch 341/500\n",
      "577/577 [==============================] - 332s 575ms/step - d_loss: -31366.9146 - g_loss: 31253.7258 - p10: 0.0340 - G10: 0.1376 - M10: 0.0961 - p20: 0.0328 - G20: 0.1689 - M20: 0.0928\n",
      "Epoch 342/500\n",
      "577/577 [==============================] - 330s 571ms/step - d_loss: -31457.8967 - g_loss: 31344.1581 - p10: 0.0335 - G10: 0.1269 - M10: 0.0844 - p20: 0.0319 - G20: 0.1616 - M20: 0.0875\n",
      "Epoch 343/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -31548.8149 - g_loss: 31434.7120 - p10: 0.0293 - G10: 0.1105 - M10: 0.0746 - p20: 0.0351 - G20: 0.1601 - M20: 0.0832\n",
      "Epoch 344/500\n",
      "577/577 [==============================] - 333s 577ms/step - d_loss: -31639.6848 - g_loss: 31525.2648 - p10: 0.0335 - G10: 0.1286 - M10: 0.0870 - p20: 0.0328 - G20: 0.1617 - M20: 0.0913\n",
      "Epoch 345/500\n",
      "577/577 [==============================] - 330s 573ms/step - d_loss: -31730.5823 - g_loss: 31615.7959 - p10: 0.0391 - G10: 0.1399 - M10: 0.0971 - p20: 0.0351 - G20: 0.1734 - M20: 0.0982\n",
      "Epoch 346/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -31821.3949 - g_loss: 31706.4199 - p10: 0.0340 - G10: 0.1373 - M10: 0.0964 - p20: 0.0298 - G20: 0.1715 - M20: 0.0994\n",
      "Epoch 347/500\n",
      "577/577 [==============================] - 333s 576ms/step - d_loss: -31912.2432 - g_loss: 31796.9200 - p10: 0.0293 - G10: 0.1121 - M10: 0.0757 - p20: 0.0288 - G20: 0.1555 - M20: 0.0867\n",
      "Epoch 348/500\n",
      "577/577 [==============================] - 331s 573ms/step - d_loss: -32003.1142 - g_loss: 31887.4393 - p10: 0.0326 - G10: 0.1296 - M10: 0.0844 - p20: 0.0288 - G20: 0.1526 - M20: 0.0825\n",
      "Epoch 349/500\n",
      "577/577 [==============================] - 330s 572ms/step - d_loss: -32093.9802 - g_loss: 31977.9753 - p10: 0.0358 - G10: 0.1298 - M10: 0.0909 - p20: 0.0356 - G20: 0.1678 - M20: 0.0956\n",
      "Epoch 350/500\n",
      "577/577 [==============================] - 330s 573ms/step - d_loss: -32184.8505 - g_loss: 32068.4578 - p10: 0.0326 - G10: 0.1232 - M10: 0.0862 - p20: 0.0307 - G20: 0.1501 - M20: 0.0821\n",
      "Epoch 351/500\n",
      "577/577 [==============================] - 331s 573ms/step - d_loss: -32275.7406 - g_loss: 32158.9561 - p10: 0.0330 - G10: 0.1363 - M10: 0.0960 - p20: 0.0305 - G20: 0.1639 - M20: 0.0942\n",
      "Epoch 352/500\n",
      "577/577 [==============================] - 362s 627ms/step - d_loss: -32366.6104 - g_loss: 32249.4633 - p10: 0.0363 - G10: 0.1328 - M10: 0.0915 - p20: 0.0326 - G20: 0.1542 - M20: 0.0862\n",
      "Epoch 353/500\n",
      "577/577 [==============================] - 329s 571ms/step - d_loss: -32457.4484 - g_loss: 32339.9880 - p10: 0.0409 - G10: 0.1517 - M10: 0.1070 - p20: 0.0358 - G20: 0.1763 - M20: 0.1051\n",
      "Epoch 354/500\n",
      "577/577 [==============================] - 348s 604ms/step - d_loss: -32548.2873 - g_loss: 32430.5246 - p10: 0.0353 - G10: 0.1328 - M10: 0.0868 - p20: 0.0321 - G20: 0.1616 - M20: 0.0906\n",
      "Epoch 355/500\n",
      "577/577 [==============================] - 343s 594ms/step - d_loss: -32639.1342 - g_loss: 32521.0320 - p10: 0.0386 - G10: 0.1159 - M10: 0.0728 - p20: 0.0344 - G20: 0.1533 - M20: 0.0783\n",
      "Epoch 356/500\n",
      "577/577 [==============================] - 367s 636ms/step - d_loss: -32730.0212 - g_loss: 32611.6232 - p10: 0.0316 - G10: 0.1112 - M10: 0.0741 - p20: 0.0309 - G20: 0.1495 - M20: 0.0771\n",
      "Epoch 357/500\n",
      "577/577 [==============================] - 386s 669ms/step - d_loss: -32820.9252 - g_loss: 32702.0551 - p10: 0.0344 - G10: 0.1273 - M10: 0.0866 - p20: 0.0342 - G20: 0.1610 - M20: 0.0915\n",
      "Epoch 358/500\n",
      "577/577 [==============================] - 334s 578ms/step - d_loss: -32911.7196 - g_loss: 32792.5651 - p10: 0.0316 - G10: 0.1241 - M10: 0.0902 - p20: 0.0307 - G20: 0.1555 - M20: 0.0890\n",
      "Epoch 359/500\n",
      "577/577 [==============================] - 333s 577ms/step - d_loss: -33002.6395 - g_loss: 32883.1074 - p10: 0.0293 - G10: 0.1216 - M10: 0.0830 - p20: 0.0342 - G20: 0.1643 - M20: 0.0858\n",
      "Epoch 360/500\n",
      "577/577 [==============================] - 332s 576ms/step - d_loss: -33093.4611 - g_loss: 32973.6180 - p10: 0.0321 - G10: 0.1256 - M10: 0.0838 - p20: 0.0321 - G20: 0.1531 - M20: 0.0801\n",
      "Epoch 361/500\n",
      "577/577 [==============================] - 394s 684ms/step - d_loss: -33184.3408 - g_loss: 33064.1291 - p10: 0.0288 - G10: 0.1161 - M10: 0.0793 - p20: 0.0295 - G20: 0.1507 - M20: 0.0818\n",
      "Epoch 362/500\n",
      "577/577 [==============================] - 345s 598ms/step - d_loss: -33275.2039 - g_loss: 33154.6667 - p10: 0.0349 - G10: 0.1246 - M10: 0.0816 - p20: 0.0323 - G20: 0.1591 - M20: 0.0871\n",
      "Epoch 363/500\n",
      "577/577 [==============================] - 333s 578ms/step - d_loss: -33366.1097 - g_loss: 33245.1817 - p10: 0.0340 - G10: 0.1218 - M10: 0.0881 - p20: 0.0330 - G20: 0.1557 - M20: 0.0896\n",
      "Epoch 364/500\n",
      "577/577 [==============================] - 393s 681ms/step - d_loss: -33456.9774 - g_loss: 33335.7003 - p10: 0.0316 - G10: 0.1141 - M10: 0.0759 - p20: 0.0309 - G20: 0.1551 - M20: 0.0845\n",
      "Epoch 365/500\n",
      "577/577 [==============================] - 367s 637ms/step - d_loss: -33547.8393 - g_loss: 33426.1802 - p10: 0.0340 - G10: 0.1272 - M10: 0.0853 - p20: 0.0342 - G20: 0.1594 - M20: 0.0881\n",
      "Epoch 366/500\n",
      "577/577 [==============================] - 354s 613ms/step - d_loss: -33638.6084 - g_loss: 33516.6947 - p10: 0.0381 - G10: 0.1427 - M10: 0.1005 - p20: 0.0351 - G20: 0.1722 - M20: 0.1011\n",
      "Epoch 367/500\n",
      "577/577 [==============================] - 333s 577ms/step - d_loss: -33729.0256 - g_loss: 33607.1223 - p10: 0.0316 - G10: 0.1315 - M10: 0.0914 - p20: 0.0300 - G20: 0.1692 - M20: 0.0974\n",
      "Epoch 368/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -33819.7532 - g_loss: 33697.5601 - p10: 0.0353 - G10: 0.1363 - M10: 0.0945 - p20: 0.0316 - G20: 0.1681 - M20: 0.0927\n",
      "Epoch 369/500\n",
      "577/577 [==============================] - 332s 576ms/step - d_loss: -33910.6362 - g_loss: 33788.1105 - p10: 0.0274 - G10: 0.1061 - M10: 0.0752 - p20: 0.0295 - G20: 0.1445 - M20: 0.0763\n",
      "Epoch 370/500\n",
      "577/577 [==============================] - 393s 681ms/step - d_loss: -34001.5232 - g_loss: 33878.6631 - p10: 0.0326 - G10: 0.1261 - M10: 0.0849 - p20: 0.0295 - G20: 0.1572 - M20: 0.0846\n",
      "Epoch 371/500\n",
      "577/577 [==============================] - 386s 668ms/step - d_loss: -34092.4106 - g_loss: 33969.1711 - p10: 0.0335 - G10: 0.1136 - M10: 0.0749 - p20: 0.0284 - G20: 0.1381 - M20: 0.0733\n",
      "Epoch 372/500\n",
      "577/577 [==============================] - 362s 628ms/step - d_loss: -34183.3133 - g_loss: 34059.5980 - p10: 0.0316 - G10: 0.1249 - M10: 0.0866 - p20: 0.0307 - G20: 0.1527 - M20: 0.0837\n",
      "Epoch 373/500\n",
      "577/577 [==============================] - 380s 659ms/step - d_loss: -34274.1624 - g_loss: 34150.0741 - p10: 0.0353 - G10: 0.1338 - M10: 0.0935 - p20: 0.0319 - G20: 0.1607 - M20: 0.0918\n",
      "Epoch 374/500\n",
      "577/577 [==============================] - 331s 573ms/step - d_loss: -34365.0630 - g_loss: 34240.6054 - p10: 0.0335 - G10: 0.1230 - M10: 0.0775 - p20: 0.0358 - G20: 0.1666 - M20: 0.0777\n",
      "Epoch 375/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -34455.9051 - g_loss: 34331.1343 - p10: 0.0358 - G10: 0.1339 - M10: 0.0939 - p20: 0.0335 - G20: 0.1674 - M20: 0.0919\n",
      "Epoch 376/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -34546.7797 - g_loss: 34421.6786 - p10: 0.0353 - G10: 0.1388 - M10: 0.1002 - p20: 0.0326 - G20: 0.1663 - M20: 0.1004\n",
      "Epoch 377/500\n",
      "577/577 [==============================] - 332s 575ms/step - d_loss: -34637.5749 - g_loss: 34512.2113 - p10: 0.0349 - G10: 0.1311 - M10: 0.0934 - p20: 0.0351 - G20: 0.1723 - M20: 0.0982\n",
      "Epoch 378/500\n",
      "577/577 [==============================] - 332s 575ms/step - d_loss: -34728.4789 - g_loss: 34602.7215 - p10: 0.0349 - G10: 0.1403 - M10: 0.0984 - p20: 0.0342 - G20: 0.1689 - M20: 0.0950\n",
      "Epoch 379/500\n",
      "577/577 [==============================] - 331s 574ms/step - d_loss: -34819.2923 - g_loss: 34693.2336 - p10: 0.0316 - G10: 0.1235 - M10: 0.0863 - p20: 0.0333 - G20: 0.1688 - M20: 0.0898\n",
      "Epoch 380/500\n",
      "577/577 [==============================] - 333s 576ms/step - d_loss: -34910.1860 - g_loss: 34783.6883 - p10: 0.0395 - G10: 0.1342 - M10: 0.0884 - p20: 0.0335 - G20: 0.1635 - M20: 0.0892\n",
      "Epoch 381/500\n",
      "577/577 [==============================] - 330s 573ms/step - d_loss: -35001.0758 - g_loss: 34874.2672 - p10: 0.0363 - G10: 0.1313 - M10: 0.0888 - p20: 0.0326 - G20: 0.1637 - M20: 0.0905\n",
      "Epoch 382/500\n",
      "577/577 [==============================] - 332s 575ms/step - d_loss: -35091.9561 - g_loss: 34964.8251 - p10: 0.0381 - G10: 0.1503 - M10: 0.1026 - p20: 0.0340 - G20: 0.1739 - M20: 0.0987\n",
      "Epoch 383/500\n",
      "577/577 [==============================] - 333s 577ms/step - d_loss: -35182.8481 - g_loss: 35055.3449 - p10: 0.0344 - G10: 0.1435 - M10: 0.1058 - p20: 0.0328 - G20: 0.1781 - M20: 0.1091\n",
      "Epoch 384/500\n",
      "577/577 [==============================] - 329s 571ms/step - d_loss: -35273.7179 - g_loss: 35145.8440 - p10: 0.0419 - G10: 0.1556 - M10: 0.1060 - p20: 0.0360 - G20: 0.1756 - M20: 0.1008\n",
      "Epoch 385/500\n",
      "460/577 [======================>.......] - ETA: 1:06 - d_loss: -35355.2897 - g_loss: 35227.0720 - p10: 0.0344 - G10: 0.1146 - M10: 0.0754 - p20: 0.0314 - G20: 0.1463 - M20: 0.0767"
     ]
    }
   ],
   "source": [
    "# Fit \n",
    "epochs = 500\n",
    "\n",
    "# Instantiate the WGAN model.\n",
    "wgan = WGAN(\n",
    "    discriminator=discriminator,\n",
    "    generator=generator,\n",
    "    discriminator_extra_steps=3\n",
    ")\n",
    "\n",
    "# Compile the WGAN model.\n",
    "wgan.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    g_optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    g_loss_fn=generator_loss,\n",
    "    d_loss_fn=discriminator_loss,\n",
    "    c_loss_fn = counter_loss,\n",
    "    run_eagerly=True)\n",
    "\n",
    "# Start training the model.\n",
    "fit = wgan.fit(train, batch_size=344, epochs=epochs, verbose=True)\n",
    "print(wgan.test_step(10), \"\\n\", wgan.test_step(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
