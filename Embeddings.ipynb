{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "265e6f95-ba4a-466d-b071-31058775918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fa_support\n",
    "import evall\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e4e1fb7-5da9-4b87-b5ad-4fcba59f63c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train =  np.load(\"new_fa_train_data.npy\")\n",
    "#test = np.load(\"new_fa_test_data.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51cef4df-af88-483e-a259-08148677ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [  310, 8, 53, 3, 13, 4, 3, 78]\n",
    "names = ['brand_id',\n",
    "         'category', 'colour', 'divisioncode', 'itemcategorycode',\n",
    "         'itemfamilycode', 'itemseason', 'productgroup']\n",
    "dic = dict(zip(names,numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae36973b-3e9c-48e2-8c55-55820e6a81ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dic =pd.read_csv(\"Eval_dic\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54ad2b36-8573-48c3-877f-47ec2e06af21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WGAN</th>\n",
       "      <th>WGAN-GP_100_epochs_lr_0.0001</th>\n",
       "      <th>GAN_lr.0001_extrastep_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.431059</td>\n",
       "      <td>0.478360</td>\n",
       "      <td>0.250895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.475948</td>\n",
       "      <td>0.524330</td>\n",
       "      <td>0.288934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.354883</td>\n",
       "      <td>0.394489</td>\n",
       "      <td>0.200178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.378315</td>\n",
       "      <td>0.421206</td>\n",
       "      <td>0.216806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.311600</td>\n",
       "      <td>0.350700</td>\n",
       "      <td>0.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.338500</td>\n",
       "      <td>0.375900</td>\n",
       "      <td>0.203800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       WGAN  WGAN-GP_100_epochs_lr_0.0001  GAN_lr.0001_extrastep_1\n",
       "0  0.431059                      0.478360                 0.250895\n",
       "1  0.475948                      0.524330                 0.288934\n",
       "2  0.354883                      0.394489                 0.200178\n",
       "3  0.378315                      0.421206                 0.216806\n",
       "4  0.311600                      0.350700                 0.179600\n",
       "5  0.338500                      0.375900                 0.203800"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af470afe-7dd7-43d2-9342-6faa64021b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_emb_dim = sum(dic.values())\n",
    "\n",
    "D_emb_dim = 50\n",
    "\n",
    "G_emb_dim = 50\n",
    "\n",
    "hidden_dim = 128\n",
    "alpha = 0\n",
    "\n",
    "# Initializer\n",
    "init = tf.initializers.glorot_normal()\n",
    "\n",
    "'''Generator and Discriminator Attribute Embeddings'''\n",
    "#D_price_embs = tf.keras.layers.Embedding(input_dim = dic['pricetype'], output_dim = D_emb_dim,\n",
    " #                                         trainable=True, weights = [init(shape=( dic['pricetype'],D_emb_dim))])\n",
    "D_brand_embs = tf.keras.layers.Embedding(input_dim = dic['brand_id'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['brand_id'],D_emb_dim))])\n",
    "D_category_embs = tf.keras.layers.Embedding(input_dim = dic['category'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['category'],D_emb_dim))])\n",
    "D_colour_embs = tf.keras.layers.Embedding(input_dim = dic['colour'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['colour'],D_emb_dim))])\n",
    "D_div_embs = tf.keras.layers.Embedding(input_dim = dic['divisioncode'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['divisioncode'],D_emb_dim))])\n",
    "D_itemcat_embs = tf.keras.layers.Embedding(input_dim = dic['itemcategorycode'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemcategorycode'],D_emb_dim))])\n",
    "D_itemfam_embs = tf.keras.layers.Embedding(input_dim = dic['itemfamilycode'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemfamilycode'],D_emb_dim))])\n",
    "D_season_embs = tf.keras.layers.Embedding(input_dim = dic['itemseason'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemseason'],D_emb_dim))])\n",
    "D_prod_embs = tf.keras.layers.Embedding(input_dim = dic['productgroup'], output_dim = D_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['productgroup'],D_emb_dim))])\n",
    "\n",
    "\n",
    "#G_price_embs = tf.keras.layers.Embedding(input_dim = dic['pricetype'], output_dim = G_emb_dim,\n",
    "#                                               trainable=True, weights = [init(shape=( dic['pricetype'],G_emb_dim))])\n",
    "G_brand_embs = tf.keras.layers.Embedding(input_dim = dic['brand_id'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['brand_id'],G_emb_dim))])\n",
    "G_category_embs = tf.keras.layers.Embedding(input_dim = dic['category'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['category'],G_emb_dim))])\n",
    "G_colour_embs = tf.keras.layers.Embedding(input_dim = dic['colour'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['colour'],G_emb_dim))])\n",
    "G_div_embs = tf.keras.layers.Embedding(input_dim = dic['divisioncode'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['divisioncode'],G_emb_dim))])\n",
    "G_itemcat_embs = tf.keras.layers.Embedding(input_dim = dic['itemcategorycode'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemcategorycode'],G_emb_dim))])\n",
    "G_itemfam_embs = tf.keras.layers.Embedding(input_dim = dic['itemfamilycode'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemfamilycode'],G_emb_dim))])\n",
    "G_season_embs = tf.keras.layers.Embedding(input_dim = dic['itemseason'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['itemseason'],G_emb_dim))])\n",
    "G_prod_embs = tf.keras.layers.Embedding(input_dim = dic['productgroup'], output_dim = G_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=( dic['productgroup'],G_emb_dim))])\n",
    "\n",
    "\n",
    "# Model input sizes\n",
    "G_input_size =  G_emb_dim*8\n",
    "D_input_size = user_emb_dim + D_emb_dim*8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0ff9300-5d36-427c-925d-7413485511a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generator_input( brand_id, category, colour, divisioncode, itemcategorycode, itemfamilycode, itemseason, productgroup):\n",
    "    emb_dic = {}\n",
    "#    dic[\"pricetype\"] = G_brand_embs(tf.constant(pricetype))\n",
    "    dic[\"brand_id\"] = G_brand_embs(tf.constant(brand_id))\n",
    "    dic[\"category\"] = G_brand_embs(tf.constant(category))\n",
    "    dic[\"colour\"] = G_brand_embs(tf.constant(colour))\n",
    "    dic[\"divisioncode\"] = G_brand_embs(tf.constant(divisioncode))\n",
    "    dic[\"itemcategorycode\"] = G_brand_embs(tf.constant(itemcategorycode))\n",
    "    dic[\"itemfamilycode\"] = G_brand_embs(tf.constant(itemfamilycode))\n",
    "    dic[\"itemseason\"] = G_brand_embs(tf.constant(itemseason))\n",
    "    dic[\"productgroup\"] = G_brand_embs(tf.constant(productgroup))\n",
    "    emb = tf.keras.layers.concatenate(list(dic.values()), 1)\n",
    "    return emb\n",
    "\n",
    "# Generates user based on concatenation of all attributes\n",
    "def generator():\n",
    "    bc_input = tf.keras.layers.Input(shape=(G_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', kernel_regularizer = 'l2')(bc_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(user_emb_dim, activation ='sigmoid', kernel_regularizer = 'l2')(x)\n",
    "    g_model = tf.keras.models.Model(bc_input, x, name = 'generator')\n",
    "    return g_model\n",
    "g_model = generator()\n",
    "\n",
    "def discriminator_input( brand_id, category, colour, divisioncode, itemcategorycode, itemfamilycode, \n",
    "                        itemseason, productgroup, user_emb):\n",
    "    emb_dic = {}\n",
    "#    dic[\"pricetype\"]  = D_brand_embs(tf.constant(pricetype))\n",
    "    dic[\"brand_id\"]  = D_brand_embs(tf.constant(brand_id))\n",
    "    dic[\"category\"]  = D_brand_embs(tf.constant(category))\n",
    "    dic[\"colour\"]  = D_brand_embs(tf.constant(colour))\n",
    "    dic[\"divisioncode\"]  = D_brand_embs(tf.constant(divisioncode))\n",
    "    dic[\"itemcategorycode\"]  = D_brand_embs(tf.constant(itemcategorycode))\n",
    "    dic[\"itemfamilycode\"]  = D_brand_embs(tf.constant(itemfamilycode))\n",
    "    dic[\"itemseason\"]  = D_brand_embs(tf.constant(itemseason))\n",
    "    dic[\"productgroup\"]  = D_brand_embs(tf.constant(productgroup))\n",
    "    user_emb = tf.cast(user_emb, dtype=float)\n",
    "    emb = tf.keras.layers.concatenate(list(dic.values()), 1)\n",
    "    final_emb = tf.keras.layers.concatenate([emb, user_emb], 1)\n",
    "    return final_emb\n",
    "\n",
    "def discriminator():\n",
    "    d_input = tf.keras.layers.Input(shape=(D_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', kernel_regularizer = 'l2')(d_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.models.Model(d_input, x, name = 'discriminator')\n",
    "    return model\n",
    "d_model = discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3bea43e-2f9e-44a8-863a-38787bb9212c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Cross entropy loss means\\ndef generator_loss(d_logits):\\n    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits, labels=tf.ones_like(d_logits)))\\n\\ndef discriminator_loss(real, fake):\\n    logit = tf.reduce_mean(fake-real)\\n    r = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real, labels=tf.ones_like(real)))\\n    f = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake, labels=tf.zeros_like(fake)))\\n    return r+f\\n\\ndef counter_loss(counter):\\n    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=counter, labels=tf.zeros_like(counter))) '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Loss functions'''\n",
    "def generator_loss(fake_user):\n",
    "    return -tf.reduce_mean(fake_user)\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    logit = tf.reduce_mean(fake- real)\n",
    "    return logit\n",
    "\n",
    "def counter_loss(counter):\n",
    "    return tf.reduce_mean(counter)\n",
    "\n",
    "def discriminator_counter_loss(real, fake, counter):\n",
    "    logit = tf.reduce_mean(real - counter - fake)\n",
    "    return logit\n",
    "''' Cross entropy loss means\n",
    "def generator_loss(d_logits):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits, labels=tf.ones_like(d_logits)))\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    logit = tf.reduce_mean(fake-real)\n",
    "    r = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real, labels=tf.ones_like(real)))\n",
    "    f = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake, labels=tf.zeros_like(fake)))\n",
    "    return r+f\n",
    "\n",
    "def counter_loss(counter):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=counter, labels=tf.zeros_like(counter))) '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10f38626-bccd-4a7f-a18a-89344490c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui_matrix = np.load(\"fa_ui_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa2d2b58-3d17-4d9d-9b81-8e10ea450422",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WGAN Class\n",
    "class WGAN(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        discriminator_extra_steps=5,\n",
    "        batch_size = 1000\n",
    "    ):\n",
    "        super(WGAN, self).__init__()\n",
    "        self.discriminator = d_model\n",
    "        self.generator = g_model\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.k = 10        \n",
    "        self.index = 0 \n",
    "        self.c_index = 0 \n",
    "        self.gp_weight = 10\n",
    "        self.eval_steps = 0\n",
    "        self.max_p10 = .01 \n",
    "        self.max_g10 = .01\n",
    "        self.max_m10 = .01\n",
    "        self.max_p20 = .01\n",
    "        self.max_g20 = .01\n",
    "        self.max_m20 = .01\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn,c_loss_fn, run_eagerly):\n",
    "        super(WGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.c_loss_fn = c_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "        self.run_eagerly = run_eagerly\n",
    "\n",
    "\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_users, fake_users,  brand_id,\\\n",
    "                                    category, colour, divisioncode, itemcategorycode, itemfamilycode, \\\n",
    "                                    itemseason, productgroup):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated image\n",
    "        alpha = tf.random.normal([batch_size,1], 0.0, 1.0)\n",
    "        diff = fake_users - real_users\n",
    "        interpolated = real_users + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            interpolated_input = discriminator_input(   brand_id,\\\n",
    "                                    category, colour, divisioncode, itemcategorycode, itemfamilycode, \\\n",
    "                                    itemseason, productgroup, interpolated)\n",
    "            pred = self.discriminator(interpolated_input)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = tape.gradient(pred, [interpolated])[0] #+1e-10\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2) +1e-16\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, real_users):\n",
    "        self.eval_steps +=1 \n",
    "        c_batch_size = 2*self.batch_size\n",
    "        for i in range(self.d_steps):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Get batch data\n",
    "                country, postcode, loyal, gender,  brand_id, category, colour, divisioncode, \\\n",
    "                itemcategorycode, itemfamilycode, itemseason, productgroup, real_users, items = fa_support.get_batchdata(self.index, self.index + self.batch_size)\n",
    "                # Get batch of counter examples\n",
    "                counter_brand_id, counter_category, counter_colour, counter_divisioncode, \\\n",
    "                counter_itemcategorycode, counter_itemfamilycode, counter_itemseason, \\\n",
    "                counter_productgroup,  counter_users = fa_support.get_counter_batch(self.c_index, self.c_index + c_batch_size)\n",
    "                # Generate fake users from attributes\n",
    "                g_input0 = generator_input(  brand_id, category, colour, divisioncode, \\\n",
    "                                           itemcategorycode, itemfamilycode, itemseason, productgroup)\n",
    "                fake_users = self.generator(g_input0)\n",
    "                # Get the logits for the fake users\n",
    "                d_input0 = discriminator_input( brand_id, category, colour, divisioncode, \\\n",
    "                                           itemcategorycode, itemfamilycode, itemseason, productgroup, fake_users)\n",
    "                fake_logits = self.discriminator(d_input0)\n",
    "                # Get the logits for the real user\n",
    "                d_input1 = discriminator_input(  brand_id, category, colour, divisioncode, \\\n",
    "                                           itemcategorycode, itemfamilycode, itemseason, productgroup, real_users)\n",
    "                real_logits = self.discriminator(d_input1)\n",
    "                # Get logits for counter examples\n",
    "                \n",
    "                d_input2 = discriminator_input( counter_brand_id, counter_category, counter_colour, counter_divisioncode, \\\n",
    "                counter_itemcategorycode, counter_itemfamilycode, counter_itemseason, \\\n",
    "                counter_productgroup,  counter_users)\n",
    "                counter_logits = self.discriminator(d_input2)\n",
    "                # Calculate the discriminator loss using the fake and real image logits\n",
    "                d_cost = self.d_loss_fn(real_logits, fake_logits)\n",
    "                c_loss = self.c_loss_fn(counter_logits)\n",
    "                # Get gradient penalty\n",
    "                gp = self.gradient_penalty(self.batch_size, real_users, fake_users, brand_id,\\\n",
    "                                    category, colour, divisioncode, itemcategorycode, itemfamilycode, \\\n",
    "                                    itemseason, productgroup)\n",
    "                # Later add counter loss\n",
    "                d_loss = d_cost + c_loss +1e-10  + gp*self.gp_weight \n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(zip(d_gradient, self.discriminator.trainable_variables))\n",
    "\n",
    "        # Train the generator\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Generate fake images using the generator\n",
    "            g_input1 = generator_input(  brand_id, category, colour, divisioncode, \\\n",
    "                                           itemcategorycode, itemfamilycode, itemseason, productgroup)\n",
    "            gen_users = self.generator(g_input1)\n",
    "            # Get the discriminator logits for fake images\n",
    "            d_input2 = discriminator_input(  brand_id, category, colour, divisioncode, \\\n",
    "                                           itemcategorycode, itemfamilycode, itemseason, productgroup, gen_users)\n",
    "            gen_logits = self.discriminator(d_input2)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_logits) +1e-10\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        '''if self.eval_steps %760==0:\n",
    "            p_at_10,G_at_10,M_at_10 = wgan.test_step(10)\n",
    "            p_at_20,G_at_20,M_at_20 = wgan.test_step(20)\n",
    "            if p_at_10 > self.max_p10:\n",
    "                self.max_p10 = p_at_10\n",
    "            if G_at_10 > self.max_g10:\n",
    "                self.max_g10 = G_at_10\n",
    "            if M_at_10 > self.max_m10:\n",
    "                self.max_m10 = M_at_10\n",
    "            if p_at_20 > self.max_p20:\n",
    "                self.max_p20 = p_at_20\n",
    "            if G_at_20 > self.max_g20:\n",
    "                self.max_g20 = G_at_20\n",
    "            if M_at_20 > self.max_m20:\n",
    "                self.max_m20 = M_at_20\n",
    "            \n",
    "            return {\"d_loss\": d_loss, \"g_loss\": g_loss, \"p10\":p_at_10,\n",
    "                           \"G10\":G_at_10,\"M10\":M_at_10, \"p20\": p_at_20,\"G20\":G_at_20,\"M20\":M_at_20}\n",
    "        else:'''\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "\n",
    "    def test_step(self, k):\n",
    "        \n",
    "        country, postcode, loyal, gender,  brand_id, category, colour, divisioncode, \\\n",
    "        itemcategorycode, itemfamilycode, itemseason, productgroup, item, user = fa_support.get_testdata(0,5000)\n",
    "        \n",
    "        test_BATCH_SIZE = country.size\n",
    "        g_input1 = generator_input( brand_id, category, colour, divisioncode, \\\n",
    "                                           itemcategorycode, itemfamilycode, itemseason, productgroup)\n",
    "        gen_users = self.generator(g_input1)\n",
    "        sim_users = fa_support.get_intersection_similar_user(gen_users, k)\n",
    "        count = 0\n",
    "\n",
    "        count = 0\n",
    "        for i, user_list in zip(item, sim_users):       \n",
    "            for u in user_list:\n",
    "                if ui_matrix[u,i] == 1:\n",
    "                    count = count + 1            \n",
    "        p_at_k = round(count/(test_BATCH_SIZE * k), 4)\n",
    "\n",
    "        RS = []\n",
    "        ans = 0.0\n",
    "        for i, user_list in zip(item, sim_users):           \n",
    "            r=[]\n",
    "            for user in user_list:\n",
    "                 r.append(ui_matrix[user][i])\n",
    "            ans = ans + evall.ndcg_at_k(r, k, method=1)\n",
    "            RS.append(r)\n",
    "        G_at_k = ans/test_BATCH_SIZE\n",
    "        M_at_k = evall.mean_average_precision(RS)\n",
    "       \n",
    "\n",
    "        return p_at_k,G_at_k,M_at_k\n",
    "    \n",
    "    def recommend(self, input_data, k):\n",
    "        country, postcode, loyal, gender,  brand_id, category, colour, divisioncode, \\\n",
    "        itemcategorycode, itemfamilycode, itemseason, productgroup, item, user = input_data\n",
    "        \n",
    "        test_BATCH_SIZE = country.size\n",
    "        g_input1 = generator_input( brand_id, category, colour, divisioncode, \\\n",
    "                                           itemcategorycode, itemfamilycode, itemseason, productgroup)\n",
    "        gen_users = self.generator(g_input1)\n",
    "        sim_users = fa_support.get_intersection_similar_user(gen_users, k)\n",
    "        return sim_users\n",
    "    def evaluate_sample(self, input_data, k):\n",
    "        country, postcode, loyal, gender,  brand_id, category, colour, divisioncode, \\\n",
    "        itemcategorycode, itemfamilycode, itemseason, productgroup, item, user = input_data\n",
    "        \n",
    "        test_BATCH_SIZE = country.size\n",
    "        g_input1 = generator_input( brand_id, category, colour, divisioncode, \\\n",
    "                                           itemcategorycode, itemfamilycode, itemseason, productgroup)\n",
    "        gen_users = self.generator(g_input1)\n",
    "        sim_users = fa_support.get_intersection_similar_user(gen_users, k)\n",
    "        count = 0\n",
    "\n",
    "        count = 0\n",
    "        for i, user_list in zip(item, sim_users):       \n",
    "            for u in user_list:\n",
    "                if ui_matrix[u,i] == 1:\n",
    "                    count = count + 1            \n",
    "        p_at_k = round(count/(test_BATCH_SIZE * k), 4)\n",
    "\n",
    "        RS = []\n",
    "        ans = 0.0\n",
    "        for i, user_list in zip(item, sim_users):           \n",
    "            r=[]\n",
    "            for user in user_list:\n",
    "                 r.append(ui_matrix[user][i])\n",
    "            ans = ans + evall.ndcg_at_k(r, k, method=1)\n",
    "            RS.append(r)\n",
    "        G_at_k = ans/test_BATCH_SIZE\n",
    "        M_at_k = evall.mean_average_precision(RS)\n",
    "       \n",
    "\n",
    "        return p_at_k,G_at_k,M_at_k\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddc4f634-0903-4a69-bb17-bf613c911d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "762/762 [==============================] - 154s 179ms/step - d_loss: -22.5297 - g_loss: 20.7297\n",
      "Epoch 2/100\n",
      "762/762 [==============================] - 137s 179ms/step - d_loss: -56.9547 - g_loss: 54.9851\n",
      "Epoch 3/100\n",
      "762/762 [==============================] - 137s 180ms/step - d_loss: -99.4468 - g_loss: 96.2456\n",
      "Epoch 4/100\n",
      "762/762 [==============================] - 139s 182ms/step - d_loss: -147.0418 - g_loss: 143.1073\n",
      "Epoch 5/100\n",
      "762/762 [==============================] - 136s 179ms/step - d_loss: -194.5206 - g_loss: 189.9387\n",
      "Epoch 6/100\n",
      "762/762 [==============================] - 136s 179ms/step - d_loss: -242.9748 - g_loss: 237.8098\n",
      "Epoch 7/100\n",
      "762/762 [==============================] - 137s 180ms/step - d_loss: -291.8533 - g_loss: 286.2440\n",
      "Epoch 8/100\n",
      "762/762 [==============================] - 136s 179ms/step - d_loss: -340.5409 - g_loss: 334.7280\n",
      "Epoch 9/100\n",
      "762/762 [==============================] - 137s 180ms/step - d_loss: -389.4988 - g_loss: 383.3969\n",
      "Epoch 10/100\n",
      "762/762 [==============================] - 136s 178ms/step - d_loss: -438.6985 - g_loss: 432.0461\n",
      "Epoch 11/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -487.7491 - g_loss: 480.5405\n",
      "Epoch 12/100\n",
      "762/762 [==============================] - 137s 179ms/step - d_loss: -536.8441 - g_loss: 529.0943\n",
      "Epoch 13/100\n",
      "762/762 [==============================] - 136s 179ms/step - d_loss: -585.9499 - g_loss: 577.6279\n",
      "Epoch 14/100\n",
      "762/762 [==============================] - 137s 180ms/step - d_loss: -635.0606 - g_loss: 626.2115\n",
      "Epoch 15/100\n",
      "762/762 [==============================] - 137s 179ms/step - d_loss: -684.1925 - g_loss: 674.8503\n",
      "Epoch 16/100\n",
      "762/762 [==============================] - 137s 180ms/step - d_loss: -733.3295 - g_loss: 723.4120\n",
      "Epoch 17/100\n",
      "762/762 [==============================] - 137s 180ms/step - d_loss: -782.4709 - g_loss: 771.9289\n",
      "Epoch 18/100\n",
      "762/762 [==============================] - 139s 183ms/step - d_loss: -831.6178 - g_loss: 820.5004\n",
      "Epoch 19/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -880.7742 - g_loss: 869.0866\n",
      "Epoch 20/100\n",
      "762/762 [==============================] - 138s 182ms/step - d_loss: -929.9380 - g_loss: 917.6489\n",
      "Epoch 21/100\n",
      "762/762 [==============================] - 139s 182ms/step - d_loss: -979.1113 - g_loss: 966.1750\n",
      "Epoch 22/100\n",
      "762/762 [==============================] - 139s 183ms/step - d_loss: -1028.2759 - g_loss: 1014.6689\n",
      "Epoch 23/100\n",
      "762/762 [==============================] - 140s 183ms/step - d_loss: -1077.4632 - g_loss: 1063.1952\n",
      "Epoch 24/100\n",
      "762/762 [==============================] - 141s 185ms/step - d_loss: -1126.6474 - g_loss: 1111.7584\n",
      "Epoch 25/100\n",
      "762/762 [==============================] - 139s 183ms/step - d_loss: -1175.8415 - g_loss: 1160.2982\n",
      "Epoch 26/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -1225.0383 - g_loss: 1208.8552\n",
      "Epoch 27/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -1274.2343 - g_loss: 1257.4250\n",
      "Epoch 28/100\n",
      "762/762 [==============================] - 137s 180ms/step - d_loss: -1323.4388 - g_loss: 1305.9649\n",
      "Epoch 29/100\n",
      "762/762 [==============================] - 139s 183ms/step - d_loss: -1372.6441 - g_loss: 1354.5399\n",
      "Epoch 30/100\n",
      "762/762 [==============================] - 140s 184ms/step - d_loss: -1421.8522 - g_loss: 1403.1160\n",
      "Epoch 31/100\n",
      "762/762 [==============================] - 139s 182ms/step - d_loss: -1471.0634 - g_loss: 1451.7040\n",
      "Epoch 32/100\n",
      "762/762 [==============================] - 138s 182ms/step - d_loss: -1520.2701 - g_loss: 1500.2430\n",
      "Epoch 33/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -1569.4808 - g_loss: 1548.7807\n",
      "Epoch 34/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -1618.6936 - g_loss: 1597.2690\n",
      "Epoch 35/100\n",
      "762/762 [==============================] - 139s 182ms/step - d_loss: -1667.9053 - g_loss: 1645.7783\n",
      "Epoch 36/100\n",
      "762/762 [==============================] - 137s 180ms/step - d_loss: -1717.1195 - g_loss: 1694.3496\n",
      "Epoch 37/100\n",
      "762/762 [==============================] - 139s 182ms/step - d_loss: -1766.3298 - g_loss: 1742.8780\n",
      "Epoch 38/100\n",
      "762/762 [==============================] - 139s 182ms/step - d_loss: -1815.5462 - g_loss: 1791.3487\n",
      "Epoch 39/100\n",
      "762/762 [==============================] - 138s 182ms/step - d_loss: -1864.7615 - g_loss: 1839.9180\n",
      "Epoch 40/100\n",
      "762/762 [==============================] - 139s 183ms/step - d_loss: -1913.9765 - g_loss: 1888.4657\n",
      "Epoch 41/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -1963.1909 - g_loss: 1936.9990\n",
      "Epoch 42/100\n",
      "762/762 [==============================] - 137s 179ms/step - d_loss: -2012.3880 - g_loss: 1985.4961\n",
      "Epoch 43/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -2061.3886 - g_loss: 2033.7855\n",
      "Epoch 44/100\n",
      "762/762 [==============================] - 139s 182ms/step - d_loss: -2110.2684 - g_loss: 2082.0044\n",
      "Epoch 45/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -2159.0775 - g_loss: 2130.1253\n",
      "Epoch 46/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -2207.8552 - g_loss: 2178.1862\n",
      "Epoch 47/100\n",
      "762/762 [==============================] - 137s 180ms/step - d_loss: -2256.6204 - g_loss: 2226.2577\n",
      "Epoch 48/100\n",
      "762/762 [==============================] - 137s 180ms/step - d_loss: -2305.3741 - g_loss: 2274.3516\n",
      "Epoch 49/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -2354.1239 - g_loss: 2322.3730\n",
      "Epoch 50/100\n",
      "762/762 [==============================] - 139s 183ms/step - d_loss: -2402.8730 - g_loss: 2370.4167\n",
      "Epoch 51/100\n",
      "762/762 [==============================] - 137s 180ms/step - d_loss: -2451.6209 - g_loss: 2418.5039\n",
      "Epoch 52/100\n",
      "762/762 [==============================] - 139s 183ms/step - d_loss: -2500.3679 - g_loss: 2466.5305\n",
      "Epoch 53/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -2549.1153 - g_loss: 2514.6209\n",
      "Epoch 54/100\n",
      "762/762 [==============================] - 139s 182ms/step - d_loss: -2597.8626 - g_loss: 2562.6648\n",
      "Epoch 55/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -2646.6084 - g_loss: 2610.7117\n",
      "Epoch 56/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -2695.3556 - g_loss: 2658.7418\n",
      "Epoch 57/100\n",
      "762/762 [==============================] - 138s 182ms/step - d_loss: -2744.1029 - g_loss: 2706.7853\n",
      "Epoch 58/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -2792.8500 - g_loss: 2754.7790\n",
      "Epoch 59/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -2841.5975 - g_loss: 2802.8036\n",
      "Epoch 60/100\n",
      "762/762 [==============================] - 137s 179ms/step - d_loss: -2890.3438 - g_loss: 2850.8333\n",
      "Epoch 61/100\n",
      "762/762 [==============================] - 138s 180ms/step - d_loss: -2939.0893 - g_loss: 2898.8908\n",
      "Epoch 62/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -2987.8360 - g_loss: 2946.8903\n",
      "Epoch 63/100\n",
      "762/762 [==============================] - 141s 185ms/step - d_loss: -3036.5822 - g_loss: 2994.8928\n",
      "Epoch 64/100\n",
      "762/762 [==============================] - 140s 184ms/step - d_loss: -3085.3286 - g_loss: 3042.9300\n",
      "Epoch 65/100\n",
      "762/762 [==============================] - 139s 182ms/step - d_loss: -3134.0764 - g_loss: 3090.9855\n",
      "Epoch 66/100\n",
      "762/762 [==============================] - 139s 182ms/step - d_loss: -3182.8224 - g_loss: 3139.0041\n",
      "Epoch 67/100\n",
      "762/762 [==============================] - 139s 182ms/step - d_loss: -3231.5691 - g_loss: 3187.0264\n",
      "Epoch 68/100\n",
      "762/762 [==============================] - 139s 182ms/step - d_loss: -3280.3137 - g_loss: 3235.0574\n",
      "Epoch 69/100\n",
      "762/762 [==============================] - 141s 185ms/step - d_loss: -3329.0627 - g_loss: 3283.1155\n",
      "Epoch 70/100\n",
      "762/762 [==============================] - 137s 180ms/step - d_loss: -3377.8088 - g_loss: 3331.1006\n",
      "Epoch 71/100\n",
      "762/762 [==============================] - 137s 179ms/step - d_loss: -3426.5540 - g_loss: 3379.1016\n",
      "Epoch 72/100\n",
      "762/762 [==============================] - 137s 180ms/step - d_loss: -3475.3009 - g_loss: 3427.0823\n",
      "Epoch 73/100\n",
      "762/762 [==============================] - 139s 182ms/step - d_loss: -3524.0467 - g_loss: 3475.0831\n",
      "Epoch 74/100\n",
      "762/762 [==============================] - 138s 182ms/step - d_loss: -3572.7942 - g_loss: 3523.1194\n",
      "Epoch 75/100\n",
      "762/762 [==============================] - 138s 182ms/step - d_loss: -3621.5413 - g_loss: 3571.1440\n",
      "Epoch 76/100\n",
      "762/762 [==============================] - 138s 182ms/step - d_loss: -3670.2880 - g_loss: 3619.2200\n",
      "Epoch 77/100\n",
      "762/762 [==============================] - 138s 182ms/step - d_loss: -3719.0337 - g_loss: 3667.2537\n",
      "Epoch 78/100\n",
      "762/762 [==============================] - 138s 180ms/step - d_loss: -3767.7800 - g_loss: 3715.2735\n",
      "Epoch 79/100\n",
      "762/762 [==============================] - 139s 182ms/step - d_loss: -3816.5278 - g_loss: 3763.2853\n",
      "Epoch 80/100\n",
      "762/762 [==============================] - 139s 183ms/step - d_loss: -3865.2734 - g_loss: 3811.3344\n",
      "Epoch 81/100\n",
      "762/762 [==============================] - 139s 183ms/step - d_loss: -3914.0204 - g_loss: 3859.4401\n",
      "Epoch 82/100\n",
      "762/762 [==============================] - 139s 182ms/step - d_loss: -3962.7662 - g_loss: 3907.4465\n",
      "Epoch 83/100\n",
      "762/762 [==============================] - 138s 182ms/step - d_loss: -4011.5138 - g_loss: 3955.5146\n",
      "Epoch 84/100\n",
      "762/762 [==============================] - 137s 180ms/step - d_loss: -4060.2614 - g_loss: 4003.5685\n",
      "Epoch 85/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -4109.0088 - g_loss: 4051.5962\n",
      "Epoch 86/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -4157.7552 - g_loss: 4099.6193\n",
      "Epoch 87/100\n",
      "762/762 [==============================] - 139s 182ms/step - d_loss: -4206.5030 - g_loss: 4147.6582\n",
      "Epoch 88/100\n",
      "762/762 [==============================] - 137s 180ms/step - d_loss: -4255.2498 - g_loss: 4195.6609\n",
      "Epoch 89/100\n",
      "762/762 [==============================] - 138s 182ms/step - d_loss: -4303.9971 - g_loss: 4243.7403\n",
      "Epoch 90/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -4352.7436 - g_loss: 4291.7335\n",
      "Epoch 91/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -4401.4906 - g_loss: 4339.7921\n",
      "Epoch 92/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -4450.2379 - g_loss: 4387.8314\n",
      "Epoch 93/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -4498.9838 - g_loss: 4435.8799\n",
      "Epoch 94/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -4547.7306 - g_loss: 4483.8638\n",
      "Epoch 95/100\n",
      "762/762 [==============================] - 139s 182ms/step - d_loss: -4596.4780 - g_loss: 4531.9118\n",
      "Epoch 96/100\n",
      "762/762 [==============================] - 138s 182ms/step - d_loss: -4645.2251 - g_loss: 4579.9809\n",
      "Epoch 97/100\n",
      "762/762 [==============================] - 138s 182ms/step - d_loss: -4693.9714 - g_loss: 4628.0039\n",
      "Epoch 98/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -4742.7194 - g_loss: 4676.0311\n",
      "Epoch 99/100\n",
      "762/762 [==============================] - 137s 180ms/step - d_loss: -4791.4664 - g_loss: 4724.0842\n",
      "Epoch 100/100\n",
      "762/762 [==============================] - 138s 181ms/step - d_loss: -4840.2138 - g_loss: 4772.1328\n",
      "(0.3474, 0.471545366534129, 0.3893744187767699) \n",
      " (0.3736, 0.5191358039517215, 0.41561262165283863)\n"
     ]
    }
   ],
   "source": [
    "# Fit \n",
    "epochs = 100\n",
    "# Instantiate the WGAN model.\n",
    "wgan = WGAN(\n",
    "    discriminator=discriminator,\n",
    "    generator=generator,\n",
    "    discriminator_extra_steps=5\n",
    ")\n",
    "\n",
    "# Compile the WGAN model.\n",
    "wgan.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    g_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    g_loss_fn=generator_loss,\n",
    "    d_loss_fn=discriminator_loss,\n",
    "    c_loss_fn = counter_loss,\n",
    "    run_eagerly=False)\n",
    "\n",
    "# Start training the model.\n",
    "fit = wgan.fit(train, batch_size=100, epochs=epochs, verbose=True)\n",
    "print(wgan.test_step(10), \"\\n\", wgan.test_step(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "037cae76-feb9-44ff-8639-2bc1d2d5e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.Series(fit.history['d_loss'])\n",
    "g = pd.Series(fit.history['g_loss'])\n",
    "losses = pd.DataFrame(pd.concat([d,g], axis=1))\n",
    "losses.rename(columns={0:'d_loss', 1:'g_loss'})\n",
    "\n",
    "losses.to_csv(\"losses/wgangp_100epoch_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff1336d-0e41-4dec-91e7-283dc4c9b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with baseline metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76357ee6-17f6-40c6-8c91-56f6ccfa609d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50,), dtype=float32, numpy=\n",
       "array([-0.01528735, -0.11605564, -0.0677366 ,  0.07035215,  0.06171633,\n",
       "       -0.10844038, -0.13454585, -0.25683078,  0.07253584, -0.13466115,\n",
       "       -0.12068694, -0.01514214, -0.15511733,  0.01069139,  0.0382417 ,\n",
       "       -0.1363697 ,  0.21189529,  0.09631862, -0.05128156, -0.01761855,\n",
       "        0.07768512, -0.07682421,  0.04833014, -0.07011563, -0.04009328,\n",
       "       -0.0637093 , -0.03538469, -0.10249022, -0.09990829,  0.04847763,\n",
       "        0.09975258, -0.11815249,  0.07830128, -0.06967121, -0.00733658,\n",
       "       -0.05188571, -0.12428847, -0.13059792, -0.0737626 , -0.0377024 ,\n",
       "       -0.1015723 ,  0.05233473, -0.00874771,  0.0237248 ,  0.15101252,\n",
       "        0.02378168,  0.12840454,  0.01683222, -0.19599509, -0.04296796],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_brand_embs(0)\n",
    "G_category_embs(0)\n",
    "G_colour_embs(0)\n",
    "G_div_embs(0)\n",
    "G_itemcat_embs(0)\n",
    "G_itemfam_embs(0)\n",
    "G_season_embs(0)\n",
    "G_prod_embs(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac454250-381b-48db-b659-acc8b705d54f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 310, 50) (1, 8, 50) (1, 53, 50) (1, 3, 50) (1, 13, 50) (1, 4, 50) (1, 3, 50) (1, 78, 50)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(G_brand_embs.get_weights()).shape,\n",
    "np.array(G_category_embs.get_weights()).shape,\n",
    "np.array(G_colour_embs.get_weights()).shape,\n",
    "np.array(G_div_embs.get_weights()).shape,\n",
    "np.array(G_itemcat_embs.get_weights()).shape,\n",
    "np.array(G_itemfam_embs.get_weights()).shape,\n",
    "np.array(G_season_embs.get_weights()).shape,\n",
    "np.array(G_prod_embs.get_weights()).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bd71885-6c5c-415d-97b6-d3f8efdf969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"embeddings/Product_embs\",np.array(G_prod_embs.weights[0]))\n",
    "np.save(\"embeddings/brand_embs\",np.array(G_brand_embs.weights[0]))\n",
    "np.save(\"embeddings/category_embs\",np.array(G_category_embs.weights[0]))\n",
    "np.save(\"embeddings/colour_embs\",np.array(G_colour_embs.weights[0]))\n",
    "np.save(\"embeddings/div_embs\",np.array(G_div_embs.weights[0]))\n",
    "np.save(\"embeddings/itemcat_embs\",np.array(G_itemcat_embs.weights[0]))\n",
    "np.save(\"embeddings/itemfam_embs\",np.array(G_itemfam_embs.weights[0]))\n",
    "np.save(\"embeddings/season_embs\",np.array(G_season_embs.weights[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
