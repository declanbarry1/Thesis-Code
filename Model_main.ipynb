{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7b6868-f48f-4c85-8765-5790439d6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import support\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from support import *\n",
    "from evall import *\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "74c45219-153d-4a90-8e46-1ea00223f403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "brand_num = 254 \n",
    "class_num =  178\n",
    "user_emb_dim = brand_num + class_num\n",
    "\n",
    "D_brand_emb_dim = 128\n",
    "D_class_emb_dim = 128\n",
    "\n",
    "G_brand_emb_dim = 128\n",
    "G_class_emb_dim = 128\n",
    "\n",
    "hidden_dim = 128\n",
    "alpha = 0\n",
    "\n",
    "# Initializer\n",
    "init = tf.initializers.glorot_normal()\n",
    "\n",
    "'''Generator and Discriminator Attribute Embeddings'''\n",
    "D_brand_embs = tf.keras.layers.Embedding(input_dim = brand_num, output_dim = D_brand_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=(brand_num,D_brand_emb_dim))])\n",
    "D_class_embs = tf.keras.layers.Embedding(input_dim = class_num, output_dim = D_class_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=(class_num,D_class_emb_dim))])\n",
    "\n",
    "G_brand_embs = tf.keras.layers.Embedding(input_dim = brand_num, output_dim = G_brand_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=(brand_num,G_brand_emb_dim))])\n",
    "G_class_embs = tf.keras.layers.Embedding(input_dim = class_num, output_dim = G_class_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=(class_num,G_class_emb_dim))])\n",
    "# Model input sizes\n",
    "G_input_size =  G_brand_emb_dim + G_class_emb_dim\n",
    "D_input_size = user_emb_dim + D_brand_emb_dim + D_class_emb_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1938b12d-1f80-457e-abc0-1157854db5a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generator_input(brand_id, class_id):\n",
    "    brand_emb = G_brand_embs(tf.constant(brand_id))\n",
    "    class_emb = G_class_embs(tf.constant(class_id))\n",
    "    brand_class_emb = tf.keras.layers.concatenate([brand_emb, class_emb], 1)\n",
    "    return brand_class_emb\n",
    "\n",
    "# Generates user based on concatenation of all attributes\n",
    "def generator():\n",
    "    bc_input = tf.keras.layers.Input(shape=(G_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='relu', kernel_regularizer = 'l2')(bc_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='relu', kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(user_emb_dim, activation ='sigmoid', kernel_regularizer = 'l2')(x)\n",
    "    g_model = tf.keras.models.Model(bc_input, x, name = 'generator')\n",
    "    return g_model\n",
    "g_model = generator()\n",
    "\n",
    "# Dictionary of attribute embeddings for attribute generators\n",
    "att_dict = {\"brand\":G_brand_embs, \"class\":G_class_embs}\n",
    "# Generates user based on one attribute\n",
    "def att_gen(att_id, att):\n",
    "    att = att_dict[att]\n",
    "    att_emb = tf.reshape(G_brand_embs(att_id), shape=(1,G_brand_emb_dim))\n",
    "    att_input = tf.keras.layers.Input(shape=(128))\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', activity_regularizer = 'l2')(att_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', activity_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(user_emb_dim, activation ='sigmoid', activity_regularizer = 'l2')(x)\n",
    "    model = tf.keras.models.Model(att_input, x, name = 'generator')\n",
    "    return model\n",
    "\n",
    "def discriminator_input(brand_id, class_id, user_emb):\n",
    "    brand_emb = G_brand_embs(tf.constant(brand_id))\n",
    "    class_emb = G_class_embs(tf.constant(class_id))\n",
    "    user_emb = tf.cast(user_emb, dtype=float)\n",
    "    d_input = tf.keras.layers.concatenate([brand_emb, class_emb, user_emb], 1)\n",
    "    return d_input\n",
    "\n",
    "def discriminator():\n",
    "    d_input = tf.keras.layers.Input(shape=(D_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', activity_regularizer = 'l2')(d_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', activity_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.models.Model(d_input, x, name = 'discriminator')\n",
    "    return model\n",
    "d_model = discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3a473388-b80c-45bf-873a-8fd53355d5db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dis_5():\n",
    "    d_input = tf.keras.layers.Input(shape=(D_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim_d1, activation =da1, kernel_regularizer = 'l2')(d_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_d2, activation =da2, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_d3, activation =da3, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_d4, activation =da4, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.models.Model(d_input, x, name = 'discriminator')\n",
    "    return model\n",
    "\n",
    "def dis_4():\n",
    "    d_input = tf.keras.layers.Input(shape=(D_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim_d1, activation =da1, kernel_regularizer = 'l2')(d_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_d2, activation =da2, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_d3, activation =da3, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.models.Model(d_input, x, name = 'discriminator')\n",
    "    return model\n",
    "def dis_3():\n",
    "    d_input = tf.keras.layers.Input(shape=(D_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim_d1, activation =da1, kernel_regularizer = 'l2')(d_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_d2, activation =da2, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.models.Model(d_input, x, name = 'discriminator')\n",
    "    return model\n",
    "\n",
    "def gen_5():\n",
    "    bc_input = tf.keras.layers.Input(shape=(G_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim_g1, activation =ga1, kernel_regularizer = 'l2')(bc_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_g2, activation =ga2, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_g3, activation =ga3, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_g4, activation =ga4, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(user_emb_dim, activation ='sigmoid', kernel_regularizer = 'l2')(x)\n",
    "    g_model = tf.keras.models.Model(bc_input, x, name = 'generator')\n",
    "    return g_model\n",
    "\n",
    "def gen_4():\n",
    "    bc_input = tf.keras.layers.Input(shape=(G_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim_g1, activation =ga1, kernel_regularizer = 'l2')(bc_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_g2, activation =ga2, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_g3, activation =ga3, kernel_regularizer = 'l2')(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(user_emb_dim, activation ='sigmoid', kernel_regularizer = 'l2')(x)\n",
    "    g_model = tf.keras.models.Model(bc_input, x, name = 'generator')\n",
    "    return g_model\n",
    "def gen_3():\n",
    "    bc_input = tf.keras.layers.Input(shape=(G_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim_g1, activation =ga1, kernel_regularizer = 'l2')(bc_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_g2, activation =ga2, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(user_emb_dim, activation ='sigmoid', kernel_regularizer = 'l2')(x)\n",
    "    g_model = tf.keras.models.Model(bc_input, x, name = 'generator')\n",
    "    return g_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ee89577c-85e1-4396-99cc-e100e1d3d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loss functions'''\n",
    "# Wgan loss\n",
    "def generator_loss(fake_user):\n",
    "    return -tf.reduce_mean(fake_user)\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    logit = tf.reduce_mean(fake-real)\n",
    "    return logit\n",
    "\n",
    "def counter_loss(counter):\n",
    "    return tf.reduce_mean(counter)\n",
    "\n",
    "def discriminator_counter_loss(real, fake, counter):\n",
    "    logit = tf.reduce_mean(fake + counter - real)\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7da9f22a-88f6-4079-a7f6-18abc59ac6ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WGAN Class\n",
    "class WGAN(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        discriminator_extra_steps=3,\n",
    "        batch_size = 577\n",
    "    ):\n",
    "        super(WGAN, self).__init__()\n",
    "        self.discriminator = d_model\n",
    "        self.generator = g_model\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.k = 10\n",
    "        self.index = 0 \n",
    "        self.c_index = 0 \n",
    "        self.gp_weight = 10\n",
    "        self.eval_steps = 0\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn,c_loss_fn, run_eagerly):\n",
    "        super(WGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.c_loss_fn = c_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "        self.run_eagerly = run_eagerly\n",
    "        #self.d_loss_metric = tf.keras.metrics.Precision(name=\"d_loss\")\n",
    "        #self.g_loss_metric = tf.keras.metrics.Precision(name=\"g_loss\")\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_users, fake_users, brand_id, class_id):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated image\n",
    "        alpha = tf.random.normal([batch_size,1], 0.0, 1.0)\n",
    "        diff = fake_users - real_users\n",
    "        interpolated = real_users + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            interpolated_input = discriminator_input(brand_id, class_id, interpolated)\n",
    "            pred = self.discriminator(interpolated_input)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, real_users):\n",
    "        self.eval_steps +=1 \n",
    "        c_batch_size = 2*self.batch_size\n",
    "        for i in range(self.d_steps):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Get batch data\n",
    "                item_id, brand_id, class_id, real_users = support.get_batchdata(self.index, self.index + self.batch_size)\n",
    "                # Get batch of counter examples\n",
    "                counter_brand_id, counter_class_id, counter_users = support.get_counter_batch(self.c_index,\n",
    "                                                                                            self.c_index + c_batch_size)\n",
    "                # Generate fake users from attributes\n",
    "                g_input0 = generator_input(brand_id, class_id)\n",
    "                fake_users = self.generator(g_input0)\n",
    "                # Get the logits for the fake users\n",
    "                d_input0 = discriminator_input(brand_id, class_id, fake_users)\n",
    "                fake_logits = self.discriminator(d_input0)\n",
    "                # Get the logits for the real user\n",
    "                d_input1 = discriminator_input(brand_id, class_id, real_users)\n",
    "                real_logits = self.discriminator(d_input1)\n",
    "                # Get logits for counter examples\n",
    "                d_input2 = discriminator_input(counter_brand_id, counter_class_id, counter_users)\n",
    "                counter_logits = self.discriminator(d_input2)\n",
    "                # Calculate the discriminator loss using the fake and real image logits\n",
    "                d_cost = self.d_loss_fn(real_logits, fake_logits)\n",
    "                c_loss = self.c_loss_fn(counter_logits)\n",
    " \n",
    "                # Get gradient penalty\n",
    "                gp = self.gradient_penalty(self.batch_size, real_users, fake_users, brand_id, class_id)\n",
    "                # Later add counter loss\n",
    "                \n",
    "                d_loss = d_cost + c_loss + gp*self.gp_weight\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(zip(d_gradient, self.discriminator.trainable_variables))\n",
    "\n",
    "        # Train the generator\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Generate fake images using the generator\n",
    "            g_input1 = generator_input(brand_id, class_id)\n",
    "            gen_users = self.generator(g_input1)\n",
    "            # Get the discriminator logits for fake images\n",
    "            d_input2 = discriminator_input(brand_id, class_id, gen_users)\n",
    "            gen_logits = self.discriminator(d_input2)\n",
    "            # Calculate the generator loss\n",
    "            #g_loss = self.g_loss_fn(gen_logits)\n",
    "            g_loss = self.g_loss_fn(gen_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "\n",
    "    def test_step(self, k):\n",
    "        item_id, brand_id, class_id = support.get_testdata()\n",
    "        test_BATCH_SIZE = item_id.size\n",
    "        g_input1 = generator_input(brand_id, class_id)\n",
    "        gen_users = self.generator(g_input1)\n",
    "        sim_users = support.get_intersection_similar_user( gen_users, k )\n",
    "        count = 0\n",
    "        for test_i, test_userlist in zip(item_id, sim_users):       \n",
    "            for test_u in test_userlist:\n",
    "                if ui_matrix[test_u, test_i] == 1:\n",
    "                    count = count + 1            \n",
    "        p_at_10 = round(count/(test_BATCH_SIZE * k), 4)\n",
    "\n",
    "        ans = 0.0\n",
    "        RS = []\n",
    "        for test_i, test_userlist in zip(item_id, sim_users):  \n",
    "            r=[]\n",
    "            for user in test_userlist:\n",
    "                r.append(ui_matrix[user][test_i])\n",
    "            RS.append( r)\n",
    "        M_at_10 = evall.mean_average_precision(RS)\n",
    "\n",
    "\n",
    "        ans = 0.0\n",
    "        for test_i, test_userlist in zip(item_id, sim_users):  \n",
    "            r=[]\n",
    "            for user in test_userlist:\n",
    "                r.append(ui_matrix[user][test_i])\n",
    "            ans = ans + evall.ndcg_at_k(r, k, method=1)\n",
    "        G_at_10 = ans/test_BATCH_SIZE\n",
    "\n",
    "        return p_at_10,G_at_10,M_at_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5c63b14e-1bd0-4217-8f7a-4197cea136cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "344/344 [==============================] - 13s 31ms/step - d_loss: -56.4202 - g_loss: 54.0531\n",
      "Epoch 2/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -169.7051 - g_loss: 169.1330\n",
      "Epoch 3/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -291.2710 - g_loss: 290.3292\n",
      "Epoch 4/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -417.6777 - g_loss: 416.2128\n",
      "Epoch 5/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -547.7923 - g_loss: 546.0351\n",
      "Epoch 6/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -676.5728 - g_loss: 674.6028\n",
      "Epoch 7/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -804.9508 - g_loss: 802.8090\n",
      "Epoch 8/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -932.6269 - g_loss: 930.3454\n",
      "Epoch 9/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -1062.1830 - g_loss: 1059.7935\n",
      "Epoch 10/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -1192.9429 - g_loss: 1190.4267\n",
      "Epoch 11/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -1323.8028 - g_loss: 1321.1808\n",
      "Epoch 12/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -1455.0338 - g_loss: 1452.2972\n",
      "Epoch 13/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -1586.9456 - g_loss: 1584.0953\n",
      "Epoch 14/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -1719.8886 - g_loss: 1716.9306\n",
      "Epoch 15/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -1852.3095 - g_loss: 1849.2035\n",
      "Epoch 16/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -1984.5252 - g_loss: 1981.2326\n",
      "Epoch 17/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -2116.6015 - g_loss: 2113.1021\n",
      "Epoch 18/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -2248.6731 - g_loss: 2244.9632\n",
      "Epoch 19/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -2381.0376 - g_loss: 2377.1330\n",
      "Epoch 20/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -2514.8462 - g_loss: 2510.7377\n",
      "Epoch 21/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -2648.1217 - g_loss: 2643.8207\n",
      "Epoch 22/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -2781.2225 - g_loss: 2776.7261\n",
      "Epoch 23/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -2914.3056 - g_loss: 2909.5934\n",
      "Epoch 24/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -3047.3891 - g_loss: 3042.4682\n",
      "Epoch 25/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -3180.4501 - g_loss: 3175.2951\n",
      "Epoch 26/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -3313.5097 - g_loss: 3308.0910\n",
      "Epoch 27/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -3446.5846 - g_loss: 3440.9574\n",
      "Epoch 28/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -3579.6567 - g_loss: 3573.8257\n",
      "Epoch 29/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -3712.7503 - g_loss: 3706.6892\n",
      "Epoch 30/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -3845.7955 - g_loss: 3839.5275\n",
      "Epoch 31/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -3978.8698 - g_loss: 3972.3658\n",
      "Epoch 32/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -4111.9666 - g_loss: 4105.2593\n",
      "Epoch 33/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -4245.0190 - g_loss: 4238.0918\n",
      "Epoch 34/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -4378.1042 - g_loss: 4370.9627\n",
      "Epoch 35/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -4511.1260 - g_loss: 4503.7621\n",
      "Epoch 36/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -4644.2181 - g_loss: 4636.6303\n",
      "Epoch 37/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -4777.2975 - g_loss: 4769.4976\n",
      "Epoch 38/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -4910.3605 - g_loss: 4902.3594\n",
      "Epoch 39/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -5043.4195 - g_loss: 5035.2396\n",
      "Epoch 40/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -5176.4701 - g_loss: 5168.0717\n",
      "Epoch 41/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -5309.5153 - g_loss: 5300.9130\n",
      "Epoch 42/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -5442.5871 - g_loss: 5433.7589\n",
      "Epoch 43/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -5575.6481 - g_loss: 5566.5893\n",
      "Epoch 44/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -5708.7086 - g_loss: 5699.4676\n",
      "Epoch 45/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -5841.7732 - g_loss: 5832.3118\n",
      "Epoch 46/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -5974.8269 - g_loss: 5965.1934\n",
      "Epoch 47/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -6107.8756 - g_loss: 6098.0088\n",
      "Epoch 48/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -6240.9170 - g_loss: 6230.7823\n",
      "Epoch 49/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -6374.0050 - g_loss: 6363.5615\n",
      "Epoch 50/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -6507.1200 - g_loss: 6496.4696\n",
      "Epoch 51/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -6640.1576 - g_loss: 6629.3633\n",
      "Epoch 52/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -6773.2969 - g_loss: 6762.1436\n",
      "Epoch 53/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -6906.2889 - g_loss: 6895.0534\n",
      "Epoch 54/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -7039.4088 - g_loss: 7027.9395\n",
      "Epoch 55/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -7172.4852 - g_loss: 7160.7749\n",
      "Epoch 56/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -7305.5396 - g_loss: 7293.6478\n",
      "Epoch 57/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -7438.5876 - g_loss: 7426.4909\n",
      "Epoch 58/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -7571.5954 - g_loss: 7559.3632\n",
      "Epoch 59/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -7704.6696 - g_loss: 7692.2309\n",
      "Epoch 60/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -7837.7032 - g_loss: 7825.1023\n",
      "Epoch 61/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -7970.8059 - g_loss: 7957.9627\n",
      "Epoch 62/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -8103.8424 - g_loss: 8090.7568\n",
      "Epoch 63/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -8236.9589 - g_loss: 8223.6408\n",
      "Epoch 64/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -8370.0123 - g_loss: 8356.5115\n",
      "Epoch 65/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -8503.0466 - g_loss: 8489.3788\n",
      "Epoch 66/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -8636.0901 - g_loss: 8622.2391\n",
      "Epoch 67/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -8769.1253 - g_loss: 8755.1010\n",
      "Epoch 68/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -8902.1902 - g_loss: 8887.9700\n",
      "Epoch 69/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -9035.2586 - g_loss: 9020.8293\n",
      "Epoch 70/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -9168.2750 - g_loss: 9153.6802\n",
      "Epoch 71/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -9301.3541 - g_loss: 9286.5328\n",
      "Epoch 72/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -9434.4525 - g_loss: 9419.3892\n",
      "Epoch 73/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -9567.5301 - g_loss: 9552.2610\n",
      "Epoch 74/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -9700.7922 - g_loss: 9685.1365\n",
      "Epoch 75/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -9833.9333 - g_loss: 9817.9678\n",
      "Epoch 76/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -9966.9269 - g_loss: 9950.7419\n",
      "Epoch 77/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -10099.6330 - g_loss: 10083.5440\n",
      "Epoch 78/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -10232.6720 - g_loss: 10216.4737\n",
      "Epoch 79/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -10365.6230 - g_loss: 10349.2641\n",
      "Epoch 80/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -10498.7295 - g_loss: 10482.1151\n",
      "Epoch 81/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -10631.7965 - g_loss: 10614.9777\n",
      "Epoch 82/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -10764.7474 - g_loss: 10747.8675\n",
      "Epoch 83/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -10897.7647 - g_loss: 10880.7410\n",
      "Epoch 84/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -11030.9115 - g_loss: 11013.5731\n",
      "Epoch 85/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -11163.9898 - g_loss: 11146.4087\n",
      "Epoch 86/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -11297.0475 - g_loss: 11279.2562\n",
      "Epoch 87/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -11430.1009 - g_loss: 11412.1397\n",
      "Epoch 88/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -11563.2128 - g_loss: 11544.9896\n",
      "Epoch 89/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -11696.2727 - g_loss: 11677.8533\n",
      "Epoch 90/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -11829.2889 - g_loss: 11810.6881\n",
      "Epoch 91/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -11962.3306 - g_loss: 11943.5419\n",
      "Epoch 92/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -12095.4126 - g_loss: 12076.3710\n",
      "Epoch 93/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -12228.0488 - g_loss: 12209.2021\n",
      "Epoch 94/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -12361.1108 - g_loss: 12341.9753\n",
      "Epoch 95/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -12495.0152 - g_loss: 12474.6960\n",
      "Epoch 96/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -12627.9201 - g_loss: 12607.5041\n",
      "Epoch 97/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -12760.9583 - g_loss: 12740.4074\n",
      "Epoch 98/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -12893.7729 - g_loss: 12873.1475\n",
      "Epoch 99/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -13026.8579 - g_loss: 13006.1001\n",
      "Epoch 100/100\n",
      "344/344 [==============================] - 11s 31ms/step - d_loss: -13159.8869 - g_loss: 13139.0557\n",
      "(0.006, 0.02439491016735797, 0.014974160206718344) \n",
      " (0.0065, 0.03769189367006485, 0.01820793641443076)\n"
     ]
    }
   ],
   "source": [
    "# Fit \n",
    "epochs = 100\n",
    "\n",
    "# Instantiate the WGAN model.\n",
    "wgan = WGAN(\n",
    "    discriminator=discriminator,\n",
    "    generator=generator,\n",
    "    discriminator_extra_steps=3\n",
    ")\n",
    "\n",
    "# Compile the WGAN model.\n",
    "wgan.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    g_optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    g_loss_fn=generator_loss,\n",
    "    d_loss_fn=discriminator_loss,\n",
    "    c_loss_fn = counter_loss,\n",
    "    run_eagerly=False\n",
    ")\n",
    "\n",
    "# Start training the model.\n",
    "fit = wgan.fit(train, batch_size=577, epochs=epochs, verbose=True)\n",
    "print(wgan.test_step(10), \"\\n\", wgan.test_step(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35608724-5a21-4f38-8317-e9e09e3e99b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    # Select hyperparameters randomly\n",
    "\n",
    "    # Discriminator\n",
    "   \n",
    "    dlayers = random.randint(0,2)\n",
    "    if dlayers ==0:\n",
    "        discriminator = dis_3\n",
    "    elif dlayers ==1:\n",
    "        discriminator = dis_4\n",
    "    elif dlayers ==2:\n",
    "        discriminator = dis_5\n",
    "    glayers = random.randint(0,2)\n",
    "    if glayers ==0:\n",
    "        generator = gen_3\n",
    "    elif glayers ==1:\n",
    "        generator = gen_4\n",
    "    elif glayers ==2:\n",
    "        generator = gen_5\n",
    "    hidden_dim_d1 = np.random.choice([100,200,300,400])\n",
    "    hidden_dim_d2 = np.random.choice([100,200,300,400])\n",
    "    hidden_dim_d3 = np.random.choice([100,200,300,400])\n",
    "    hidden_dim_d4 = np.random.choice([100,200,300,400])\n",
    "    da1 = np.random.choice(['sigmoid', 'tanh','relu'])\n",
    "    da2 = np.random.choice(['sigmoid', 'tanh','relu'])\n",
    "    da3 = np.random.choice(['sigmoid', 'tanh','relu'])\n",
    "    da4 = np.random.choice(['sigmoid', 'tanh','relu'])\n",
    "    hidden_dim_g1 = np.random.choice([100,200,300,400])\n",
    "    hidden_dim_g2 = np.random.choice([100,200,300,400])\n",
    "    hidden_dim_g3 = np.random.choice([100,200,300,400])\n",
    "    hidden_dim_g4 = np.random.choice([100,200,300,400])\n",
    "    ga1 = np.random.choice(['sigmoid', 'tanh','relu'])\n",
    "    ga2 = np.random.choice(['sigmoid', 'tanh','relu'])\n",
    "    ga3 = np.random.choice(['sigmoid', 'tanh','relu'])\n",
    "    ga4 = np.random.choice(['sigmoid', 'tanh','relu'])\n",
    "\n",
    "    # Fit \n",
    "    epochs = 50\n",
    "\n",
    "    # Instantiate the WGAN model.\n",
    "    wgan = WGAN(\n",
    "        discriminator=discriminator,\n",
    "        generator=generator,\n",
    "        discriminator_extra_steps=3\n",
    "    )\n",
    "\n",
    "    # Compile the WGAN model.\n",
    "    wgan.compile(\n",
    "        d_optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        g_optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        g_loss_fn=generator_loss,\n",
    "        d_loss_fn=discriminator_loss,\n",
    "        c_loss_fn = counter_loss,\n",
    "        run_eagerly=False\n",
    "    )\n",
    "\n",
    "    # Start training the model.\n",
    "    fit = wgan.fit(train, batch_size=577, epochs=epochs, verbose=False)\n",
    "    print(\"discriminator layers = \", dlayers+3,\"generator layers = \", glayers+3,\"ga1 = \",ga1,\"ga2 = \",ga2,\"ga3 = \",ga3,\"ga4 = \",ga4,\n",
    "          \"da1 = \", da1,\"da2 = \",da2,\"da3 = \",da3,\"da4 = \",da4, \"hidden dims discriminator:   \",hidden_dim_d1, hidden_dim_d2, hidden_dim_d3, hidden_dim_d4,\n",
    "          \"hidden dims generator:   \",hidden_dim_g1, hidden_dim_g2, hidden_dim_g3, hidden_dim_g4,\n",
    "          wgan.test_step(10), \"\\n\", wgan.test_step(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
