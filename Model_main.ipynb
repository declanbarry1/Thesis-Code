{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e7b6868-f48f-4c85-8765-5790439d6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import support\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from support import *\n",
    "from evall import *\n",
    "import random\n",
    "\n",
    "train = np.load(\"np_train_data.npy\")\n",
    "ucb_matrix = np.load(\"ucb_matrix_test.npy\")\n",
    "ui_matrix = np.load(\"ui_matrix.npy\")\n",
    "test = np.load(\"np_test_data.npy\")\n",
    "test_data = test_data[:, 1:4]\n",
    "test_data = np.unique(test_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "74c45219-153d-4a90-8e46-1ea00223f403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "brand_num = 254 \n",
    "class_num =  178\n",
    "user_emb_dim = brand_num + class_num\n",
    "\n",
    "D_brand_emb_dim = 128\n",
    "D_class_emb_dim = 128\n",
    "\n",
    "G_brand_emb_dim = 128\n",
    "G_class_emb_dim = 128\n",
    "\n",
    "hidden_dim = 128\n",
    "alpha = 0\n",
    "\n",
    "# Initializer\n",
    "init = tf.initializers.glorot_normal()\n",
    "\n",
    "'''Generator and Discriminator Attribute Embeddings'''\n",
    "D_brand_embs = tf.keras.layers.Embedding(input_dim = brand_num, output_dim = D_brand_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=(brand_num,D_brand_emb_dim))])\n",
    "D_class_embs = tf.keras.layers.Embedding(input_dim = class_num, output_dim = D_class_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=(class_num,D_class_emb_dim))])\n",
    "\n",
    "G_brand_embs = tf.keras.layers.Embedding(input_dim = brand_num, output_dim = G_brand_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=(brand_num,G_brand_emb_dim))])\n",
    "G_class_embs = tf.keras.layers.Embedding(input_dim = class_num, output_dim = G_class_emb_dim,\n",
    "                                          trainable=True, weights = [init(shape=(class_num,G_class_emb_dim))])\n",
    "# Model input sizes\n",
    "G_input_size =  G_brand_emb_dim + G_class_emb_dim\n",
    "D_input_size = user_emb_dim + D_brand_emb_dim + D_class_emb_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1938b12d-1f80-457e-abc0-1157854db5a9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generator_input(brand_id, class_id):\n",
    "    brand_emb = G_brand_embs(tf.constant(brand_id))\n",
    "    class_emb = G_class_embs(tf.constant(class_id))\n",
    "    brand_class_emb = tf.keras.layers.concatenate([brand_emb, class_emb], 1)\n",
    "    return brand_class_emb\n",
    "\n",
    "# Generates user based on concatenation of all attributes\n",
    "def generator():\n",
    "    bc_input = tf.keras.layers.Input(shape=(G_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='relu', kernel_regularizer = 'l2')(bc_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='relu', kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(user_emb_dim, activation ='sigmoid', kernel_regularizer = 'l2')(x)\n",
    "    g_model = tf.keras.models.Model(bc_input, x, name = 'generator')\n",
    "    return g_model\n",
    "g_model = generator()\n",
    "\n",
    "# Dictionary of attribute embeddings for attribute generators\n",
    "att_dict = {\"brand\":G_brand_embs, \"class\":G_class_embs}\n",
    "# Generates user based on one attribute\n",
    "def att_gen(att_id, att):\n",
    "    att = att_dict[att]\n",
    "    att_emb = tf.reshape(G_brand_embs(att_id), shape=(1,G_brand_emb_dim))\n",
    "    att_input = tf.keras.layers.Input(shape=(128))\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', kernel_regularizer = 'l2')(att_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(user_emb_dim, activation ='sigmoid', kernel_regularizer = 'l2')(x)\n",
    "    model = tf.keras.models.Model(att_input, x, name = 'generator')\n",
    "    return model\n",
    "\n",
    "def discriminator_input(brand_id, class_id, user_emb):\n",
    "    brand_emb = G_brand_embs(tf.constant(brand_id))\n",
    "    class_emb = G_class_embs(tf.constant(class_id))\n",
    "    user_emb = tf.cast(user_emb, dtype=float)\n",
    "    d_input = tf.keras.layers.concatenate([brand_emb, class_emb, user_emb], 1)\n",
    "    return d_input\n",
    "\n",
    "def discriminator():\n",
    "    d_input = tf.keras.layers.Input(shape=(D_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', kernel_regularizer = 'l2')(d_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation ='sigmoid', kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.models.Model(d_input, x, name = 'discriminator')\n",
    "    return model\n",
    "d_model = discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a473388-b80c-45bf-873a-8fd53355d5db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dis_5():\n",
    "    d_input = tf.keras.layers.Input(shape=(D_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim_d1, activation =da1, kernel_regularizer = 'l2')(d_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_d2, activation =da2, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_d3, activation =da3, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_d4, activation =da4, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.models.Model(d_input, x, name = 'discriminator')\n",
    "    return model\n",
    "\n",
    "def dis_4():\n",
    "    d_input = tf.keras.layers.Input(shape=(D_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim_d1, activation =da1, kernel_regularizer = 'l2')(d_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_d2, activation =da2, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_d3, activation =da3, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.models.Model(d_input, x, name = 'discriminator')\n",
    "    return model\n",
    "def dis_3():\n",
    "    d_input = tf.keras.layers.Input(shape=(D_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim_d1, activation =da1, kernel_regularizer = 'l2')(d_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_d2, activation =da2, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.models.Model(d_input, x, name = 'discriminator')\n",
    "    return model\n",
    "\n",
    "def gen_5():\n",
    "    bc_input = tf.keras.layers.Input(shape=(G_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim_g1, activation =ga1, kernel_regularizer = 'l2')(bc_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_g2, activation =ga2, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_g3, activation =ga3, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_g4, activation =ga4, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(user_emb_dim, activation ='sigmoid', kernel_regularizer = 'l2')(x)\n",
    "    g_model = tf.keras.models.Model(bc_input, x, name = 'generator')\n",
    "    return g_model\n",
    "\n",
    "def gen_4():\n",
    "    bc_input = tf.keras.layers.Input(shape=(G_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim_g1, activation =ga1, kernel_regularizer = 'l2')(bc_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_g2, activation =ga2, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_g3, activation =ga3, kernel_regularizer = 'l2')(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(user_emb_dim, activation ='sigmoid', kernel_regularizer = 'l2')(x)\n",
    "    g_model = tf.keras.models.Model(bc_input, x, name = 'generator')\n",
    "    return g_model\n",
    "def gen_3():\n",
    "    bc_input = tf.keras.layers.Input(shape=(G_input_size))\n",
    "    x = tf.keras.layers.Dense(hidden_dim_g1, activation =ga1, kernel_regularizer = 'l2')(bc_input)\n",
    "    x = tf.keras.layers.Dense(hidden_dim_g2, activation =ga2, kernel_regularizer = 'l2')(x)\n",
    "    x = tf.keras.layers.Dense(user_emb_dim, activation ='sigmoid', kernel_regularizer = 'l2')(x)\n",
    "    g_model = tf.keras.models.Model(bc_input, x, name = 'generator')\n",
    "    return g_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ee89577c-85e1-4396-99cc-e100e1d3d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loss functions'''\n",
    "'''def generator_loss(fake_user):\n",
    "    return -tf.reduce_mean(fake_user)\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    logit = tf.reduce_mean(fake-real)\n",
    "    return logit\n",
    "\n",
    "def counter_loss(counter):\n",
    "    return tf.reduce_mean(counter)\n",
    "\n",
    "def discriminator_counter_loss(real, fake, counter):\n",
    "    logit = tf.reduce_mean(fake + counter - real)\n",
    "    return logit'''\n",
    "# Cross entropy loss means\n",
    "def generator_loss(d_logits):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits, labels=tf.ones_like(d_logits)))\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    logit = tf.reduce_mean(fake-real)\n",
    "    r = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real, labels=tf.ones_like(real)))\n",
    "    f = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake, labels=tf.zeros_like(fake)))\n",
    "    return r+f\n",
    "\n",
    "def counter_loss(counter):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=counter, labels=tf.zeros_like(counter))) \n",
    "\n",
    "'''optimizer'''\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "#generator_optimizer = tf.keras.optimizers.RMSprop(learning_rate = 0.0005)\n",
    "#discriminator_optimizer = tf.keras.optimizers.RMSprop(learning_rate = 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7da9f22a-88f6-4079-a7f6-18abc59ac6ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WGAN Class\n",
    "class WGAN(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        discriminator_extra_steps=5,\n",
    "        batch_size = 577\n",
    "    ):\n",
    "        super(WGAN, self).__init__()\n",
    "        self.discriminator = d_model\n",
    "        self.generator = g_model\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.k = 10\n",
    "        self.ucb_matrix = ucb_matrix\n",
    "        self.ui_matrix = ui_matrix\n",
    "        self.sim = support.get_intersection_similar_user\n",
    "        self.index = 0 \n",
    "        self.c_index = 0 \n",
    "        self.gp_weight = 10\n",
    "        self.eval_steps = 0\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn,c_loss_fn, run_eagerly):\n",
    "        super(WGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.c_loss_fn = c_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "        self.run_eagerly = run_eagerly\n",
    "        #self.d_loss_metric = tf.keras.metrics.Precision(name=\"d_loss\")\n",
    "        #self.g_loss_metric = tf.keras.metrics.Precision(name=\"g_loss\")\n",
    "    def precision_at_k(self, generated_users, item_id, k):\n",
    "        sim_users = self.sim(generated_users, k)\n",
    "        count = 0\n",
    "        for i, userlist in zip(item_id, sim_users):       \n",
    "            for u in userlist:\n",
    "                if ui_matrix[u, i] == 1:\n",
    "                    count = count + 1            \n",
    "        p_k = round(count/(self.batch_size * 10), 4)\n",
    "        return p_k\n",
    "\n",
    "    '''@property\n",
    "    def metrics(self):\n",
    "        return [precision_at_k] '''\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_users, fake_users, brand_id, class_id):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated image\n",
    "        alpha = tf.random.normal([batch_size,1], 0.0, 1.0)\n",
    "        diff = fake_users - real_users\n",
    "        interpolated = real_users + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            interpolated_input = discriminator_input(brand_id, class_id, interpolated)\n",
    "            pred = self.discriminator(interpolated_input)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, real_users):\n",
    "        self.eval_steps +=1 \n",
    "        c_batch_size = 2*self.batch_size\n",
    "        for i in range(self.d_steps):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Get batch data\n",
    "                item_id, brand_id, class_id, real_users = support.get_batchdata(self.index, self.index + self.batch_size)\n",
    "                # Get batch of counter examples\n",
    "                counter_brand_id, counter_class_id, counter_users = support.get_counter_batch(self.c_index,\n",
    "                                                                                            self.c_index + c_batch_size)\n",
    "                # Generate fake users from attributes\n",
    "                g_input0 = generator_input(brand_id, class_id)\n",
    "                fake_users = self.generator(g_input0)\n",
    "                # Get the logits for the fake users\n",
    "                d_input0 = discriminator_input(brand_id, class_id, fake_users)\n",
    "                fake_logits = self.discriminator(d_input0)\n",
    "                # Get the logits for the real user\n",
    "                d_input1 = discriminator_input(brand_id, class_id, real_users)\n",
    "                real_logits = self.discriminator(d_input1)\n",
    "                # Get logits for counter examples\n",
    "                d_input2 = discriminator_input(counter_brand_id, counter_class_id, counter_users)\n",
    "                counter_logits = self.discriminator(d_input2)\n",
    "                # Calculate the discriminator loss using the fake and real image logits\n",
    "                d_cost = self.d_loss_fn(real_logits, fake_logits)\n",
    "                c_loss = self.c_loss_fn(counter_logits)\n",
    "                # Get gradient penalty\n",
    "               # gp = self.gradient_penalty(self.batch_size, real_users, fake_users, brand_id, class_id)\n",
    "                # Later add counter loss\n",
    "                d_loss = d_cost +c_loss #+ gp*self.gp_weight\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(zip(d_gradient, self.discriminator.trainable_variables))\n",
    "\n",
    "        # Train the generator\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Generate fake images using the generator\n",
    "            g_input1 = generator_input(brand_id, class_id)\n",
    "            gen_users = self.generator(g_input1)\n",
    "            # Get the discriminator logits for fake images\n",
    "            d_input2 = discriminator_input(brand_id, class_id, gen_users)\n",
    "            gen_logits = self.discriminator(d_input2)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        # Training precision at k\n",
    "        '''if self.eval_steps%100 == 0:\n",
    "            p_k = self.precision_at_k(gen_users, item_id, self.k)\n",
    "            return {\"d_loss\": d_loss, \"g_loss\": g_loss, \"p_k\":p_k}\n",
    "        else:'''\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "\n",
    "    def test_step(self, k):\n",
    "        item_id, brand_id, class_id = support.get_testdata()\n",
    "        test_BATCH_SIZE = item_id.size\n",
    "        g_input1 = generator_input(brand_id, class_id)\n",
    "        gen_users = self.generator(g_input1)\n",
    "        sim_users = support.get_intersection_similar_user( gen_users, k )\n",
    "        count = 0\n",
    "        for test_i, test_userlist in zip(item_id, sim_users):       \n",
    "            for test_u in test_userlist:\n",
    "                if ui_matrix[test_u, test_i] == 1:\n",
    "                    count = count + 1            \n",
    "        p_at_10 = round(count/(test_BATCH_SIZE * k), 4)\n",
    "\n",
    "        ans = 0.0\n",
    "        RS = []\n",
    "        for test_i, test_userlist in zip(item_id, sim_users):  \n",
    "            r=[]\n",
    "            for user in test_userlist:\n",
    "                r.append(ui_matrix[user][test_i])\n",
    "            RS.append( r)\n",
    "        M_at_10 = evall.mean_average_precision(RS)\n",
    "\n",
    "\n",
    "        ans = 0.0\n",
    "        for test_i, test_userlist in zip(item_id, sim_users):  \n",
    "            r=[]\n",
    "            for user in test_userlist:\n",
    "                r.append(ui_matrix[user][test_i])\n",
    "            ans = ans + evall.ndcg_at_k(r, k, method=1)\n",
    "        G_at_10 = ans/test_BATCH_SIZE\n",
    "\n",
    "        return p_at_10,G_at_10,M_at_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c63b14e-1bd0-4217-8f7a-4197cea136cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "344/344 [==============================] - 14s 35ms/step - d_loss: 0.7274 - g_loss: 4.9410\n",
      "Epoch 2/50\n",
      "344/344 [==============================] - 13s 37ms/step - d_loss: 0.0257 - g_loss: 9.0551\n",
      "Epoch 3/50\n",
      "344/344 [==============================] - 12s 36ms/step - d_loss: 0.0171 - g_loss: 10.6910\n",
      "Epoch 4/50\n",
      "344/344 [==============================] - 12s 36ms/step - d_loss: 0.0025 - g_loss: 13.1635\n",
      "Epoch 5/50\n",
      "344/344 [==============================] - 12s 36ms/step - d_loss: 8.1297e-04 - g_loss: 13.7932\n",
      "Epoch 6/50\n",
      "344/344 [==============================] - 12s 36ms/step - d_loss: 2.4404e-04 - g_loss: 13.7813\n",
      "Epoch 7/50\n",
      "344/344 [==============================] - 12s 35ms/step - d_loss: 3.7325e-04 - g_loss: 14.5578\n",
      "Epoch 8/50\n",
      "344/344 [==============================] - 12s 36ms/step - d_loss: 3.2416e-05 - g_loss: 15.3489\n",
      "Epoch 9/50\n",
      "344/344 [==============================] - 12s 36ms/step - d_loss: 1.3739e-05 - g_loss: 16.4574\n",
      "Epoch 10/50\n",
      "344/344 [==============================] - 12s 36ms/step - d_loss: 7.8342e-06 - g_loss: 17.2975\n",
      "Epoch 11/50\n",
      "283/344 [=======================>......] - ETA: 2s - d_loss: 5.1220e-06 - g_loss: 17.8330"
     ]
    }
   ],
   "source": [
    "# Fit \n",
    "epochs = 50\n",
    "\n",
    "# Instantiate the WGAN model.\n",
    "wgan = WGAN(\n",
    "    discriminator=discriminator,\n",
    "    generator=generator,\n",
    "    discriminator_extra_steps=3\n",
    ")\n",
    "\n",
    "# Compile the WGAN model.\n",
    "wgan.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    g_optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    g_loss_fn=generator_loss,\n",
    "    d_loss_fn=discriminator_loss,\n",
    "    c_loss_fn = counter_loss,\n",
    "    run_eagerly=False\n",
    ")\n",
    "\n",
    "# Start training the model.\n",
    "fit = wgan.fit(train, batch_size=577, epochs=epochs, verbose=True)\n",
    "print(wgan.test_step(10), \"\\n\", wgan.test_step(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35608724-5a21-4f38-8317-e9e09e3e99b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    # Select hyperparameters randomly\n",
    "\n",
    "    # Discriminator\n",
    "   \n",
    "    dlayers = random.randint(0,2)\n",
    "    if dlayers ==0:\n",
    "        discriminator = dis_3\n",
    "    elif dlayers ==1:\n",
    "        discriminator = dis_4\n",
    "    elif dlayers ==2:\n",
    "        discriminator = dis_5\n",
    "    glayers = random.randint(0,2)\n",
    "    if glayers ==0:\n",
    "        generator = gen_3\n",
    "    elif glayers ==1:\n",
    "        generator = gen_4\n",
    "    elif glayers ==2:\n",
    "        generator = gen_5\n",
    "    hidden_dim_d1 = np.random.choice([100,200,300,400])\n",
    "    hidden_dim_d2 = np.random.choice([100,200,300,400])\n",
    "    hidden_dim_d3 = np.random.choice([100,200,300,400])\n",
    "    hidden_dim_d4 = np.random.choice([100,200,300,400])\n",
    "    da1 = np.random.choice(['sigmoid', 'tanh','relu'])\n",
    "    da2 = np.random.choice(['sigmoid', 'tanh','relu'])\n",
    "    da3 = np.random.choice(['sigmoid', 'tanh','relu'])\n",
    "    da4 = np.random.choice(['sigmoid', 'tanh','relu'])\n",
    "    hidden_dim_g1 = np.random.choice([100,200,300,400])\n",
    "    hidden_dim_g2 = np.random.choice([100,200,300,400])\n",
    "    hidden_dim_g3 = np.random.choice([100,200,300,400])\n",
    "    hidden_dim_g4 = np.random.choice([100,200,300,400])\n",
    "    ga1 = np.random.choice(['sigmoid', 'tanh','relu'])\n",
    "    ga2 = np.random.choice(['sigmoid', 'tanh','relu'])\n",
    "    ga3 = np.random.choice(['sigmoid', 'tanh','relu'])\n",
    "    ga4 = np.random.choice(['sigmoid', 'tanh','relu'])\n",
    "\n",
    "    # Fit \n",
    "    epochs = 50\n",
    "\n",
    "    # Instantiate the WGAN model.\n",
    "    wgan = WGAN(\n",
    "        discriminator=discriminator,\n",
    "        generator=generator,\n",
    "        discriminator_extra_steps=3\n",
    "    )\n",
    "\n",
    "    # Compile the WGAN model.\n",
    "    wgan.compile(\n",
    "        d_optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        g_optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        g_loss_fn=generator_loss,\n",
    "        d_loss_fn=discriminator_loss,\n",
    "        c_loss_fn = counter_loss,\n",
    "        run_eagerly=False\n",
    "    )\n",
    "\n",
    "    # Start training the model.\n",
    "    fit = wgan.fit(train, batch_size=577, epochs=epochs, verbose=False)\n",
    "    print(\"discriminator layers = \", dlayers+3,\"generator layers = \", glayers+3,\"ga1 = \",ga1,\"ga2 = \",ga2,\"ga3 = \",ga3,\"ga4 = \",ga4,\n",
    "          \"da1 = \", da1,\"da2 = \",da2,\"da3 = \",da3,\"da4 = \",da4, \"hidden dims discriminator:   \",hidden_dim_d1, hidden_dim_d2, hidden_dim_d3, hidden_dim_d4,\n",
    "          \"hidden dims generator:   \",hidden_dim_g1, hidden_dim_g2, hidden_dim_g3, hidden_dim_g4,\n",
    "          wgan.test_step(10), \"\\n\", wgan.test_step(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
